# Embeding ä¸“é¢˜ {ignore=true}

[TOC]

Embedding æ—©å·²æœ‰ä¹‹ï¼ˆè¿˜è®°å¾—æœ¬ç§‘ä¸ŠæŠ½è±¡ä»£æ•°çš„æ—¶å€™ï¼Œå¬è€å¸ˆè¯´æ—©å…ˆçš„å­¦è€…å°†åµŒå…¥è¯»ä½œ kan å…¥ï¼‰ï¼Œè‡ª Word2vec ä¸€å‡ºï¼ŒEmbeding æ›´åŠ æµè¡Œï¼Œè”šç„¶æˆé£ã€‚

> ä¸‡ç‰©çš†å¯ Embedding

éœ€è¦ç‰¹åˆ«æŒ‡å‡ºçš„æ˜¯ï¼ŒEmbedding å¹¶ä¸ä»…ä»…æ˜¯è¯åµŒå…¥ï¼Œè¿˜åŒ…æ‹¬æ›´å¹¿æ³›çš„ç‰¹å¾åµŒå…¥ç­‰ï¼Œå› æ­¤ï¼Œåƒ FM å’Œ MF ç­‰éƒ½æ˜¯å­¦ä¹  Embedding çš„æ–¹æ³•ï¼Œåœ¨è¿™ä¸ªæ„ä¹‰ä¸Šï¼ŒEmbedding ä¸å…¶è¯´æ˜¯ä¸€ç§æ–¹æ³•ï¼Œæ¯‹å®è¯´æ˜¯ä¸€ç§æ€æƒ³ã€‚

## LSA

æ½œåœ¨è¯­ä¹‰åˆ†æ latent semantic analysis:LSA çš„åŸºæœ¬å‡è®¾æ˜¯ï¼šå¦‚æœä¸¤ä¸ªè¯å¤šæ¬¡å‡ºç°åœ¨åŒä¸€ç¯‡æ–‡æ¡£ä¸­ï¼Œåˆ™è¿™ä¸¤ä¸ªè¯å…·æœ‰è¯­ä¹‰ä¸Šçš„ç›¸ä¼¼æ€§ã€‚

å¯¹æ–‡æ¡£å•è¯çŸ©é˜µ $D$ åš `SVD` åˆ†è§£ï¼š

$$ \mathbf { D } = \mathbf { P } \boldsymbol { \Sigma } \mathbf { Q } ^ { T } $$

1. $\mathbf { P } \in \mathbb { R } ^ { N \times N } , \mathbf { Q } \in \mathbb { R } ^ { V \times V }$ ä¸ºå•ä½æ­£äº¤é˜µ
2. $\boldsymbol { \Sigma } \in \mathbb { R } ^ { N \times V }$ ä¸ºå¹¿ä¹‰å¯¹è§’çŸ©é˜µã€‚

ç†è§£ä¸ºï¼š `æ–‡æ¡£-å•è¯` çŸ©é˜µ = `æ–‡æ¡£-ä¸»é¢˜` çŸ©é˜µ x `ä¸»é¢˜å¼ºåº¦` x `ä¸»é¢˜-å•è¯` çŸ©é˜µã€‚

å°†å¥‡å¼‚å€¼ä»å¤§åˆ°å°è¿›è¡Œæ’åˆ—, é€‰æ‹© top k çš„å¥‡å¼‚å€¼æ¥è¿‘ä¼¼ $D$ï¼Œå³

$$
\mathbf { D } _ { k } = \mathbf { P } _ { k } \boldsymbol { \Sigma } _ { k } \mathbf { Q } _ { k } ^ { T }
$$

ä»»æ„ä¸¤ä¸ªä¸»é¢˜ä¹‹é—´çš„ç›¸ä¼¼åº¦ä¸º 0

LSA è™½ç„¶é€šè¿‡**å…¨å±€çŸ©é˜µåˆ†è§£**é™ç»´è·å¾—äº†ä¸€å®šçš„è¯­ä¹‰å…³è”ï¼Œä½†æ˜¯ï¼Œç”±äºè¯è¢‹æ¨¡å‹çš„å±€é™æ€§ï¼Œæ— æ³•æ•æ‰è¯è¯­ä¹‹é—´çš„å…ˆåå…³ç³»ã€‚

## Word2vec

è¯åµŒå…¥(word embeding)

CBOW (Continuous Bag-of-Word)
: è¿ç»­è¯è¢‹ã€‚ä»¥ä¸Šä¸‹æ–‡è¯æ±‡é¢„æµ‹å½“å‰è¯: $w_{tâˆ’1}$å’Œ$w_{t+1}$å»é¢„æµ‹ $w_t$

SkipGram
: ä»¥å½“å‰è¯é¢„æµ‹å…¶ä¸Šä¸‹æ–‡è¯æ±‡: $w_t$ å»é¢„æµ‹ $w_{tâˆ’1}$ å’Œ $w_{t+1}$ã€‚

<div align="center">
    <figure align='center'>
        <img src="img-nlp_word2vec_embedding/2019-05-22-11-37-59.png" style="width:600px" />
        <figcaption></figcaption>
    </figure>
</div>

- CBOW
  è¾“å…¥æ˜¯å¤šä¸ªè¯ï¼
  éšå±‚çš„å–æ³•ï¼š å–å¹³å‡
- SkipGram
  è¾“å‡ºæ˜¯å¤šä¸ªè¯ï¼

åœ¨ CBOW ä¸­ï¼ŒåŒä¸€ä¸ªå•è¯çš„è¡¨è¾¾ï¼ˆå³è¾“å…¥å‘é‡ $\overrightarrow { \mathbf { w } } _ { I }$ï¼‰ æ˜¯ç›¸åŒçš„ï¼Œ å› ä¸ºå‚æ•° $\mathbf W$ æ˜¯å…±äº«çš„ã€‚è¾“å‡ºå‘é‡éšç€ä¸Šä¸‹æ–‡ä¸åŒè€Œä¸åŒã€‚ è€Œåœ¨ SkipGram ä¸­æ­£å¥½ç›¸åï¼Œè¾“å…¥å‘é‡éšç€ä¸Šä¸‹æ–‡ä¸åŒè€Œä¸åŒï¼Œä½†è¾“å‡ºå‘é‡æ˜¯ç›¸åŒçš„ã€‚

ç»éªŒä¸Šè®² Skip-gram çš„æ•ˆæœå¥½ä¸€ç‚¹ã€‚

word2vec ä¸¥æ ¼æ¥è¯´ç«‹è¿ç¥ç»ç½‘ç»œéƒ½ç®—ä¸ä¸Šï¼Œå› ä¸ºå…¶æ•´ä¸ªç½‘ç»œç»“æ„æ˜¯çº¿æ€§çš„ï¼Œæ²¡æœ‰æ¿€æ´»å‡½æ•°ã€‚

### CBOW

#### ä¸€ä¸ªå•è¯ä¸Šä¸‹æ–‡ One-word context

è¾“å…¥è¾“å‡ºéƒ½åªæœ‰ä¸€ä¸ªè¯, è¿™æ˜¯æœ€ç®€å•çš„æƒ…å½¢ã€‚è¾“å…¥æ˜¯å‰ä¸€ä¸ªå•è¯ï¼Œè¾“å‡ºæ˜¯åä¸€ä¸ªå•è¯ã€‚

<div align="center">
    <figure align='center'>
        <img src="img-nlp_word2vec_embedding/2019-05-22-12-04-34.png" style="width:500px" />
        <figcaption>ä¸€ä¸ªå•è¯ä¸Šä¸‹æ–‡çš„ç½‘ç»œç»“æ„</figcaption>
    </figure>
</div>

å…¶ä¸­ï¼Œ

- è¾“å…¥å±‚çš„å•è¯ $w_I$ ä½¿ç”¨ `one-hot` æ¥è¡¨ç¤º,
- $N$ ä¸ºéšå±‚çš„å¤§å°ï¼Œå³éšå‘é‡ $\overrightarrow { \mathbf { h } } \in \mathbb { R } ^ { N }$
- ç½‘ç»œè¾“å‡º $\overrightarrow { \mathbf { y } } = \left( y _ { 1 } , y _ { 2 } , \cdots , y _ { V } \right) ^ { T } \in \mathbb { R } ^ { V }$ æ˜¯è¾“å‡ºå•è¯ä¸ºè¯æ±‡è¡¨å„å•è¯çš„æ¦‚ç‡ã€‚
- ç›¸é‚»å±‚ä¹‹é—´ä¸ºå…¨è¿æ¥
- å¯¹åº”çš„éšå‘é‡ $\mathbf { h } = \mathbf { W } ^ { T } \mathbf { x } = \mathbf { W } _ { ( k , \cdot ) } ^ { T } : = \mathbf { v } _ { w _ { I } } ^ { T }$

å› æ­¤ï¼Œ$\mathbf { h } = \mathbf { W } ^ { T } \mathbf { x }$ **ç›¸å½“äºæ‹·è´äº† $W$ å¯¹åº”çš„ç¬¬ $i$ è¡Œ(è¡Œå‘é‡è½¬ç§©ä¸ºåˆ—å‘é‡)**, å³ï¼Œå¯ä»¥ç†è§£ä¸ºæ˜¯ç¬¬ $i$ ä¸ªè¯çš„è¾“å…¥æƒé‡å‘é‡ã€‚

<aside class='caution'>
æ³¨æ„word2vecçš„éšå±‚å¹¶æ²¡æœ‰æ¿€æ´»å‡½æ•°ï¼Œæˆ–è€…è¯´åªæ˜¯ä¸€ä¸ªç®€å•çš„çº¿æ€§æ¿€æ´»å‡½æ•°
</aside>

è¾“å‡ºå‘é‡

$$
u _ { j } = \mathbf { v } _ { w _ { j } } ^ { \prime T } \mathbf { h }
$$

$W^{\prime}$ çš„ç¬¬ $i$ åˆ—ç§°ä½œå•è¯ $V_i$ çš„è¾“å‡ºå‘é‡ã€‚

å¯ä»¥å°† $W^{\prime}$ çš„æ¯ä¸€åˆ—çœ‹åšæ¯ä¸ªå•è¯çš„æ–¹å‘å‘é‡ï¼Œç„¶åè¾“å‡ºå‘é‡å°±æ˜¯çœ‹å‹ç¼©åçš„å•è¯çš„å‘é‡åœ¨æ¯ä¸ªå•è¯ä¸Šçš„æŠ•å½±ã€‚

$\overrightarrow { \mathbf { u } }$ ä¹‹åæ¥å…¥ä¸€å±‚ softmax:

$$
p \left( w _ { j } | w _ { I } \right) = y _ { j } = \frac { \exp \left( u _ { j } \right) } { \sum _ { j ^ { \prime } = 1 } ^ { V } \exp \left( u _ { j ^ { \prime } } \right) }
$$

åŒ–ç®€å¾—

$$
p \left( w _ { j } | w _ { I } \right) = \frac { \exp \left( \mathbf { v } _ { w _ { j } } ^ { \prime } T _ { \mathbf { v } _ { \boldsymbol { W } _ { I } } } \right) } { \sum _ { j ^ { \prime } = 1 } ^ { V } \exp \left( \mathbf { v } _ { w _ { j ^ { \prime } } } ^ { \prime T } \mathbf { v } _ { w _ { I } } \right) }
$$

##### ä¼˜åŒ–ç›®æ ‡

å‡è®¾ç»™å®šä¸€ä¸ªå•è¯ $\text { word } _ { I }$ ï¼ˆå®ƒç§°ä½œä¸Šä¸‹æ–‡ï¼‰ï¼Œè§‚æµ‹åˆ°å®ƒçš„ä¸‹ä¸€ä¸ªå•è¯ä¸º $\text { word } _ { O }$ ã€‚
å‡è®¾ $\text { word } _ { O }$ å¯¹åº”çš„ç½‘ç»œè¾“å‡ºç¼–å·ä¸º $j^*$ ï¼Œåˆ™ç½‘ç»œçš„ä¼˜åŒ–ç›®æ ‡æ˜¯ï¼š

$$
\begin{align}
\max _ { \mathbf { W } , \mathbf { W } ^ { \prime } } p \left( \text { word } _ { O } | \text { word } _ { I } \right) & = \max _ { \mathbf { W } , \mathbf { W } ^ { \prime } } y _ { j ^ { * } } = \max _ { \mathbf { W } , \mathbf { W } ^ { \prime } } \log \frac { \exp \left( \overrightarrow { \mathbf { w } } _ { j ^ { * } } ^ { \prime } \cdot \overrightarrow { \mathbf { w } } _ { I } \right) } { \sum _ { i = 1 } ^ { V } \exp \left( \overrightarrow { \mathbf { w } } _ { i } ^ { \prime } \cdot \overrightarrow { \mathbf { w } } _ { I } \right) } \\\
& =  \max _ { \mathbf { W } , \mathbf { W } ^ { \prime } } \left[ \overrightarrow { \mathbf { w } } _ { j ^ { \prime } } ^ { \prime } \cdot \overrightarrow { \mathbf { w } } _ { I } - \log \sum _ { i = 1 } ^ { V } \exp \left( \overrightarrow { \mathbf { w } } _ { i } ^ { \prime } \cdot \overrightarrow { \mathbf { w } } _ { I } \right) \right]
\end{align}
$$

$u_j = \overrightarrow { \mathbf { w } } _ { j } ^ { \prime } \cdot \overrightarrow { \mathbf { w } } _ { I }$, å®šä¹‰

$$
E = - \log p \left( \text { word } _ { O } | \text { word } _ { I } \right) = - \left[ \overrightarrow { \mathbf { w } } _ { j ^ { * } } ^ { \prime } \cdot \overrightarrow { \mathbf { w } } _ { I } - \log \sum _ { i = 1 } ^ { V } \exp \left( \overrightarrow { \mathbf { w } } _ { i } ^ { \prime } \cdot \overrightarrow { \mathbf { w } } _ { I } \right) \right] \\\
= - \left[ u _ { j ^ { * } } - \log \sum _ { i = 1 } ^ { V } \exp \left( u _ { i } \right) \right]
$$

##### å‚æ•°æ›´æ–°

å®šä¹‰ $t _ { j } = \mathbb { I } \left( j = j ^ { * } \right)$

åˆ™

$$
\frac { \partial E } { \partial u _ { j } } = y _ { j } - t _ { j } : = e _ { j }
$$

æ ¹æ®é“¾å¼æ³•åˆ™

$$
\frac { \partial E } { \partial w _ { i j } ^ { \prime } } = \frac { \partial E } { \partial u _ { j } } \cdot \frac { \partial u _ { j } } { \partial w _ { i j } ^ { \prime } } = e _ { j } \cdot h _ { i }
$$

å› æ­¤ï¼Œ$W^{\prime}$çš„æƒé‡æ›´æ–°å…¬å¼

$$
w _ { i j } ^ { \prime ( \text { new } ) } = w _ { i j } ^ { \prime ( \text { old} ) } - \eta \cdot e _ { j } \cdot h _ { i }
$$

$W$ çš„ æƒé‡æ›´æ–°å…¬å¼çš„è¯ï¼Œæ³¨æ„åˆ°

$$
\frac { \partial E } { \partial h _ { i } } = \sum _ { j = 1 } ^ { V } \frac { \partial E } { \partial u _ { j } } \cdot \frac { \partial u _ { j } } { \partial h _ { i } } = \sum _ { j = 1 } ^ { V } e _ { j } \cdot w _ { i j } ^ { \prime } : = \mathrm { EH } _ { i }
$$

ä¸”

$$
h _ { i } = \sum _ { k = 1 } ^ { V } x _ { k } \cdot w _ { k i }
$$

å› æ­¤

$$
\frac { \partial E } { \partial w _ { k i } } = \frac { \partial E } { \partial h _ { i } } \cdot \frac { \partial h _ { i } } { \partial w _ { k i } } = \mathrm { EH } _ { i } \cdot x _ { k }
$$

ç­‰ä»·äº

$$
\frac { \partial E } { \partial \mathbf { W } } = \mathbf { x } \otimes \mathrm { EH } = \mathbf { x } \mathrm { EH } ^ { T }
$$

å› æ­¤ï¼Œ$W$ çš„å‚æ•°æ›´æ–°å…¬å¼ä¸º

$$
\mathbf { v } _ { w _ { I } } ^ { ( \text { new } ) } = \mathbf { v } _ { w _ { I } } ^ { ( \text { old } ) } - \eta \mathrm { EH } ^ { T }
$$

#### å¤šä¸ªå•è¯ä¸Šä¸‹æ–‡ Multi-word context

<div >
    <figure >
        <img src="./img-nlp_word2vec_embedding/2019-07-03-14-27-04.png" style="width:500px" />
        <figcaption align='center' >å¤šä¸ªå•è¯ä¸Šä¸‹æ–‡çš„ç½‘ç»œç»“æ„</figcaption>
    </figure>
</div>

éšå‘é‡ä¸ºæ‰€æœ‰è¾“å…¥å•è¯æ˜ å°„ç»“æœçš„å‡å€¼ï¼š

$$
\overrightarrow { \mathbf { h } } = \frac { 1 } { C } \mathbf { W } ^ { T } \left( \overrightarrow { \mathbf { x } } _ { 1 } + \overrightarrow { \mathbf { x } } _ { 2 } + \cdots + \overrightarrow { \mathbf { x } } _ { C } \right) = \frac { 1 } { C } \left( \overrightarrow { \mathbf { w } } _ { I _ { 1 } } + \overrightarrow { \mathbf { w } } _ { I _ { 2 } } + \cdots + \overrightarrow { \mathbf { w } } _ { I _ { C } } \right)
$$

æŸå¤±å‡½æ•°å…¶å®å’Œå•ä¸ªä¸Šä¸‹æ–‡çš„æ˜¯ä¸€æ ·çš„ï¼Œå”¯ä¸€ä¸åŒçš„æ˜¯$h$; å› æ­¤ï¼Œä»éšå±‚åˆ°è¾“å‡ºå±‚çš„æƒé‡ $W^{\prime}$ çš„å‚æ•°æ›´æ–°å…¬å¼æ˜¯ä¸å˜çš„ï¼š

$$
\mathbf { v } _ { w _ { j } } ^ { \prime  ( \text { new } )} = \mathbf { v } _ { w _ { j } } ^ { \prime  ( \text { old } )} - \eta \cdot e _ { j } \cdot \mathbf { h } \qquad \text { for } j = 1,2 , \cdots , V
$$

è¾“å…¥åˆ°éšå±‚çš„æƒé‡ $W$çš„å‚æ•°æ›´æ–°å…¬å¼åˆ™ç¨æœ‰ä¸åŒ

$$
\mathbf { v } _ { w _ { I , c } } ^ { ( \mathrm { new } ) } = \mathbf { v } _ { w _ { I , c } } ^ { ( \text { old } ) } - \frac { 1 } { C } \cdot \eta \cdot \mathrm { EH } ^ { T } \qquad \text { for } c = 1,2 , \cdots , C
$$

### SkipGram

<div >
    <figure >
        <img src="img-nlp_word2vec_embedding/2019-07-03-14-33-04.png" style="width:500px" />
        <figcaption align='center'>SkipGram æ¨¡å‹ç½‘ç»œç»“æ„</figcaption>
    </figure>
</div>

ä»è¾“å…¥åˆ°éšå±‚è¿™å—å’Œ CBOW æ˜¯ä¸€æ ·çš„ï¼Œéšå‘é‡è¿˜æ˜¯ç›¸å½“äºå°†è¾“å…¥-éšè—æƒé‡çŸ©é˜µçš„å¯¹åº”è¡Œå‘é‡æ‹·è´è¿‡æ¥ã€‚

è€Œè¾“å‡ºéƒ¨åˆ†å°±ä¸åŒäº†ï¼Œéœ€è¦è¾“å‡º$C$ä¸ªå¤šé¡¹åˆ†å¸ƒ

$$
p \left( w _ { c , j } = w _ { O , c } | w _ { I } \right) = y _ { c , j } = \frac { \exp \left( u _ { c , j } \right) } { \sum _ { j ^ { \prime } = 1 } ^ { V } \exp \left( u _ { j ^ { \prime } } \right) }
$$

å¥½åœ¨ä»éšå±‚åˆ°è¾“å‡ºå±‚çš„æƒé‡æ˜¯å…±äº«çš„ï¼Œå› æ­¤æœ‰

$$
u _ { c , j } = u _ { j } = \mathbf { v } _ { w _ { j } } ^ { \prime T } \cdot \mathbf { h } , \text { for } c = 1,2 , \cdots , C
$$

æŸå¤±å‡½æ•°

$$
\begin{aligned} E & = - \log p \left( w _ { O , 1 } , w _ { O , 2 } , \cdots , w _ { O , C } | w _ { I } \right) \\\ & = - \log \prod _ { c = 1 } ^ { C } \frac { \exp \left( u _ { c , j _ { c } ^ { * } } \right) } { \sum _ { j ^ { \prime } = 1 } ^ { V } \exp \left( u _ { j ^ { \prime } } \right) } \\\ & = - \sum _ { c = 1 } ^ { C } u _ { j _ { c } ^ { * } } + C \cdot \log \sum _ { j ^ { \prime } = 1 } ^ { V } \exp \left( u _ { j ^ { \prime } } \right) \end{aligned}
$$

æ±‚å¯¼

$$
\frac { \partial E } { \partial u _ { c , j } } = y _ { c , j } - t _ { c , j } : = e _ { c , j }
$$

è®° $\mathrm { EI } = \left\\{ \mathrm { EI } _ { 1 } , \cdots , \mathrm { EI } _ { V } \right\\}$ , å…¶ä¸­ $\mathrm { EI } _ { j } = \sum _ { c = 1 } ^ { C } e _ { c , j }$

åˆ™ å…³äºéšå±‚åˆ°è¾“å‡ºå±‚æƒé‡çŸ©é˜µ $W^{\prime}$ çš„æ±‚å¯¼

$$
\frac { \partial E } { \partial w _ { i j } ^ { \prime } } = \sum _ { c = 1 } ^ { C } \frac { \partial E } { \partial u _ { c , j } } \cdot \frac { \partial u _ { c , j } } { \partial w _ { i j } ^ { \prime } } = \mathrm { EI } _ { j } \cdot h _ { i }
$$

å› æ­¤ï¼Œå‚æ•°æ›´æ–°å…¬å¼ä¸º

$$
w _ { i j } ^ { \prime  ( \text { new } )} = w _ { i j } ^ { \prime  \text { (old) }} - \eta \cdot \mathrm { EI } _ { j } \cdot h _ { i }
$$

è€Œè¾“å…¥åˆ°éšå±‚çš„æƒé‡çŸ©é˜µå‚æ•°æ›´æ–°å…¬å¼ä¸å˜

$$
\mathbf { v } _ { w _ { I } } ^ { ( \text { new } ) } = \mathbf { v } _ { w _ { I } } ^ { ( \text { old } ) } - \eta \cdot \mathrm { EH } ^ { T }
$$

### è®¡ç®—æ•ˆç‡ä¼˜åŒ–

å¦‚ä¸Šï¼Œæ¯ä¸ªå•è¯å…¶å®æœ‰è¾“å…¥å‘é‡å’Œè¾“å‡ºå‘é‡ä¸¤ä¸ªå‘é‡è¡¨ç¤ºã€‚è¾“å…¥å‘é‡çš„å­¦ä¹ æ˜¯æ¯”è¾ƒå®¹æ˜“çš„ï¼Œè€Œå­¦ä¹ è¾“å‡ºå‘é‡çš„ä»£ä»·å´ååˆ†æ˜‚è´µã€‚

å¤æ‚åº¦å¾ˆå¤§çš„æ ¹æœ¬åŸå› æ˜¯ softmax çš„åˆ†æ¯ä¸Šçš„ $\sum$ï¼Œ å¯ä»¥é€šè¿‡ `åˆ†å±‚ softmax` æ¥é«˜æ•ˆè®¡ç®—è¿›è¡Œä¼˜åŒ–ã€‚å¦å¤–ï¼Œå¯ä»¥é€šè¿‡è´Ÿé‡‡æ ·æ¥ç¼©å‡è¾“å‡ºå•å…ƒçš„æ•°é‡ã€‚

ä¼˜åŒ–çš„ä¸»è¦æ€æƒ³æ˜¯ï¼šé™åˆ¶è¾“å‡ºå•å…ƒçš„æ•°é‡ã€‚

#### Hierarchical Softmax

`Hierarchical softmax(åˆ†å±‚ softmax)` æ˜¯ä¸€ç§é«˜æ•ˆè®¡ç®— softmax çš„æ–¹æ³•, å› ä¸ºå®ƒå°†è¯æ±‡ç¼–ç ä¸ºå“ˆå¤«æ›¼æ ‘.
æ¯ä¸ªè¯ä¸å†æœ‰è¾“å‡ºå‘é‡è¡¨ç¤ºã€‚å–è€Œä»£ä¹‹ï¼Œå¯¹æ¯ä¸ªå†…éƒ¨èŠ‚ç‚¹æœ‰ä¸€ä¸ªå‘é‡è¡¨ç¤º(ä½œä¸ºæ¯ä¸ªèŠ‚ç‚¹é€»è¾‘å›å½’çš„å‚æ•°å‘é‡ï¼Œå†…éƒ¨èŠ‚ç‚¹å…±æœ‰ $V-1$ä¸ªï¼Œå› æ­¤ï¼Œå‚æ•°é‡å‡ ä¹æ²¡æœ‰å‡å°‘)ã€‚æ ¹èŠ‚ç‚¹çš„è¯å‘é‡å¯¹åº”æˆ‘ä»¬çš„æŠ•å½±åçš„è¯å‘é‡ã€‚

ç”±äºæ¯ä¸ªè¯éƒ½æœ‰ä¸€ä¸ªéœå¤«æ›¼ç¼–ç ï¼Œå› æ­¤å¯¹ç›®æ ‡è¯ï¼Œæˆ‘ä»¬çŸ¥é“æ¯ä¸€æ­¥å‘å³è¿˜æ˜¯å‘å·¦ï¼Œå°†è¿™äº›æ­¥çš„æ¦‚ç‡ä¹˜èµ·æ¥ï¼Œå°±æ˜¯ç›®æ ‡è¯ä½œä¸ºè¾“å‡ºè¯çš„é¢„æµ‹æ¦‚ç‡ã€‚ï¼ˆåœ¨ä¸ç”¨ Hierarchical Softmax æ—¶ï¼Œæˆ‘ä»¬è®¡ç®—ç›®æ ‡è¯çš„è¾“å‡ºæ¦‚ç‡æ—¶ï¼Œè¦è®¡ç®—å…¶ä»–æ‰€æœ‰è¯çš„æ¦‚ç‡ä½œä¸ºåˆ†æ¯ï¼Œæ‰€æœ‰å…¶ä»–è¯çš„æ¦‚ç‡è®¡ç®—ä»…ä»…æ˜¯ä¸ºäº†è®¡ç®—åˆ†æ¯ï¼ç°åœ¨åˆ™ä¸éœ€è¦äº†ï¼Œç›´æ¥è¿ä¹˜å¾—åˆ°ç›®æ ‡è¯çš„è¾“å‡ºæ¦‚ç‡ã€‚ï¼‰

ç”±äºæ˜¯äºŒå‰æ ‘ï¼Œä¹‹å‰è®¡ç®—é‡ä¸º $V$ ,ç°åœ¨å˜æˆäº† $log2V$
![](img-nlp_word2vec_embedding/2019-07-07-20-16-03.png)

$$
p \left( w = w _ { O } \right) = \prod _ { j = 1 } ^ { L ( w ) - 1 } \sigma \left( \mathbb { I } n ( w , j + 1 ) = \operatorname { ch } ( n ( w , j ) ) \mathbb{I} \cdot \mathbf { v } _ { n ( w , j ) } ^ { \prime } T _ { \mathbf { h } } \right)
$$

å…¶ä¸­ï¼Œ$\operatorname { ch } ( n )$ æ˜¯å•å…ƒ n çš„å·¦èŠ‚ç‚¹ï¼Œä¸”

$$
\mathbb { I } x \| = \left\\{ \begin{array} { l l } { 1 } & { \text { if } x \text { is true; } } \\\ { - 1 } & { \text { otherwise } } \end{array} \right.
$$

éœå¤«æ›¼æ ‘çš„ç»“æ„æ˜¯åŸºäºè´ªå¿ƒçš„æ€æƒ³ï¼Œ**å¯¹è®­ç»ƒé¢‘ç‡å¾ˆå¤§çš„è¯å¾ˆæœ‰æ•ˆï¼Œä½†æ˜¯å¯¹è¯é¢‘å¾ˆä½çš„è¯å¾ˆä¸å‹å¥½ï¼Œå› ä¸ºè·¯å¾„å¤ªæ·±**

#### Negative Sampling

åœ¨å‚æ•°çš„æ¯ä¸€è½®æ›´æ–°ä¸­ï¼Œå®é™…ä¸Šåªéœ€è¦ç”¨åˆ°ä¸€éƒ¨åˆ†å•è¯çš„è¾“å‡ºæ¦‚ç‡ï¼›å¤§éƒ¨åˆ†å•è¯çš„è¾“å‡ºæ¦‚ç‡ä¸º 0 ã€‚

ä¾‹å¦‚

$$
P _ { n } ( w ) = \frac { f r e q ( w ) ^ { 3 / 4 } } { \sum _ { w \neq w _ { O } } f r e q ( w ) ^ { 3 / 4 } }
$$

èƒŒåçš„ç‰©ç†æ„ä¹‰ä¸ºï¼šå•è¯åœ¨è¯­æ–™åº“ä¸­å‡ºç°çš„æ¦‚ç‡è¶Šå¤§ï¼Œåˆ™è¶Šå¯èƒ½è¢«æŒ‘ä¸­.

ç›®æ ‡å‡½æ•°ç®€åŒ–ä¸º

$$
E = -\log \sigma \left( \mathbf { v } _ { w _ { O } } ^ { \prime  ^ { T }} \mathbf { h } \right) - \sum _ { w _ { j } \in \mathcal { W } _ { \mathrm { neg } } } \log \sigma \left( - \mathbf { v } _ { w _ { j } } ^ { \prime T } \mathbf { h } \right)
$$

#### å…¶ä»–ç»†èŠ‚

1. $\sigma (x)$ çš„è¿‘ä¼¼è®¡ç®—
2.

### Glove

Global Vectors for Word Representationï¼Œ æ˜¯ä¸€ä¸ª**åŸºäºå…¨å±€è¯é¢‘ç»Ÿè®¡**ï¼ˆcount-based & overall statisticsï¼‰çš„è¯è¡¨å¾ï¼ˆword representationï¼‰å·¥å…·.

ç»“åˆäº† LSA ç®—æ³•å’Œ Word2Vec ç®—æ³•çš„ä¼˜ç‚¹ï¼Œæ—¢è€ƒè™‘äº†å…¨å±€ç»Ÿè®¡ä¿¡æ¯ï¼Œåˆåˆ©ç”¨äº†å±€éƒ¨ä¸Šä¸‹æ–‡ã€‚

[GloVe](http://www-nlp.stanford.edu/projects/glove/glove.pdf)(GLobal Vector for word representation)

[A GloVe implementation in python](http://www.foldl.me/2014/glove-python/)
é¦–å…ˆæ‰«æé¢„æ–™ï¼Œå»ºç«‹è¯è¯­å…±ç”ŸçŸ©é˜µï¼Œåç»­å°±ç”¨è¯¥å…±ç”ŸçŸ©é˜µæ›¿ä»£åŸé¢„æ–™ã€‚

W2V çš„ç¼ºç‚¹ï¼ŒW2V ä¸­æ‰€ç”¨çš„ softmax æ¯”èµ·æ™®é€š softmax æœ‰ä½•åŒºåˆ«ï¼Œä¸ºä»€ä¹ˆèƒ½å‡å°‘è®¡ç®—é‡

### fastText

1. ç»“æ„ä¸ CBOW ç±»ä¼¼ï¼Œä½†å­¦ä¹ ç›®æ ‡æ˜¯äººå·¥æ ‡æ³¨çš„åˆ†ç±»ç»“æœ

2. é‡‡ç”¨ hierarchical softmax å¯¹è¾“å‡ºçš„åˆ†ç±»æ ‡ç­¾å»ºç«‹å“ˆå¤«æ›¼æ ‘ï¼Œæ ·æœ¬ä¸­æ ‡ç­¾å¤šçš„ç±»åˆ«è¢«åˆ†é…çŸ­çš„æœå¯»è·¯å¾„ï¼›

3. å¼•å…¥ N-gramï¼Œè€ƒè™‘è¯åºç‰¹å¾

4. å¼•å…¥ subword æ¥å¤„ç†é•¿è¯ï¼Œå¤„ç†æœªç™»é™†è¯é—®é¢˜

## Doc2Vec

Distributed Representations of Sentences and Documents
å…¶å®æ˜¯ paragraph vector, æˆ–ç§° sentence embeddings

æ¯ä¸ªæ®µè½æ‹¥æœ‰è‡ªå·±çš„æ®µè½å‘é‡. å¯¹äºæ¯æ¬¡è®­ç»ƒï¼Œè¿™é‡Œé€‰æ‹©è”ç»“(concatenate)çš„æ–¹å¼ï¼Œå°† Word2Vec ä¸­çš„è¾“å…¥è¯å‘é‡ä¸è¾“å…¥è¯å‘é‡æ‰€å±çš„æ®µè½å‘é‡è”ç»“ï¼Œæ‹¼æ¥æˆä¸€ä¸ªå¤§å‘é‡ã€‚

### PV-DM

æŠŠå¥å­å‘é‡ä¹Ÿçœ‹åšä¸€ä¸ªå•è¯åŠ å…¥ã€‚å®ƒçš„ä½œç”¨ç›¸å½“äºæ˜¯ä¸Šä¸‹æ–‡çš„è®°å¿†å•å…ƒæˆ–è€…æ˜¯è¿™ä¸ªæ®µè½çš„ä¸»é¢˜

### PV-DBOW

Distributed Bag of Words version of Paragraph Vector()

## Item2Vec

[Item2Vec-Neural Item Embedding for Collaborative Filtering (Microsoft 2016)]()
å¾®è½¯å°† word2vec åº”ç”¨äºæ¨èé¢†åŸŸçš„ä¸€ç¯‡å®ç”¨æ€§å¾ˆå¼ºçš„æ–‡ç« 

$$
\frac { 1 } { K } \sum _ { i = 1 } ^ { K } \sum _ { j \neq i } ^ { K } \log p \left( w _ { j } | w _ { i } \right)
$$

KDD 2018 best paper [Real-time Personalization using Embeddings for Search Ranking at Airbnb]() ä¹Ÿä»‹ç»äº† Airbnb çš„ embedding æœ€ä½³å®è·µ

å…·ä½“åˆ° embedding ä¸Šï¼Œæ–‡ç« é€šè¿‡ä¸¤ç§æ–¹å¼ç”Ÿæˆäº†ä¸¤ç§ä¸åŒçš„ embedding åˆ†åˆ« capture ç”¨æˆ·çš„ short term å’Œ long term çš„å…´è¶£ã€‚

- ä¸€æ˜¯é€šè¿‡ click session æ•°æ®ç”Ÿæˆ listing çš„ embeddingï¼Œç”Ÿæˆè¿™ä¸ª embedding çš„ç›®çš„æ˜¯ä¸ºäº†è¿›è¡Œ listing çš„ç›¸ä¼¼æ¨èï¼Œä»¥åŠå¯¹ç”¨æˆ·è¿›è¡Œ session å†…çš„å®æ—¶ä¸ªæ€§åŒ–æ¨èã€‚
- äºŒæ˜¯é€šè¿‡ booking session ç”Ÿæˆ user-type å’Œ listing-type çš„ embeddingï¼Œç›®çš„æ˜¯æ•æ‰ä¸åŒ user-type çš„ long term å–œå¥½ã€‚ç”±äº booking signal è¿‡äºç¨€ç–ï¼ŒAirbnb å¯¹åŒå±æ€§çš„ user å’Œ listing è¿›è¡Œäº†èšåˆï¼Œå½¢æˆäº† user-type å’Œ listing-type è¿™ä¸¤ä¸ª embedding çš„å¯¹è±¡ã€‚

## ç¦»æ•£ç‰¹å¾åµŒå…¥

è¿™é‡Œä¸€èˆ¬é’ˆå¯¹çš„æ˜¯é«˜ç»´ã€ç¨€ç–ã€å¤š Fieldã€ç¦»æ•£ç‰¹å¾ã€‚

ç”¨ç¥ç»ç½‘ç»œåš Embedding çš„ä¸€èˆ¬åšæ³•ï¼š ç½‘ç»œçš„è¾“å…¥å±‚æ˜¯å®ä½“ IDï¼ˆcategorical ç‰¹å¾ï¼‰çš„ one-hot ç¼–ç å‘é‡ã€‚ä¸è¾“å…¥å±‚ç›¸è¿çš„ä¸€å±‚å°±æ˜¯ Embedding å±‚ï¼Œä¸¤å±‚ä¹‹é—´é€šè¿‡å…¨è¿æ¥çš„æ–¹å¼ç›¸è¿ã€‚Embedding å±‚çš„ç¥ç»å…ƒä¸ªæ•°å³ Embeeding å‘é‡çš„ç»´æ•°ï¼ˆğ‘šï¼‰ã€‚è¾“å…¥å±‚ä¸ Embedding å±‚çš„é“¾æ¥å¯¹åº”çš„æƒé‡çŸ©é˜µ ğ‘€(ğ‘›Ã—ğ‘š)ï¼Œå³å¯¹åº” ğ‘› ä¸ªè¾“å…¥å®ä½“çš„ m ç»´ embedding å‘é‡ã€‚ç”±äº one-hot å‘é‡åŒä¸€æ—¶åˆ»åªä¼šæœ‰ä¸€ä¸ªå…ƒç´ å€¼ä¸º 1ï¼Œå…¶ä»–å€¼éƒ½æ˜¯ 0ï¼Œå› æ­¤å¯¹äºå½“å‰æ ·æœ¬ï¼Œåªæœ‰ä¸å€¼ä¸º 1 çš„è¾“å…¥èŠ‚ç‚¹ç›¸è¿çš„è¾¹ä¸Šçš„æƒé‡ä¼šè¢«æ›´æ–°ï¼Œå³ä¸åŒ ID çš„å®ä½“æ‰€åœ¨çš„æ ·æœ¬è®­ç»ƒè¿‡ç¨‹ä¸­åªä¼šå½±å“ä¸è¯¥å®ä½“å¯¹åº”çš„ embedding è¡¨ç¤ºã€‚å‡è®¾æŸå®ä½“ ID çš„ one-hot å‘é‡ä¸­ä¸‹æ ‡ä¸º ğ‘– çš„å€¼ä¸º 1ï¼Œåˆ™è¯¥å®ä½“çš„ embedding å‘é‡ä¸ºæƒé‡çŸ©é˜µ ğ‘€ çš„ç¬¬ ğ‘– è¡Œã€‚

<div align="center">
    <figure align='center'>
        <img src="/img-nlp_word2vec_embedding/2019-06-14-14-19-33.png" style="width:300px" />
    </figure>
</div>

## Graph Embedding

åœ¨é¢å¯¹å›¾ç»“æ„çš„æ—¶å€™ï¼Œä¼ ç»Ÿçš„åºåˆ— embedding æ–¹æ³•å°±æ˜¾å¾—åŠ›ä¸ä»å¿ƒäº†ã€‚åœ¨è¿™æ ·çš„èƒŒæ™¯ä¸‹ï¼Œå¯¹å›¾ç»“æ„ä¸­é—´çš„èŠ‚ç‚¹è¿›è¡Œè¡¨è¾¾çš„ graph embedding æˆä¸ºäº†æ–°çš„ç ”ç©¶æ–¹å‘ï¼Œå¹¶é€æ¸åœ¨æ·±åº¦å­¦ä¹ æ¨èç³»ç»Ÿé¢†åŸŸæµè¡Œèµ·æ¥ã€‚

word2vec å’Œ item2vec å…¨éƒ½æ˜¯åœ¨åºåˆ—ï¼ˆå¥å­æˆ–è€…ç‰©å“åˆ—è¡¨ï¼‰çš„åŸºç¡€ä¸Šï¼Œè€Œåœ¨äº’è”ç½‘åœºæ™¯ä¸‹ï¼Œæ•°æ®å¯¹è±¡ä¹‹é—´æ›´å¤šå‘ˆç°çš„æ˜¯å›¾ç»“æ„ã€‚å…¸å‹çš„åœºæ™¯æ˜¯ç”±ç”¨æˆ·è¡Œä¸ºæ•°æ®ç”Ÿæˆçš„ç‰©å“å…¨å±€å…³ç³»å›¾ï¼Œä»¥åŠåŠ å…¥æ›´å¤šå±æ€§çš„ç‰©å“ç»„æˆçš„çŸ¥è¯†å›¾è°±.

å›¾æ¯”åºåˆ—å¥½çš„ä¸€ç‚¹ï¼Œå°±æ˜¯å›¾ä¸Šå¯ä»¥å¾—åˆ°åºåˆ—ï¼Œå¹¶ä¸”èƒ½ç”Ÿæˆä¸€äº›å¹¶ä¸å­˜åœ¨çš„åºåˆ—ï¼Œå³ä¸€å®šç¨‹åº¦çš„æ³›åŒ–ï¼Œå¹¶ä¸”å›¾å¯ä»¥è®°å½•è¾¹çš„æƒé‡ï¼ŒèŠ‚ç‚¹æœ‰å±æ€§é¢å¤–ä¿¡æ¯ã€‚

<div >
    <figure >
        <img src="img-nlp_word2vec_embedding/2019-05-28-09-18-20.png" style="width:500px" />
        <figcaption>ç”±ç”¨æˆ·è¡Œä¸ºåºåˆ—ç”Ÿæˆçš„ç‰©å“å…¨å±€å…³ç³»å›¾ (å¼•è‡ªé˜¿é‡Œè®ºæ–‡)</figcaption>
    </figure>
</div>

<div >
    <figure >
        <img src="img-nlp_word2vec_embedding/2019-05-28-09-19-14.png" style="width:500px" />
        <figcaption>ç”±å±æ€§ã€å®ä½“ã€å„ç±»çŸ¥è¯†ç»„æˆçš„çŸ¥è¯†å›¾è°±</figcaption>
    </figure>
</div>

### ç»å…¸çš„ Graph Embedding æ–¹æ³•â€”â€”DeepWalk

æ—©æœŸå½±å“åŠ›è¾ƒå¤§çš„ graph embedding æ–¹æ³•æ˜¯ 2014 å¹´æå‡ºçš„ DeepWalkï¼Œå®ƒçš„ä¸»è¦æ€æƒ³æ˜¯åœ¨ç”±ç‰©å“ç»„æˆçš„å›¾ç»“æ„(ç”±ç”¨æˆ·è¡Œä¸ºåºåˆ—æ„å»º)ä¸Šè¿›è¡Œéšæœºæ¸¸èµ°ï¼Œäº§ç”Ÿå¤§é‡ç‰©å“åºåˆ—ï¼Œç„¶åå°†è¿™äº›ç‰©å“åºåˆ—ä½œä¸ºè®­ç»ƒæ ·æœ¬è¾“å…¥ word2vec è¿›è¡Œè®­ç»ƒï¼Œå¾—åˆ°ç‰©å“çš„ embedding

![](img-nlp_word2vec_embedding/2019-05-28-09-24-26.png)

æ ¸å¿ƒæ˜¾ç„¶åœ¨ç¬¬ä¸‰æ­¥ï¼šå¦‚ä½•éšæœºæ¸¸èµ°ï¼Ÿ ä¹Ÿå°±æ˜¯ï¼Œå¦‚ä½•å®šä¹‰éšæœºæ¸¸èµ°çš„è½¬ç§»æ¦‚ç‡ï¼Ÿ
ç®€å•çš„å½“ç„¶æ˜¯åŠ æƒä¸€ä¸‹æŒ‰æ¯”ä¾‹åˆ†é…äº†

$$
P \left( v _ { j } | v _ { i } \right) = \left\\{ \begin{array} { l l } { \frac { \mathrm { M } _ { i j } } { \sum _ { j \in N _ { + } \left( v _ { i } \right) } \mathrm { M } _ { i j } } , } & { v _ { j } \in N _ { + } \left( v _ { i } \right) } \\\ { 0 , } & { e _ { i j } \notin \varepsilon } \end{array} \right.
$$

æ˜¾ç„¶ï¼ŒDeepWalk éå†èŠ‚ç‚¹çš„æ–¹å¼ï¼Œæœ¬è´¨è¿˜æ˜¯å±äº `DFS`

### 2015-LINE

Large-scale Information Network Embedding (MSRA 2015)

#### ç›¸ä¼¼åº¦

ä¸€é˜¶ç›¸ä¼¼åº¦ï¼š ç›´æ¥ç›¸è¿åˆ™ç›¸ä¼¼åº¦ä¸ºè¾¹æƒï¼Œå¦åˆ™ä¸º 0

### DeepWalk çš„è¿›ä¸€æ­¥æ”¹è¿›â€”â€”Node2vec

2016 å¹´ï¼Œæ–¯å¦ç¦å¤§å­¦åœ¨ DeepWalk çš„åŸºç¡€ä¸Šæ›´è¿›ä¸€æ­¥ï¼Œé€šè¿‡è°ƒæ•´éšæœºæ¸¸èµ°æƒé‡çš„æ–¹æ³•ä½¿ graph embedding çš„ç»“æœåœ¨ç½‘ç»œçš„ **åŒè´¨æ€§**ï¼ˆhomophilyï¼‰å’Œ**ç»“æ„æ€§**ï¼ˆstructural equivalenceï¼‰ä¸­è¿›è¡Œæƒè¡¡ã€‚

Scalable Feature Learning for Networks (Stanford 2016)

åŒè´¨æ€§
: è·ç¦»ç›¸è¿‘èŠ‚ç‚¹çš„ embedding åº”è¯¥å°½é‡è¿‘ä¼¼ï¼Œå› æ­¤å€¾å‘äºæ·±åº¦ä¼˜å…ˆæœç´¢ DFS

ç»“æ„æ€§
: ç»“æ„ä¸Šç›¸ä¼¼çš„èŠ‚ç‚¹çš„ embedding åº”è¯¥å°½é‡æ¥ï¼Œå› æ­¤å€¾å‘äºå®½åº¦ä¼˜å…ˆæœç´¢ BFS ï¼ˆè·³çš„æ›´è¿œä¸€ç‚¹ï¼Œå»å‘ç°ç›¸åŒçš„ç»“æ„ï¼‰

<div >
    <figure >
        <img src="img-nlp_word2vec_embedding/2019-05-28-09-39-57.png" style="width:500px" />
        <figcaption>node2vec å®éªŒç»“æœ</figcaption>
    </figure>
</div>

### 2016-SDNE

Structural Deep Network Embedding
å’Œ node2vec å¹¶åˆ—çš„å·¥ä½œï¼Œå‡å‘è¡¨åœ¨ 2016 å¹´çš„ KDD ä¼šè®®ä¸­ã€‚

### 2017-Structure2vec

å‚è€ƒ [Struc2Vecï¼šç®—æ³•åŸç†ï¼Œå®ç°å’Œåº”ç”¨](https://zhuanlan.zhihu.com/p/56733145)

ä¸åŒäºé‚£äº›é¢†åŸŸç›¸ä¼¼å‡è®¾çš„æ–¹æ³•ï¼Œè®¤ä¸º*ä¸¤ä¸ªä¸æ˜¯è¿‘é‚»çš„é¡¶ç‚¹ä¹Ÿå¯èƒ½æ‹¥æœ‰å¾ˆé«˜çš„ç›¸ä¼¼æ€§*.
é‡ç‚¹å…³æ³¨ç»“æ„ç›¸ä¼¼æ€§ï¼Œ å› æ­¤ä»ç©ºé—´ç»“æ„ç›¸ä¼¼æ€§çš„è§’åº¦å®šä¹‰é¡¶ç‚¹ç›¸ä¼¼åº¦ã€‚

ç›´è§‚æ¥çœ‹ï¼Œå…·æœ‰ç›¸åŒåº¦æ•°çš„é¡¶ç‚¹æ˜¯ç»“æ„ç›¸ä¼¼çš„ï¼Œè‹¥å„è‡ªé‚»æ¥é¡¶ç‚¹ä»ç„¶å…·æœ‰ç›¸åŒåº¦æ•°ï¼Œé‚£ä¹ˆä»–ä»¬çš„ç›¸ä¼¼åº¦å°±æ›´é«˜ã€‚

#### é¡¶ç‚¹å¯¹è·ç¦»å®šä¹‰

ä»¤ $R _ { k } ( u )$ è¡¨ç¤ºåˆ°é¡¶ç‚¹ $u$ è·ç¦»ä¸º $k$ çš„é¡¶ç‚¹é›†åˆ (ç‰¹åˆ«çš„ï¼Œ$R_1(u)$ è¡¨ç¤º$u$ çš„ ç›´æ¥ç›¸é‚»è¿‘é‚»é›†åˆã€‚

ä»¤ $s(S)$ è¡¨ç¤ºé¡¶ç‚¹é›†åˆ $S$ çš„æœ‰åºåº¦åºåˆ—ã€‚

å¦‚ä¸‹å®šä¹‰é¡¶ç‚¹ $u$ å’Œ $v$ ä¹‹é—´è·ç¦»ä¸º $k$ï¼ˆè¿™é‡Œçš„è·ç¦» k å®é™…ä¸Šæ˜¯æŒ‡è·ç¦»å°äºç­‰äº $k$ çš„èŠ‚ç‚¹é›†åˆï¼‰çš„ç¯è·¯ä¸Šçš„ç»“æ„è·ç¦»

$$
f _ { k } ( u , v ) = f _ { k - 1 } ( u , v ) + g \left( s \left( R _ { k } ( u ) \right) , s \left( R _ { k } ( v ) \right) \right) , k \geq 0 \text { and } \left| R _ { k } ( u ) \right| , \left| R _ { k } ( v ) \right| > 0
$$

å…¶ä¸­ $g \left( D _ { 1 } , D _ { 2 } \right) \geq 0$ æ˜¯è¡¡é‡æœ‰åºåº¦åºåˆ— $D_1$ å’Œ $D_2$ çš„è·ç¦»çš„å‡½æ•°ï¼Œä¸” $f_{-1} = 0$

ç”±äºä¸¤ä¸ªåºåˆ—çš„é•¿åº¦å¯èƒ½ä¸åŒï¼Œå¹¶ä¸”å¯èƒ½å«æœ‰é‡å¤å…ƒç´ ï¼Œå› æ­¤æ–‡ç« é‡‡ç”¨äº† `Dynamic Time Warping(DTW)` æ¥è¡¡é‡ä¸¤ä¸ªæœ‰åºåº¦åºåˆ—ã€‚

#### æ„å»ºå±‚æ¬¡å¸¦æƒå›¾

#### é‡‡æ ·è·å–é¡¶ç‚¹åºåˆ—

### é˜¿é‡Œçš„ Graph Embedding æ–¹æ³• EGES

2018 å¹´é˜¿é‡Œå…¬å¸ƒäº†å…¶åœ¨æ·˜å®åº”ç”¨çš„ Embedding æ–¹æ³• EGESï¼ˆEnhanced Graph Embedding with Side Informationï¼‰ï¼Œå…¶åŸºæœ¬æ€æƒ³æ˜¯åœ¨ DeepWalk ç”Ÿæˆçš„ graph embedding åŸºç¡€ä¸Šå¼•å…¥é™„åŠ ä¿¡æ¯ï¼Œä»¥é‡ç‚¹è§£å†³å†·å¯åŠ¨å’Œé•¿å°¾å•†å“çš„é—®é¢˜ã€‚

ç”Ÿæˆ Graph embedding çš„ç¬¬ä¸€æ­¥æ˜¯ç”Ÿæˆç‰©å“å…³ç³»å›¾ï¼Œé€šè¿‡ç”¨æˆ·è¡Œä¸ºåºåˆ—å¯ä»¥ç”Ÿæˆç‰©å“ç›¸å…³å›¾ï¼Œåˆ©ç”¨ç›¸åŒå±æ€§ã€ç›¸åŒç±»åˆ«ç­‰ä¿¡æ¯ï¼Œä¹Ÿå¯ä»¥é€šè¿‡è¿™äº›ç›¸ä¼¼æ€§å»ºç«‹ç‰©å“ä¹‹é—´çš„è¾¹ï¼Œä»è€Œç”ŸæˆåŸºäºå†…å®¹çš„ knowledge graphã€‚è€ŒåŸºäº knowledge graph ç”Ÿæˆçš„ç‰©å“å‘é‡å¯ä»¥è¢«ç§°ä¸ºè¡¥å……ä¿¡æ¯ï¼ˆside informationï¼‰embedding å‘é‡ï¼Œå½“ç„¶ï¼Œæ ¹æ®è¡¥å……ä¿¡æ¯ç±»åˆ«çš„ä¸åŒï¼Œå¯ä»¥æœ‰å¤šä¸ª side information embedding å‘é‡ã€‚

é‚£ä¹ˆå¦‚ä½•èåˆä¸€ä¸ªç‰©å“çš„å¤šä¸ª embedding å‘é‡ï¼Œä½¿ä¹‹å½¢æˆç‰©å“æœ€åçš„ embedding å‘¢ï¼Ÿæœ€ç®€å•çš„æ–¹æ³•æ˜¯åœ¨æ·±åº¦ç¥ç»ç½‘ç»œä¸­åŠ å…¥ average pooling å±‚å°†ä¸åŒ embedding å¹³å‡èµ·æ¥ï¼Œé˜¿é‡Œåœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œäº†åŠ å¼ºï¼Œå¯¹æ¯ä¸ª embedding åŠ ä¸Šäº†æƒé‡

## åŠ¨æ€è¯å‘é‡

ä¹‹å‰çš„è¯å‘é‡å‡æ˜¯é™æ€çš„è¯å‘é‡ï¼Œæ— æ³•è§£å†³ä¸€æ¬¡å¤šä¹‰ç­‰é—®é¢˜ã€‚

### ELMO-GPT-Bert

elmo é‡‡ç”¨ LSTM è¿›è¡Œç‰¹å¾æå–, è€Œ GPT å’Œ bert å‡é‡‡ç”¨ Transformer[^1] è¿›è¡Œç‰¹å¾æå–ã€‚
[^1]: Transformer : å‚è§ attension ä¸“é¢˜

GPT é‡‡ç”¨å•å‘è¯­è¨€æ¨¡å‹ï¼Œelmo å’Œ bert é‡‡ç”¨åŒå‘è¯­è¨€æ¨¡å‹ã€‚ä½†æ˜¯ elmo å®é™…ä¸Šæ˜¯ä¸¤ä¸ªå•å‘è¯­è¨€æ¨¡å‹ï¼ˆæ–¹å‘ç›¸åï¼‰çš„æ‹¼æ¥ï¼Œè¿™ç§èåˆç‰¹å¾çš„èƒ½åŠ›æ¯” bert ä¸€ä½“åŒ–èåˆç‰¹å¾æ–¹å¼å¼±ã€‚

GPT å’Œ bert éƒ½é‡‡ç”¨ Transformerï¼ŒTransformer æ˜¯ encoder-decoder ç»“æ„ï¼ŒGPT çš„å•å‘è¯­è¨€æ¨¡å‹é‡‡ç”¨ decoder éƒ¨åˆ†ï¼Œdecoder çš„éƒ¨åˆ†è§åˆ°çš„éƒ½æ˜¯ä¸å®Œæ•´çš„å¥å­ï¼›bert çš„åŒå‘è¯­è¨€æ¨¡å‹åˆ™é‡‡ç”¨ encoder éƒ¨åˆ†ï¼Œé‡‡ç”¨äº†å®Œæ•´å¥å­ã€‚

#### ELMO

Embedding from Language Models

å‡ºè‡ªè®ºæ–‡ `Deep contextualized word representation`, å…³é”®è¯ï¼š åœºæ™¯åŒ–

ä¸»è¦æ€è·¯ï¼š æ ¹æ®å½“å‰ä¸Šä¸‹æ–‡å¯¹ Word Embedding åŠ¨æ€è°ƒæ•´

åŒå±‚åŒå‘ LSTMï¼Œ å¾—åˆ° 3 ä¸ª embedding, ä¸‹æ¸¸ä½¿ç”¨çš„æ—¶å€™å¯¹æƒé‡è¿›è¡Œæ•´åˆã€‚

#### GPT

GPT æ˜¯â€œGenerative Pre-Trainingâ€çš„ç®€ç§°

#### GPT2

æ¨¡å‹å¼ºå¤§åˆ° OpenAI éƒ½ä¸æ•¢å…¬å¸ƒå®Œæ•´æ¨¡å‹ï¼

GPT-2 å‚æ•°è¾¾åˆ°äº† 15 äº¿ä¸ªï¼Œä½¿ç”¨äº†åŒ…å« 800 ä¸‡ä¸ªç½‘é¡µçš„æ•°æ®é›†æ¥è®­ç»ƒï¼Œå…±æœ‰ 40GB

#### Bert

æ¥è‡ªè°·æ­Œ

Bidirectional Encoder Representation from Transformersï¼Œbert çš„æ ¸å¿ƒæ˜¯åŒå‘ Transformer Encoder

BERT Transformer ä½¿ç”¨åŒå‘ self-attentionï¼Œè€Œ GPT Transformer ä½¿ç”¨å—é™åˆ¶çš„ self-attentionï¼Œå…¶ä¸­æ¯ä¸ª token åªèƒ½å¤„ç†å…¶å·¦ä¾§çš„ä¸Šä¸‹æ–‡ã€‚åŒå‘ Transformer é€šå¸¸è¢«ç§°ä¸ºâ€œTransformer encoderâ€ï¼Œè€Œå·¦ä¾§ä¸Šä¸‹æ–‡è¢«ç§°ä¸ºâ€œTransformer decoderâ€ï¼Œdecoder æ˜¯ä¸èƒ½è·è¦é¢„æµ‹çš„ä¿¡æ¯çš„ã€‚

ä¸ºäº†è®­ç»ƒä¸€ä¸ªæ·±åº¦åŒå‘è¡¨ç¤ºï¼ˆdeep bidirectional representationï¼‰ï¼Œç ”ç©¶å›¢é˜Ÿé‡‡ç”¨äº†ä¸€ç§ç®€å•çš„æ–¹æ³•ï¼Œå³éšæœºå±è”½ï¼ˆmaskingï¼‰éƒ¨åˆ†è¾“å…¥ tokenï¼Œç„¶ååªé¢„æµ‹é‚£äº›è¢«å±è”½çš„ tokenã€‚è®ºæ–‡å°†è¿™ä¸ªè¿‡ç¨‹ç§°ä¸ºâ€œmasked LMâ€(MLM)ã€‚

##### Bert åº”ç”¨

- è°·æ­Œ[å®˜æ–¹ä»£ç ](https://github.com/google-research/bert#pre-trained-models)

- [Bert-As-Service](https://github.com/hanxiao/bert-as-service)
  åˆ©ç”¨ ZeroMQ (åˆ†å¸ƒå¼æ¶ˆæ¯) å®ç°çš„å¥å­ç¼–ç æœåŠ¡ï¼Œå…¶ä¸­ï¼Œå¥å­é•¿åº¦å¯å˜ã€‚
  `pip install -U bert-serving-server bert-serving-client`
  ä¸‹è½½ä¸­æ–‡æ¨¡å‹ï¼ˆ364Mï¼Œè§£å‹å 412Mï¼‰
  èµ·æœåŠ¡ `bert-serving-start -model_dir /Users/zhangxisheng/Documents/models/chinese_L-12_H-768_A-12 -num_worker=1`

### XLNet

é€šè¿‡ `permutation loss` çš„æ–¹å¼é¿å…äº† BERT é‡Œé¢ `[MASK]` å¸¦æ¥çš„ä¸Šä¸‹æ–‡çš„æŸå¤±ï¼Œå´åˆèƒ½å…¼é¡¾å·¦å³çš„ä¿¡æ¯

attention mask

## å…¶ä»–

### Time Embedding

ä¸‡ç‰©çš†å¯åµŒå…¥ï¼Œè¿æ—¶é—´ä¹Ÿè¡Œã€‚

[Time2Vec: Learning a Vector Representation of Time](https://www.arxiv-vanity.com/papers/1907.05321/)

## å‚è€ƒ

- [Deep Learning in NLP ï¼ˆä¸€ï¼‰è¯å‘é‡å’Œè¯­è¨€æ¨¡å‹](http://licstar.net/archives/328)
- [Word2vec Parameter Learning Explained (UMich 2016)]()
  by Xin Rong
- [æ·±åº¦å­¦ä¹ ä¸­ä¸å¾—ä¸å­¦çš„ Graph Embedding æ–¹æ³•](https://zhuanlan.zhihu.com/p/64200072)
- - [Learning Word Representations by Jointly Modeling Syntagmatic and Paradigmatic Relations ]()

- [Word Representations via Gaussian Embedding by by Luke Vilnis, et al., 2014](http://arxiv.org/abs/1412.6623)

- [Not All Contexts Are Created Equal: Better Word Representations with Variable Attention](http://www.cs.cmu.edu/~lingwang/papers/emnlp2015-2.pdf)

- [lda2vec](https://github.com/cemoody/lda2vec) by Chris Moody

- [åˆ©ç”¨ n å…ƒç¬”ç”»ä¿¡æ¯ä½œä¸­æ–‡è¯åµŒå…¥](file:///Users/zhangxisheng/Downloads/cw2vec.pdf)
  ä¾‹å¦‚ï¼Œæœ¨æ å’Œ æ£®æ— æ˜¯å…³ç³»æ¯”è¾ƒè¿‘çš„ï¼Œä½†å¦‚æœåªè€ƒè™‘å•å­—ï¼Œåˆ™æ— å…³è”ï¼Œè¿™æ˜¯ä¸åº”è¯¥çš„ã€‚
  å……åˆ†åˆ©ç”¨æ±‰å­ä¸°å¯Œçš„å†…åœ¨ç‰¹å¾

  æ™ºï¼Œå¦‚æœè¢«åˆ†ä¸º çŸ¢ï¼Œå£ï¼Œæ—¥ï¼Œåˆ™ä¸ç›¸å…³ï¼Œä½†æ˜¯å¦‚æœæ°›å›´ çŸ¥ï¼Œæ—¥ï¼Œåˆ™ç›¸å…³æ€§å°±é«˜äº†ã€‚
  å…·ä½“çš„åšæ³•ï¼š å°†ç¬”ç”»åˆ†ä¸º 5 ç±»ï¼Œ

- [Learning Embeddings into Entropic Wasserstein Spaces](https://arxiv.org/abs/1905.03329)

* _[How to Generate a good Word Embeding?](http://arxiv.org/abs/1507.05523)_

  ä½œè€…æœ¬äººè¿˜å†™äº†ä¸€ç¯‡[åšå®¢](http://licstar.net/archives/620)ä½œä¸ºè®ºæ–‡å¯¼è¯»

* ["The Sum of Its Parts": Joint Learning of Word and Phrase Representations with Autoencoders by RÃ©mi Lebret, 2015](http://arxiv.org/abs/1506.05703)
