(window.webpackJsonp=window.webpackJsonp||[]).push([[20],{258:function(t,s,a){t.exports=a.p+"assets/img/2019-12-24-14-22-39.7c2a4e76.png"},259:function(t,s,a){t.exports=a.p+"assets/img/2019-08-12-19-49-33.56356a72.png"},260:function(t,s,a){t.exports=a.p+"assets/img/2019-12-24-14-30-04.973dade2.png"},504:function(t,s,a){"use strict";a.r(s);var e=a(17),n=Object(e.a)({},(function(){var t=this,s=t.$createElement,e=t._self._c||s;return e("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[e("h1",{attrs:{id:"attension-专题-ignore-true"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#attension-专题-ignore-true"}},[t._v("#")]),t._v(" Attension 专题 {ignore=true}")]),t._v(" "),e("p",[t._v("[TOC]")]),t._v(" "),e("blockquote",[e("p",[t._v("细想神毫注意深")])]),t._v(" "),e("p",[t._v("注意力是形成短期记忆的重要环节， 而短期记忆经过回忆确认反复之后才会形成长期记忆。")]),t._v(" "),e("p",[t._v("Attension 大约在 2014 年的时候被引入计算机视觉领域，后来逐渐扩张到了 NLP.")]),t._v(" "),e("p",[t._v("NIPS 2017「Attension is All you Need」论文提出完全用 attention 来做序列转换，抛弃了以往的 CNN 和 RNN 结构，是之后大名鼎鼎的 BERT 的基础。")]),t._v(" "),e("p",[t._v("RNN 结构在语言模型和 Seq2Seq 模型中被广泛使用，但是 RNN 的一个缺陷是无法像 CNN 那样可以并行，这极大的限制了 RNN 的训练.")]),t._v(" "),e("p",[t._v("注意机制的核心问题在于：如何确定在哪里倾注注意力以及如何量化。就好比老师在期末考试复习的时候告诉大家要抓重点，但真正的问题在于重点在哪里。")]),t._v(" "),e("h2",{attrs:{id:"attension"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#attension"}},[t._v("#")]),t._v(" Attension")]),t._v(" "),e("h3",{attrs:{id:"self-attention"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#self-attention"}},[t._v("#")]),t._v(" Self-Attention")]),t._v(" "),e("p",[e("span",{staticClass:"katex-display"},[e("span",{staticClass:"katex"},[e("span",{staticClass:"katex-mathml"},[e("math",[e("semantics",[e("mrow",[e("msub",[e("mrow",[e("mi",{attrs:{mathvariant:"bold"}},[t._v("y")])],1),e("mrow",[e("mi",[t._v("i")])],1)],1),e("mo",[t._v("=")]),e("msub",[e("mo",[t._v("∑")]),e("mrow",[e("mi",[t._v("j")])],1)],1),e("msub",[e("mi",[t._v("w")]),e("mrow",[e("mi",[t._v("i")]),e("mi",[t._v("j")])],1)],1),e("msub",[e("mi",[t._v("x")]),e("mrow",[e("mi",[t._v("j")])],1)],1)],1),e("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("\\mathbf{y}_{i}=\\sum_{j} w_{i j} x_{j}\n")])],1)],1)],1),e("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[e("span",{staticClass:"strut",staticStyle:{height:"1.050005em"}}),e("span",{staticClass:"strut bottom",staticStyle:{height:"2.463782em","vertical-align":"-1.413777em"}}),e("span",{staticClass:"base displaystyle textstyle uncramped"},[e("span",{},[e("span",{staticClass:"mord displaystyle textstyle uncramped"},[e("span",{staticClass:"mord mathbf",staticStyle:{"margin-right":"0.01597em"}},[t._v("y")])]),e("span",{staticClass:"vlist"},[e("span",{staticStyle:{top:"0.24444em","margin-right":"0.05em"}},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),e("span",{staticClass:"reset-textstyle scriptstyle cramped"},[e("span",{staticClass:"mord scriptstyle cramped"},[e("span",{staticClass:"mord mathit"},[t._v("i")])])])]),e("span",{staticClass:"baseline-fix"},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),t._v("​")])])]),e("span",{staticClass:"mrel"},[t._v("=")]),e("span",{staticClass:"mop op-limits"},[e("span",{staticClass:"vlist"},[e("span",{staticStyle:{top:"1.177669em","margin-left":"0em"}},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),e("span",{staticClass:"reset-textstyle scriptstyle cramped"},[e("span",{staticClass:"mord scriptstyle cramped"},[e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.05724em"}},[t._v("j")])])])]),e("span",{staticStyle:{top:"-0.000005000000000032756em"}},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),e("span",[e("span",{staticClass:"op-symbol large-op mop"},[t._v("∑")])])]),e("span",{staticClass:"baseline-fix"},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),t._v("​")])])]),e("span",{staticClass:"mord"},[e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.02691em"}},[t._v("w")]),e("span",{staticClass:"vlist"},[e("span",{staticStyle:{top:"0.15em","margin-right":"0.05em","margin-left":"-0.02691em"}},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),e("span",{staticClass:"reset-textstyle scriptstyle cramped"},[e("span",{staticClass:"mord scriptstyle cramped"},[e("span",{staticClass:"mord mathit"},[t._v("i")]),e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.05724em"}},[t._v("j")])])])]),e("span",{staticClass:"baseline-fix"},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),t._v("​")])])]),e("span",{staticClass:"mord"},[e("span",{staticClass:"mord mathit"},[t._v("x")]),e("span",{staticClass:"vlist"},[e("span",{staticStyle:{top:"0.15em","margin-right":"0.05em","margin-left":"0em"}},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),e("span",{staticClass:"reset-textstyle scriptstyle cramped"},[e("span",{staticClass:"mord scriptstyle cramped"},[e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.05724em"}},[t._v("j")])])])]),e("span",{staticClass:"baseline-fix"},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),t._v("​")])])])])])])])]),t._v(" "),e("p",[t._v("其中，权重的取法可以是")]),t._v(" "),e("p",[e("span",{staticClass:"katex-display"},[e("span",{staticClass:"katex"},[e("span",{staticClass:"katex-mathml"},[e("math",[e("semantics",[e("mrow",[e("msubsup",[e("mi",[t._v("w")]),e("mrow",[e("mi",[t._v("i")]),e("mi",[t._v("j")])],1),e("mrow",[e("mi",{attrs:{mathvariant:"normal"}},[t._v("′")])],1)],1),e("mo",[t._v("=")]),e("msubsup",[e("mi",[t._v("x")]),e("mrow",[e("mi",[t._v("i")])],1),e("mrow",[e("mi",{attrs:{mathvariant:"normal"}},[t._v("⊤")])],1)],1),e("msub",[e("mi",[t._v("x")]),e("mrow",[e("mi",[t._v("j")])],1)],1)],1),e("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("w_{i j}^{\\prime}=x_{i}^{\\top} x_{j}\n")])],1)],1)],1),e("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[e("span",{staticClass:"strut",staticStyle:{height:"0.8991079999999998em"}}),e("span",{staticClass:"strut bottom",staticStyle:{height:"1.2822159999999998em","vertical-align":"-0.383108em"}}),e("span",{staticClass:"base displaystyle textstyle uncramped"},[e("span",{staticClass:"mord"},[e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.02691em"}},[t._v("w")]),e("span",{staticClass:"vlist"},[e("span",{staticStyle:{top:"0.24700000000000003em","margin-left":"-0.02691em","margin-right":"0.05em"}},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),e("span",{staticClass:"reset-textstyle scriptstyle cramped"},[e("span",{staticClass:"mord scriptstyle cramped"},[e("span",{staticClass:"mord mathit"},[t._v("i")]),e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.05724em"}},[t._v("j")])])])]),e("span",{staticStyle:{top:"-0.41300000000000003em","margin-right":"0.05em"}},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),e("span",{staticClass:"reset-textstyle scriptstyle uncramped"},[e("span",{staticClass:"mord scriptstyle uncramped"},[e("span",{staticClass:"mord mathrm"},[t._v("′")])])])]),e("span",{staticClass:"baseline-fix"},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),t._v("​")])])]),e("span",{staticClass:"mrel"},[t._v("=")]),e("span",{staticClass:"mord"},[e("span",{staticClass:"mord mathit"},[t._v("x")]),e("span",{staticClass:"vlist"},[e("span",{staticStyle:{top:"0.247em","margin-left":"0em","margin-right":"0.05em"}},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),e("span",{staticClass:"reset-textstyle scriptstyle cramped"},[e("span",{staticClass:"mord scriptstyle cramped"},[e("span",{staticClass:"mord mathit"},[t._v("i")])])])]),e("span",{staticStyle:{top:"-0.4129999999999999em","margin-right":"0.05em"}},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),e("span",{staticClass:"reset-textstyle scriptstyle uncramped"},[e("span",{staticClass:"mord scriptstyle uncramped"},[e("span",{staticClass:"mord mathrm"},[t._v("⊤")])])])]),e("span",{staticClass:"baseline-fix"},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),t._v("​")])])]),e("span",{staticClass:"mord"},[e("span",{staticClass:"mord mathit"},[t._v("x")]),e("span",{staticClass:"vlist"},[e("span",{staticStyle:{top:"0.15em","margin-right":"0.05em","margin-left":"0em"}},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),e("span",{staticClass:"reset-textstyle scriptstyle cramped"},[e("span",{staticClass:"mord scriptstyle cramped"},[e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.05724em"}},[t._v("j")])])])]),e("span",{staticClass:"baseline-fix"},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),t._v("​")])])])])])])])]),t._v(" "),e("p",[t._v("当然，可以先将权重 softmax 归一化。")]),t._v(" "),e("p",[t._v("例如，"),e("code",[t._v("the cat walks on the street")])]),t._v(" "),e("p",[t._v("这里也可以看到，self-attension 将输入视为集合，而不是序列，即 permutation equivariant.")]),t._v(" "),e("div",{staticClass:"language-python extra-class"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[t._v("raw_weights "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("bmm"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" x"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("transpose"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nweights "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" F"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("softmax"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("raw_weights"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" dim"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ny "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("bmm"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("weights"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" x"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),e("p",[t._v("多头： 多个头权重矩阵。")]),t._v(" "),e("p",[e("img",{attrs:{src:a(258),alt:""}})]),t._v(" "),e("p",[e("span",{staticClass:"katex-display"},[e("span",{staticClass:"katex"},[e("span",{staticClass:"katex-mathml"},[e("math",[e("semantics",[e("mrow",[e("msubsup",[e("mi",[t._v("w")]),e("mrow",[e("mi",[t._v("i")]),e("mi",[t._v("j")])],1),e("mrow",[e("mi",{attrs:{mathvariant:"normal"}},[t._v("′")])],1)],1),e("mo",[t._v("=")]),e("mfrac",[e("mrow",[e("msubsup",[e("mrow",[e("mi",{attrs:{mathvariant:"bold"}},[t._v("q")])],1),e("mrow",[e("mi",[t._v("i")])],1),e("mrow",[e("mi",{attrs:{mathvariant:"normal"}},[t._v("⊤")])],1)],1),e("msub",[e("mrow",[e("mi",{attrs:{mathvariant:"bold"}},[t._v("k")])],1),e("mrow",[e("mi",[t._v("j")])],1)],1)],1),e("mrow",[e("msqrt",[e("mrow",[e("mi",[t._v("k")])],1)],1)],1)],1)],1),e("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("w_{i j}^{\\prime}=\\frac{\\mathbf{q}_{i}^{\\top} \\mathbf{k}_{j}}{\\sqrt{k}}\n")])],1)],1)],1),e("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[e("span",{staticClass:"strut",staticStyle:{height:"1.526108em"}}),e("span",{staticClass:"strut bottom",staticStyle:{height:"2.4561080000000004em","vertical-align":"-0.9300000000000002em"}}),e("span",{staticClass:"base displaystyle textstyle uncramped"},[e("span",{staticClass:"mord"},[e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.02691em"}},[t._v("w")]),e("span",{staticClass:"vlist"},[e("span",{staticStyle:{top:"0.24700000000000003em","margin-left":"-0.02691em","margin-right":"0.05em"}},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),e("span",{staticClass:"reset-textstyle scriptstyle cramped"},[e("span",{staticClass:"mord scriptstyle cramped"},[e("span",{staticClass:"mord mathit"},[t._v("i")]),e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.05724em"}},[t._v("j")])])])]),e("span",{staticStyle:{top:"-0.41300000000000003em","margin-right":"0.05em"}},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),e("span",{staticClass:"reset-textstyle scriptstyle uncramped"},[e("span",{staticClass:"mord scriptstyle uncramped"},[e("span",{staticClass:"mord mathrm"},[t._v("′")])])])]),e("span",{staticClass:"baseline-fix"},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),t._v("​")])])]),e("span",{staticClass:"mrel"},[t._v("=")]),e("span",{staticClass:"mord reset-textstyle displaystyle textstyle uncramped"},[e("span",{staticClass:"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"}),e("span",{staticClass:"mfrac"},[e("span",{staticClass:"vlist"},[e("span",{staticStyle:{top:"0.8222200000000002em"}},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"1em"}},[t._v("​")])]),e("span",{staticClass:"reset-textstyle textstyle cramped"},[e("span",{staticClass:"mord textstyle cramped"},[e("span",{staticClass:"sqrt mord"},[e("span",{staticClass:"sqrt-sign",staticStyle:{top:"-0.09221999999999997em"}},[e("span",{staticClass:"style-wrap reset-textstyle textstyle uncramped"},[t._v("√")])]),e("span",{staticClass:"vlist"},[e("span",{staticStyle:{top:"0em"}},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"1em"}},[t._v("​")])]),e("span",{staticClass:"mord textstyle cramped"},[e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.03148em"}},[t._v("k")])])]),e("span",{staticStyle:{top:"-0.85222em"}},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"1em"}},[t._v("​")])]),e("span",{staticClass:"reset-textstyle textstyle uncramped sqrt-line"})]),e("span",{staticClass:"baseline-fix"},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"1em"}},[t._v("​")])]),t._v("​")])])])])])]),e("span",{staticStyle:{top:"-0.22999999999999998em"}},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"1em"}},[t._v("​")])]),e("span",{staticClass:"reset-textstyle textstyle uncramped frac-line"})]),e("span",{staticStyle:{top:"-0.677em"}},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"1em"}},[t._v("​")])]),e("span",{staticClass:"reset-textstyle textstyle uncramped"},[e("span",{staticClass:"mord textstyle uncramped"},[e("span",{},[e("span",{staticClass:"mord textstyle uncramped"},[e("span",{staticClass:"mord mathbf"},[t._v("q")])]),e("span",{staticClass:"vlist"},[e("span",{staticStyle:{top:"0.258664em","margin-right":"0.05em"}},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),e("span",{staticClass:"reset-textstyle scriptstyle cramped"},[e("span",{staticClass:"mord scriptstyle cramped"},[e("span",{staticClass:"mord mathit"},[t._v("i")])])])]),e("span",{staticStyle:{top:"-0.363em","margin-right":"0.05em"}},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),e("span",{staticClass:"reset-textstyle scriptstyle uncramped"},[e("span",{staticClass:"mord scriptstyle uncramped"},[e("span",{staticClass:"mord mathrm"},[t._v("⊤")])])])]),e("span",{staticClass:"baseline-fix"},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),t._v("​")])])]),e("span",{},[e("span",{staticClass:"mord textstyle uncramped"},[e("span",{staticClass:"mord mathbf"},[t._v("k")])]),e("span",{staticClass:"vlist"},[e("span",{staticStyle:{top:"0.15em","margin-right":"0.05em"}},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),e("span",{staticClass:"reset-textstyle scriptstyle cramped"},[e("span",{staticClass:"mord scriptstyle cramped"},[e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.05724em"}},[t._v("j")])])])]),e("span",{staticClass:"baseline-fix"},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),t._v("​")])])])])])]),e("span",{staticClass:"baseline-fix"},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"1em"}},[t._v("​")])]),t._v("​")])])]),e("span",{staticClass:"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"})])])])])])]),t._v(" "),e("p",[t._v("软对齐模型(soft alignment models) 和硬对齐模型(hard alignment models)")]),t._v(" "),e("p",[t._v("Global （Soft）和 Local （Hard） Attention")]),t._v(" "),e("p",[t._v("硬注意机制每次只注意一个地方，而软注意机制只在一个地方倾注相对更多的注意而已。因此，前者需要随机抽样，需要的计算量相对少一些，而后者是确定性的，虽然计算量大一点，但可以用后向传播训练并因此容易嵌入其他网络。")]),t._v(" "),e("p",[t._v("图片描述和 MT 的区别：前者存在一个粒度(granularity)问题。比如，应该说图片中有一只猫，还是说有一只看起来很慵懒的眯着眼睛的大花猫？")]),t._v(" "),e("p",[t._v("和 显著图 (Saliency Map) 的区别和联系:")]),t._v(" "),e("aside",{staticClass:"key-point"},[t._v("\nbottom up saliency, top down attention\n")]),t._v(" "),e("p",[t._v("2017 Google 「Attension is All You Need」")]),t._v(" "),e("p",[t._v("本质上来说，都是抛弃了 RNN 结构来做 Seq2Seq 任务。")]),t._v(" "),e("p",[t._v("NLP 领域一般都采用基于 RNN 的 Attension 机制，")]),t._v(" "),e("h3",{attrs:{id:"基于-rnn-的注意力机制"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#基于-rnn-的注意力机制"}},[t._v("#")]),t._v(" 基于 RNN 的注意力机制")]),t._v(" "),e("h3",{attrs:{id:"multi-head-attension"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#multi-head-attension"}},[t._v("#")]),t._v(" Multi-Head Attension")]),t._v(" "),e("h3",{attrs:{id:"self-attension"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#self-attension"}},[t._v("#")]),t._v(" Self Attension")]),t._v(" "),e("h3",{attrs:{id:"position-embedding"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#position-embedding"}},[t._v("#")]),t._v(" Position Embedding")]),t._v(" "),e("h3",{attrs:{id:"position-encoding"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#position-encoding"}},[t._v("#")]),t._v(" Position Encoding")]),t._v(" "),e("h3",{attrs:{id:"atrous-self-attention"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#atrous-self-attention"}},[t._v("#")]),t._v(" Atrous Self Attention")]),t._v(" "),e("div",{attrs:{align:"center"}},[e("figure",{attrs:{align:"center"}},[e("img",{staticStyle:{width:"500px"},attrs:{src:a(259)}}),t._v(" "),e("figcaption",[t._v("Atrous Self Attention的注意力矩阵（左）和关联图示（右）.")])])]),t._v(" "),e("h2",{attrs:{id:"adversarial-attention"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#adversarial-attention"}},[t._v("#")]),t._v(" Adversarial Attention")]),t._v(" "),e("h2",{attrs:{id:"transformer"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#transformer"}},[t._v("#")]),t._v(" Transformer")]),t._v(" "),e("p",[e("a",{attrs:{href:"http://www.peterbloem.nl/blog/transformers",target:"_blank",rel:"noopener noreferrer"}},[t._v("TRANSFORMERS FROM SCRATCH"),e("OutboundLink")],1)]),t._v(" "),e("blockquote",[e("p",[t._v("Transformer = Attension in NLP")])]),t._v(" "),e("p",[t._v("构建在已有的 Encoder-Decoder 框架下")]),t._v(" "),e("p",[t._v("A transformer is not just a self-attention layer, it is an architecture")]),t._v(" "),e("p",[t._v("Normalization and residual connections are standard tricks used to help deep neural networks train faster and more accurately. The layer normalization is applied over the embedding dimension only.")]),t._v(" "),e("p",[e("img",{attrs:{src:a(260),alt:""}})]),t._v(" "),e("p",[e("strong",[t._v("position embeddings")])]),t._v(" "),e("p",[e("strong",[t._v("position encodings")])]),t._v(" "),e("h2",{attrs:{id:"工具和应用"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#工具和应用"}},[t._v("#")]),t._v(" 工具和应用")]),t._v(" "),e("ul",[e("li",[e("p",[e("a",{attrs:{href:"http://arxiv.org/abs/1406.6247",target:"_blank",rel:"noopener noreferrer"}},[t._v("Recurrent Models for Visual Attention"),e("OutboundLink")],1)])]),t._v(" "),e("li",[e("p",[e("a",{attrs:{href:"http://arxiv.org/abs/1409.0473",target:"_blank",rel:"noopener noreferrer"}},[t._v("Neural Machine Translation by Jointly Learning to Align and Translate by Dzmitry Bahdanau, et al., ICLR 2015."),e("OutboundLink")],1)]),t._v(" "),e("p",[t._v("这篇文章应该是最早将注意机制从视觉引入自然语言处理的，Yoshua Bengio 也是作者之一。我们知道，基于深度学习的机器语言翻译采用的是 "),e("em",[t._v("编码-解码")]),t._v(" 的方式，通过对解码端引入视觉机制，减轻了编码端的压力。")])]),t._v(" "),e("li",[e("p",[e("a",{attrs:{href:"http://arxiv.org/abs/1503.01838",target:"_blank",rel:"noopener noreferrer"}},[t._v("Encoding Source Language with Convolutional Neural Network for Machine Translation by Fandong Meng, et al., 中科院"),e("OutboundLink")],1)]),t._v(" "),e("p",[t._v("也是机器翻译领域的。")])]),t._v(" "),e("li",[e("p",[e("a",{attrs:{href:"http://arxiv.org/abs/1508.04025",target:"_blank",rel:"noopener noreferrer"}},[t._v("Effective Approaches to Attention-based Neural Machine Translation by Minh-Thang Luong, et al., EMNLP 2015, camera-ready version"),e("OutboundLink")],1)]),t._v(" "),e("p",[t._v("集成了全局和局部两种注意机制后得到的结果更好。")])]),t._v(" "),e("li",[e("p",[e("a",{attrs:{href:"http://arxiv.org/abs/1509.00685",target:"_blank",rel:"noopener noreferrer"}},[t._v("A Neural Attention Model for Abstractive Sentence Summarization by Alexander M.Rush, et al., 2015"),e("OutboundLink")],1)]),t._v(" "),e("p",[t._v("首次将注意机制引入了句子摘要问题。")])]),t._v(" "),e("li",[e("p",[e("a",{attrs:{href:"http://arxiv.org/abs/1502.04623",target:"_blank",rel:"noopener noreferrer"}},[t._v("DRAW: A Recurrent Neural Network for Image Generation by Karol Gregor, et al., Google, 2015"),e("OutboundLink")],1)]),t._v(" "),e("p",[t._v("提出了一种被称作 "),e("em",[t._v("DRAW")]),t._v("(deep recurrent attentive writer) 的用以生成图片的网络架构。主要的一个观察：人在作画的时候也不是一笔挥就一蹴而成，而是经过不断的调整、修改细节才最终成型。因此，DRAW 每次会注意到某个部分，将之修改. 问题在于，如何确定这个注意部分呢？\n和传统的注意机制的不同之处：传统的注意机制中，注意是在解码阶段被引入的，并且不会去影响到编码阶段，而 DRAW")])]),t._v(" "),e("li",[e("p",[e("a",{attrs:{href:"http://arxiv.org/abs/1506.03340",target:"_blank",rel:"noopener noreferrer"}},[t._v("Teaching Machines to Read and Comprehend by Karl Moritz Hermann, et al.,Google, NIPS 2015, to appear"),e("OutboundLink")],1)]),t._v(" "),e("p",[t._v("教会机器阅读理解。构建了一个 "),e("em",[t._v("（（文档，提问），答案）")]),t._v(" 数据集。两个模型，"),e("em",[t._v("attentive reader")]),t._v(" 和 "),e("em",[t._v("impatient reader")]),t._v(". 后者需要一遍又一遍地重读文档，")])]),t._v(" "),e("li",[e("p",[e("a",{attrs:{href:"http://arxiv.org/abs/1509.06812",target:"_blank",rel:"noopener noreferrer"}},[t._v("Learning Wake-Sleep Recurrent Attention Models by Jimmy Ba, 2015"),e("OutboundLink")],1)]),t._v(" "),e("p",[t._v("Ruslan Salakhutdinov 也是作者之一。采用了硬注意机制(hard attention mechanism)，")])]),t._v(" "),e("li",[e("p",[e("a",{attrs:{href:"http://www.cs.cmu.edu/~lingwang/papers/emnlp2015-2.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Not All Contexts Are Created Equal: Better Word Representations with Variable Attention by Wang Lin, et al."),e("OutboundLink")],1)]),t._v(" "),e("p",[t._v("不得不说，文章名称非常有吸引力，")])]),t._v(" "),e("li",[e("p",[e("a",{attrs:{href:"http://arxiv.org/abs/1502.03044",target:"_blank",rel:"noopener noreferrer"}},[t._v("Show, Attend and Tell: Neural Image Caption Generation with Visual Attention by Kelvin Xu, et al., 2015"),e("OutboundLink")],1)])])]),t._v(" "),e("ul",[e("li",[e("p",[e("a",{attrs:{href:"http://www.shuwu.name/sw/Predicting%20the%20Next%20Location%20A%20Recurrent%20Model%20with%20Spatial%20and%20Temporal%20Contexts.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Predicting the Next Location: A Recurrent Model with Spatial and Temporal\nContexts by Qiang LIu, et al., 中科院"),e("OutboundLink")],1)]),t._v(" "),e("div",{staticClass:"language- extra-class"},[e("pre",[e("code",[t._v("时空上下文。\n")])])])])]),t._v(" "),e("ul",[e("li",[t._v("spatial transformer networks 空间变换网络\nspatial transformer ： affine transformasion ，projective transformation、 thin plate spline（薄板样条）")])]),t._v(" "),e("ul",[e("li",[e("a",{attrs:{href:"https://github.com//WarBean/tps_stn_pytorch",target:"_blank",rel:"noopener noreferrer"}},[t._v("PyTorch implementation of Spatial Transformer Network (STN) with Thin Plate Spline (TPS)"),e("OutboundLink")],1)])]),t._v(" "),e("h2",{attrs:{id:"参考"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#参考"}},[t._v("#")]),t._v(" 参考")]),t._v(" "),e("ul",[e("li",[e("a",{attrs:{href:"http://yanran.li/peppypapers/2015/10/07/survey-attention-model-1.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("Survey on Attension-based Models Applied in NLP, 2015.10.07"),e("OutboundLink")],1)]),t._v(" "),e("li",[e("a",{attrs:{href:"http://yanran.li/peppypapers/2015/10/07/survey-attention-model-1.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("Survey on Attention-based Models Applied in NLP"),e("OutboundLink")],1)])])])}),[],!1,null,null,null);s.default=n.exports}}]);