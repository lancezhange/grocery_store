(window.webpackJsonp=window.webpackJsonp||[]).push([[136],{571:function(a,e,t){"use strict";t.r(e);var r=t(17),s=Object(r.a)({},(function(){var a=this,e=a.$createElement,t=a._self._c||e;return t("ContentSlotsDistributor",{attrs:{"slot-key":a.$parent.slotKey}},[t("h2",{attrs:{id:"vae"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#vae"}},[a._v("#")]),a._v(" VAE")]),a._v(" "),t("p",[a._v("变分自编码器 和 普通的自编码器 的联系和区别")]),a._v(" "),t("h3",{attrs:{id:"ae"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#ae"}},[a._v("#")]),a._v(" AE")]),a._v(" "),t("p",[a._v("变分推断(variational Inference)")]),a._v(" "),t("ul",[t("li",[t("p",[t("a",{attrs:{href:"http://shakirm.com/papers/VITutorial.pdf",target:"_blank",rel:"noopener noreferrer"}},[a._v("Variational Inference for Machine Learning"),t("OutboundLink")],1)]),a._v(" "),t("p",[a._v("出自 Shakir Mohamed from Google DeepMind.")])])]),a._v(" "),t("p",[a._v("变分方法\nVariational bounds")]),a._v(" "),t("h3",{attrs:{id:"vae-2"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#vae-2"}},[a._v("#")]),a._v(" VAE")]),a._v(" "),t("p",[a._v("Denoising Autoencoder(DAE)是在 AE 的基础之上，对输入的训练数据加入噪声。所以 DAE 必须学习去除这些噪声而获得真正的没有被噪声污染过的输入数据。因此，这就迫使编码器去学习输入数据的更加鲁棒的表达，通常 DAE 的泛化能力比一般的 AE 强")]),a._v(" "),t("h3",{attrs:{id:"sdae"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#sdae"}},[a._v("#")]),a._v(" SDAE")]),a._v(" "),t("p",[a._v("Stacked Denoising Autoencoder(SDAE)是一个多层的 AE 组成的神经网络，其前一层自编码器的输出作为其后一层自编码器的输入。")]),a._v(" "),t("h3",{attrs:{id:"vq-vae"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#vq-vae"}},[a._v("#")]),a._v(" VQ_VAE")]),a._v(" "),t("p",[a._v("生成出的图像，号称比 BigGAN 更加高清逼真，而且更具有多样性！")]),a._v(" "),t("h2",{attrs:{id:"神经变分推断"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#神经变分推断"}},[a._v("#")]),a._v(" 神经变分推断")])])}),[],!1,null,null,null);e.default=s.exports}}]);