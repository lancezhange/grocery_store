(window.webpackJsonp=window.webpackJsonp||[]).push([[94],{503:function(r,t,e){"use strict";e.r(t);var o=e(17),a=Object(o.a)({},(function(){var r=this,t=r.$createElement,e=r._self._c||t;return e("ContentSlotsDistributor",{attrs:{"slot-key":r.$parent.slotKey}},[e("h1",{attrs:{id:"bert"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#bert"}},[r._v("#")]),r._v(" Bert")]),r._v(" "),e("p"),e("div",{staticClass:"table-of-contents"},[e("ul",[e("li",[e("a",{attrs:{href:"#模型迭代"}},[r._v("模型迭代")]),e("ul",[e("li",[e("a",{attrs:{href:"#bert"}},[r._v("Bert")])]),e("li",[e("a",{attrs:{href:"#albert"}},[r._v("ALBert")])]),e("li",[e("a",{attrs:{href:"#roberta"}},[r._v("RoBERTa")])]),e("li",[e("a",{attrs:{href:"#tinybert"}},[r._v("Tinybert")])]),e("li",[e("a",{attrs:{href:"#reformer"}},[r._v("Reformer")])])])])])]),e("p"),r._v(" "),e("br"),r._v(" "),e("section",{staticStyle:{"margin-bottom":"-16px"}},[e("section",{staticStyle:{"margin-top":"0px","margin-right":"0px","margin-bottom":"0px","margin-left":"2em","padding-top":"2px","padding-right":"1em","padding-bottom":"2px","padding-left":"1em","max-width":"100%",display:"inline-block","background-image":"none","background-color":"rgb(196, 212, 218)",color:"rgb(61, 88, 98)","font-size":"16px","text-align":"center","letter-spacing":"1.5px","line-height":"1.75em","border-top-left-radius":"16px","border-top-right-radius":"16px","border-bottom-right-radius":"16px","border-bottom-left-radius":"16px","box-sizing":"border-box","word-wrap":"break-word"}},[e("strong",[r._v("前言")])])]),r._v(" "),e("section",{staticStyle:{"margin-top":"0px","margin-right":"0px","margin-bottom":"20px","margin-left":"0px","padding-top":"2.5em","padding-right":"1em","padding-bottom":"1em","padding-left":"1em","max-width":"100%","box-sizing":"border-box","border-top-width":"1px","border-right-width":"1px","border-bottom-width":"1px","border-left-width":"1px","border-top-style":"solid","border-right-style":"solid","border-bottom-style":"solid","border-left-style":"solid","border-top-color":"rgb(196, 212, 218)","border-right-color":"rgb(196, 212, 218)","border-bottom-color":"rgb(196, 212, 218)","border-left-color":"rgb(196, 212, 218)","border-top-left-radius":"10px","border-top-right-radius":"10px","border-bottom-right-radius":"10px","border-bottom-left-radius":"10px","word-wrap":"break-word"}},[r._v("Bert 在 NLP 发展历史上是一个划时代的存在，因此，这里将 Bert 从 NLP 一章中单拿出来学习。\n")]),r._v(" "),e("br"),r._v(" "),e("h2",{attrs:{id:"模型迭代"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#模型迭代"}},[r._v("#")]),r._v(" 模型迭代")]),r._v(" "),e("h3",{attrs:{id:"bert-2"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#bert-2"}},[r._v("#")]),r._v(" Bert")]),r._v(" "),e("h3",{attrs:{id:"albert"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#albert"}},[r._v("#")]),r._v(" ALBert")]),r._v(" "),e("p",[r._v("A lite version of Bert.")]),r._v(" "),e("p",[r._v("主要是通过矩阵分解和跨层参数共享来做到对参数量的减少，除此以外也是用 "),e("strong",[r._v("SOP（Sentence Order Prediction）")]),r._v(" 替换了 "),e("strong",[r._v("NSP（Next Sentence Prediction）")]),r._v("，并且停用了 Dropout，最后 GLUE 上一举达到了 SOTA 的效果")]),r._v(" "),e("h3",{attrs:{id:"roberta"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#roberta"}},[r._v("#")]),r._v(" RoBERTa")]),r._v(" "),e("h3",{attrs:{id:"tinybert"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#tinybert"}},[r._v("#")]),r._v(" Tinybert")]),r._v(" "),e("h3",{attrs:{id:"reformer"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#reformer"}},[r._v("#")]),r._v(" Reformer")]),r._v(" "),e("p",[r._v("使用局部敏感哈希的点乘注意力将 transorfmer 中的 attention 复杂度从 O(N^2 ) 变为 O(N log N)，将算法复杂度降低，得到了更高的效率。")])])}),[],!1,null,null,null);t.default=a.exports}}]);