(window.webpackJsonp=window.webpackJsonp||[]).push([[44],{350:function(e,r,a){e.exports=a.p+"assets/img/2019-06-15-10-44-28.9563a6af.png"},547:function(e,r,a){"use strict";a.r(r);var t=a(17),n=Object(t.a)({},(function(){var e=this,r=e.$createElement,t=e._self._c||r;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("h1",{attrs:{id:"多模型多任务专题-ignore-ture"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#多模型多任务专题-ignore-ture"}},[e._v("#")]),e._v(" 多模型多任务专题 {ignore=ture}")]),e._v(" "),t("p",[e._v("[TOC]")]),e._v(" "),t("h2",{attrs:{id:"多模态"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#多模态"}},[e._v("#")]),e._v(" 多模态")]),e._v(" "),t("p",[e._v("多模态指将文本与图像/音频/视频等结合")]),e._v(" "),t("ul",[t("li",[t("p",[t("a",{attrs:{href:"http://arxiv.org/abs/1411.2539",target:"_blank",rel:"noopener noreferrer"}},[e._v("Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models, by Ryan Kiros, et al. 2014"),t("OutboundLink")],1)]),e._v(" "),t("p",[e._v("先八卦几句。作者 Ryan Kiros 是 Toronto 大学博士，导师为 Ruslan Salakhutdinov （2016 年就要去 CMU 了）和 Richard Zemel.\n他们以及 YuKun Zhu(本硕在上交) 等人构成了自然语言处理和计算机视觉领域的 Toronto 帮派。Ryan Kiros 本人在 github 上的 3 个"),t("a",{attrs:{href:"https://github.com/ryankiros?tab=repositories",target:"_blank",rel:"noopener noreferrer"}},[e._v("代码库"),t("OutboundLink")],1),e._v("（分别是 neural-storyteller, skip-thoughts, bisual-semantic-embedding）均获得了大量的赞.后者正是该文的伴随代码。")]),e._v(" "),t("p",[e._v("该文的一大惊艳成果 "),t("code",[e._v('_image of a blue car_ - "blue" + "red" is near images of red cars.')])])]),e._v(" "),t("li",[t("p",[t("a",{attrs:{href:"http://www.iro.umontreal.ca/~bengioy/cifar/NCAP2014-summerschool/slides/ryan_kiros_cifar2014kiros.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("多模态的语言学规律(multimodal linguistic regularities)"),t("OutboundLink")],1)])]),e._v(" "),t("li",[t("p",[t("a",{attrs:{href:"http://arxiv.org/abs/1511.06361",target:"_blank",rel:"noopener noreferrer"}},[e._v("Order-Embeddings of Images and Language by Ivan Vendrov, et al., 2015, ICLR 2016, under review"),t("OutboundLink")],1)]),e._v(" "),t("p",[e._v("图片和语言的有序嵌入. 上位词、文本蕴含、图片描述，其实都可以视为在建模层次结构。该层次结构又有明显的偏序结构，可以用 "),t("em",[e._v("偏序表示")]),e._v(" 来刻画，本文提出了一种获得偏序表示的方法。")]),e._v(" "),t("p",[e._v("偏序结构并不具有对称性，所以作者认为嵌入空间中使用对称的相似距离测度（例如欧几里得距离）会带来系统性误差。")]),e._v(" "),t("p",[e._v("一般嵌入的时候，都要求嵌入映射是保距的，因为我们希望原空间中相似的物体在嵌入空间中也相似，但作者认为，在建模偏序结构时，"),t("em",[e._v("与其保距，不如保序")]),e._v("。")])])]),e._v(" "),t("ul",[t("li",[t("p",[t("a",{attrs:{href:"http://arxiv.org/abs/1504.06063",target:"_blank",rel:"noopener noreferrer"}},[e._v("Multimodal Convolutional Neural Networks for Matching Image and Sentence by Lin Ma, et al., ICCV 2015"),t("OutboundLink")],1)]),e._v(" "),t("p",[e._v("提出了多模态卷积神经网络模型(m-CNNs)，用以图片和句子的匹配。")])])]),e._v(" "),t("h2",{attrs:{id:"多任务"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#多任务"}},[e._v("#")]),e._v(" 多任务")]),e._v(" "),t("p",[e._v("Multi-Task Learning")]),e._v(" "),t("p",[e._v("多任务学习 Multi-task Learning 有时也称作联合学习、learning to learn、带辅助任务的学习。")]),e._v(" "),t("p",[e._v("在深度学习中，多任务学习通过隐藏层 hard 或者 soft 参数共享来实现。")]),e._v(" "),t("p",[t("strong",[e._v("Hard Parameter Sharing")]),e._v("\nHard parameter sharing 是神经网络中使用 MTL 的最常见的方法。通常通过所有任务中共用隐藏层，而针对不同任务使用多个输出层来实现。")]),e._v(" "),t("p",[t("strong",[e._v("Soft Parameter Sharing")])]),e._v(" "),t("p",[e._v("在软参数共享中，每个任务都有单独的模型，每个模型包含各自的参数。模型参数之间的距离会作为正则项来保证参数尽可能相似")]),e._v(" "),t("h3",{attrs:{id:"esmm"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#esmm"}},[e._v("#")]),e._v(" ESMM")]),e._v(" "),t("p",[e._v("完整空间多任务模型.")]),e._v(" "),t("p",[t("code",[e._v("Entire Space Multi-Task Model: An Eﬀective Approach for Estimating Post-Click Conversion Rate")]),e._v(", SIGIR18\n在完整的样本数据空间"),t("strong",[e._v("同时学习点击率和转化率")])]),e._v("\n\\underbrace { p ( y = 1 , z = 1 | \\boldsymbol { x } ) } _ { p \\subset T C V R } = \\underbrace { p ( y = 1 | \\boldsymbol { x } ) } _ { p C T R } \\times \\underbrace { p ( z = 1 | y = 1 , \\boldsymbol { x } ) } _ { p C V R }\n\n"),t("p",[e._v("该模型主要解决的是 CVR 预估中的两个主要问题：样本选择偏差和稀疏数据。")]),e._v(" "),t("p",[e._v("同时解决了"),t("strong",[e._v("训练空间和预测空间不一致")]),e._v("以及"),t("strong",[e._v("同时利用点击和转化数据进行全局优化")]),e._v("两个关键的问题。")]),e._v(" "),t("p",[e._v("样本选择偏差\n: 大多数 CVR 预估问题是 8*在用户点击过的样本空间上进行训练**的，而预测的时候却要对整个样本空间的样本进行预测。这种训练样本从整体样本空间的一个较小子集中提取，而训练得到的模型却需要对整个样本空间中的样本做推断预测的现象称之为样本选择偏差。")]),e._v(" "),t("p",[e._v("数据稀疏\n: 用户点击过的物品只占整个样本空间的很小一部分，使得模型训练十分困难")]),e._v(" "),t("p",[t("img",{attrs:{src:a(350),alt:""}})]),e._v(" "),t("p",[e._v("对于一个给定的展现，ESMM 模型能够同时输出预估的 pCTR、pCVR 和 pCTCVR")]),e._v(" "),t("p",[e._v("从模型结构上看，最底层的 embedding 层是 CVR 部分和 CTR 部分共享的，共享 Embedding 层的目的主要是解决 CVR 任务正样本稀疏的问题，利用 CTR 的数据生成用户（user）和物品（item）更准确的特征表达。\n中间层是 CVR 部分和 CTR 部分各自利用完全隔离的神经网络拟合自己的优化目标，pCVR 和 pCTR。最终，将 pCVR 和 pCTR 相乘得到 pCTCVR。")]),e._v("\n\\underbrace { p ( y = 1 , z = 1 | x ) } _ { p C T C V R } = \\underbrace { p ( y = 1 | x ) } _ { p C T R } \\times \\underbrace { p ( z = 1 | y = 1 , x ) } _ { p C V R }\n\n"),t("h3",{attrs:{id:"_2018-dicm"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2018-dicm"}},[e._v("#")]),e._v(" 2018-DICM")]),e._v(" "),t("p",[e._v("DICM, Deep Image CTR Model, CIKM’18\nImage Matters: Visually modeling user behaviors using Advanced Model Server")]),e._v(" "),t("blockquote",[t("p",[e._v("DICM 开启了在推荐系统中引入多媒体特征的新篇章。")])]),e._v(" "),t("ol",[t("li",[t("p",[e._v("之前的工作尽管也在推荐/搜索算法中引入了图片信息，可是那些图片只用于物料侧，用于丰富商品、文章的特征表示。而阿里的这篇论文，是第一次将图片用于用户侧建模，"),t("strong",[e._v("基于用户历史点击过的图片（user behavior images）来建模用户的视觉偏好")]),e._v("。加入了用户的视觉偏好，补齐了一块信息短板.")])]),e._v(" "),t("li",[t("p",[e._v("图片特征引入的大数据量成为技术瓶颈。为此，阿里团队通过给每个 server 增加一个可学习的“压缩”模型，先压缩 image embedding 再传递给 worker，大大降低了 worker/server 之间的通信量，使 DICM 的效率能够满足线上系统的要求。这种为 server 增加“模型训练”功能的 PS，被称为 Advanced Model Server （AMS）")])])]),e._v(" "),t("h3",{attrs:{id:"dupn"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#dupn"}},[e._v("#")]),e._v(" DUPN")])])}),[],!1,null,null,null);r.default=n.exports}}]);