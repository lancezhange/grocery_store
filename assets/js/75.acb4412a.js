(window.webpackJsonp=window.webpackJsonp||[]).push([[75],{474:function(t,a,s){"use strict";s.r(a);var n=s(17),e=Object(n.a)({},(function(){var t=this,a=t.$createElement,s=t._self._c||a;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("h1",{attrs:{id:"python"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#python"}},[t._v("#")]),t._v(" Python")]),t._v(" "),s("p"),s("div",{staticClass:"table-of-contents"},[s("ul",[s("li",[s("a",{attrs:{href:"#语言特性"}},[t._v("语言特性")])]),s("li",[s("a",{attrs:{href:"#环境"}},[t._v("环境")])]),s("li",[s("a",{attrs:{href:"#日志"}},[t._v("日志")])]),s("li",[s("a",{attrs:{href:"#基本数据结构"}},[t._v("基本数据结构")]),s("ul",[s("li",[s("a",{attrs:{href:"#宏"}},[t._v("宏")])]),s("li",[s("a",{attrs:{href:"#元类-metaclass"}},[t._v("元类(metaclass)")])]),s("li",[s("a",{attrs:{href:"#鸭子类型"}},[t._v("鸭子类型")])]),s("li",[s("a",{attrs:{href:"#单元测试"}},[t._v("单元测试")])])])]),s("li",[s("a",{attrs:{href:"#pystyle"}},[t._v("pystyle")]),s("ul",[s("li",[s("a",{attrs:{href:"#滑动-卷-往后倒-求平均-周平均"}},[t._v("滑动 卷 往后倒 求平均 周平均")])]),s("li",[s("a",{attrs:{href:"#等频聚合，例如，按周聚合，"}},[t._v("等频聚合，例如，按周聚合，")])]),s("li",[s("a",{attrs:{href:"#推荐这种方式"}},[t._v("推荐这种方式")])])])]),s("li",[s("a",{attrs:{href:"#numpy"}},[t._v("numpy")]),s("ul",[s("li",[s("a",{attrs:{href:"#sklern"}},[t._v("============= sklern")])])])]),s("li",[s("a",{attrs:{href:"#python-环境管理"}},[t._v("python 环境管理")])]),s("li",[s("a",{attrs:{href:"#高级语法"}},[t._v("高级语法")])]),s("li",[s("a",{attrs:{href:"#常用"}},[t._v("常用")]),s("ul",[s("li",[s("a",{attrs:{href:"#启动脚本个性化"}},[t._v("启动脚本个性化")])]),s("li",[s("a",{attrs:{href:"#pep8"}},[t._v("PEP8")])])])]),s("li",[s("a",{attrs:{href:"#疑难点"}},[t._v("疑难点")]),s("ul",[s("li",[s("a",{attrs:{href:"#函数的默认参数"}},[t._v("函数的默认参数")])])])]),s("li",[s("a",{attrs:{href:"#多进程并行-parallel-multiprocessing"}},[t._v("多进程并行 Parallel Multiprocessing")])]),s("li",[s("a",{attrs:{href:"#应用"}},[t._v("应用")]),s("ul",[s("li",[s("a",{attrs:{href:"#替换目录下所有文件中的某个关键词"}},[t._v("替换目录下所有文件中的某个关键词")])])])]),s("li",[s("a",{attrs:{href:"#python3-8"}},[t._v("python3.8")])]),s("li",[s("a",{attrs:{href:"#参考资料"}},[t._v("参考资料")])])])]),s("p"),t._v(" "),s("h2",{attrs:{id:"语言特性"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#语言特性"}},[t._v("#")]),t._v(" 语言特性")]),t._v(" "),s("blockquote",[s("p",[t._v("所有的变量都可以理解是内存中一个对象的“引用”")])]),t._v(" "),s("h2",{attrs:{id:"环境"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#环境"}},[t._v("#")]),t._v(" 环境")]),t._v(" "),s("p",[s("code",[t._v("brew upgrade python # 安装 python3")])]),t._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v("Python has been installed as\n  /usr/local/bin/python3\n3.7.3\n\n\nUnversioned symlinks `python`, `python-config`, `pip` etc. pointing to\n`python3`, `python3-config`, `pip3` etc., respectively, have been installed into\n  /usr/local/opt/python/libexec/bin\n\nIf you need Homebrew's Python 2.7 run\n  brew install python@2\n\nPip, setuptools, and wheel have been installed. To update them run\n  pip3 install --upgrade pip setuptools wheel\n\nYou can install Python packages with\n  pip3 install <package>\nThey will install into the site-package directory\n  /usr/local/lib/python3.7/site-packages\n")])])]),s("p",[s("code",[t._v("/Users/zhangxisheng/anaconda/bin/jupyter notebook")]),t._v(" 用 python2 作为主环境的 jupyter notebook")]),t._v(" "),s("p",[s("code",[t._v("brew info python")]),t._v("\n会发现，我的机器上其实有 2.7， 3.6 和 3.7 多个环境！")]),t._v(" "),s("p",[t._v("将 python 3.6 路径加入 path 之后，python3 又可以关联到 python 3.6 了！\n但是 pip3 还是 python 3.7 的（通过 pip3 --version 可以看到）")]),t._v(" "),s("p",[t._v("那么，python3.6 如何安装包呢")]),t._v(" "),s("p",[s("code",[t._v("python3 -m pip install --upgrade sacredboard")]),t._v("\nor\n"),s("code",[t._v("alias pip3='python3 -m pip'")]),t._v(" (但是 sudo pip3 依然用的是系统的 3.7)")]),t._v(" "),s("p",[t._v("因此，以后==全部用 anaconda 版本的==")]),t._v(" "),s("p",[t._v("下载速度慢，换源\n"),s("code",[t._v("python3 -m pip install --upgrade scikit-image -i https://pypi.tuna.tsinghua.edu.cn/simple")]),t._v(" 用清华的源")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("PACKAGE_DIR "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'/Users/zhangxisheng/github_opensouced_project/deep-ctr-prediction'")]),t._v("\nsys"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("path"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("insert"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" PACKAGE_DIR"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("h2",{attrs:{id:"日志"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#日志"}},[t._v("#")]),t._v(" 日志")]),t._v(" "),s("p",[t._v("log")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" logging\nlogging"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("basicConfig"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("format")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'%(asctime)s : %(levelname)s : %(message)s'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" level"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("logging"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ERROR"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("try:\nexcept:")]),t._v(" "),s("h2",{attrs:{id:"基本数据结构"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#基本数据结构"}},[t._v("#")]),t._v(" 基本数据结构")]),t._v(" "),s("ol",[s("li",[s("strong",[t._v("带有默认值的字典")])])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" collections "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" defaultdict\nd "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("dict")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nd "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" defaultdict"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("lambda")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" d"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nd"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'a'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])])]),s("ol",{attrs:{start:"2"}},[s("li",[s("p",[t._v("list comprehension\n"),s("code",[t._v("t = [True if (i == 'n' or i == 'v') else False for i in x.split(\"/\")]")])])]),t._v(" "),s("li",[s("p",[t._v("flatMap 的一种实现：利用 reudce 函数\nreduce(list."),s("strong",[t._v("add")]),t._v(', [i.split(",") for i in x])\n或者较为复杂的 list comprehension\nflattened_list = [y for x in list_of_lists for y in x]')])])]),t._v(" "),s("p",[t._v(".lower() 小写")]),t._v(" "),s("p",[t._v("叉积，组合，交叉\nfor p in itertools.product([1, 2, 3], [4, 5]):\nprint(p)")]),t._v(" "),s("p",[t._v("for p in itertools.combinations([1,2,3],2):print(p)")]),t._v(" "),s("p",[t._v("flattern")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" itertools\n\na "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("6")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("7")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("list")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("itertools"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("chain"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_iterable"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("a"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("sum")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("a"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("a "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"abcdef"')]),t._v("\nit "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("iter")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("a"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("next")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("it"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("it"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 剩余的元素")]),t._v("\n")])])]),s("p",[t._v("集合中随机选取元素\nimport random\nfor i in random.sample(poi_tag, 2):\nprint i, poi_tag[i]")]),t._v(" "),s("h4",{attrs:{id:"魔力方法-属性"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#魔力方法-属性"}},[t._v("#")]),t._v(" 魔力方法/属性")]),t._v(" "),s("p",[t._v("我们知道，python 中，"),s("strong",[t._v("一切皆对象")]),t._v("，同时，python 也是一门多范式的语言：面向过程/面向对象/函数式和谐共存，这背后的奥秘就是 "),s("strong",[t._v("magic method")]),t._v(".")]),t._v(" "),s("p",[t._v("事实上，许多运算符和内置函数都是用魔力方法实现的。例如，"),s("em",[t._v("+")]),t._v(" 其实是 "),s("code",[t._v("__add__()")]),t._v(", "),s("code",[t._v("len()")]),t._v(" 为 "),s("code",[t._v("__len__()")]),t._v("，而任何具有"),s("code",[t._v("__call__()")]),t._v(" 方法的对象都被当做是函数。此外，"),s("code",[t._v("with")]),s("strong",[t._v("上下文管理器")]),t._v("也是借助"),s("code",[t._v("__enter__()")]),t._v(" 和 "),s("code",[t._v("__exit__()")]),t._v("魔力函数实现的。")]),t._v(" "),s("p",[t._v("如下列举一些重要的魔力方法和魔力属性")]),t._v(" "),s("ol",[s("li",[s("code",[t._v("__dict__")]),t._v(" : 属性列表，"),s("code",[t._v("object.attr")]),t._v(" 其实就是 "),s("code",[t._v("object.__dict__[attr]")]),t._v("，许多内置类型，如 list, 都没有该属性；类的属性")]),t._v(" "),s("li",[s("code",[t._v("__class__")]),t._v(", "),s("code",[t._v("__bases__")]),t._v(","),s("code",[t._v("__name__")]),t._v(",")]),t._v(" "),s("li",[s("code",[t._v("__slots__")]),t._v(" 对拥有该属性的类的对象，只能对"),s("code",[t._v("__slots__")]),t._v("中列出的属性做设置")]),t._v(" "),s("li",[t._v("用以包构建的，如 "),s("code",[t._v("__all__")])]),t._v(" "),s("li",[s("code",[t._v("__setitem__")])]),t._v(" "),s("li",[s("code",[t._v("__init__()")]),t._v(","),s("code",[t._v("__new__")])])]),t._v(" "),s("p",[t._v("一个类的实例像函数一样被调用，就要借助 "),s("strong",[t._v("call")])]),t._v(" "),s("p",[s("strong",[t._v("init")]),t._v(" 其实是初始化方法，真正的构造方法是 "),s("strong",[t._v("new")]),t._v("\n可以说，"),s("strong",[t._v("new")]),t._v(" 是个 static class method, 而 "),s("strong",[t._v("init")]),t._v(" 是 instance method")]),t._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("Use __new__ when you need to control the creation of a new instance.\nUse __init__ when you need to control initialization of a new instance.*\n")])])]),s("ol",[s("li",[s("code",[t._v("__repr__")]),t._v(", "),s("code",[t._v("__str__")])]),t._v(" "),s("li",[s("code",[t._v("__exit__(self, type, value, traceback)")])]),t._v(" "),s("li",[s("code",[t._v("__iter__")]),t._v("(通常和 "),s("code",[t._v("yield")]),t._v(" 一起用以定义可迭代类)")]),t._v(" "),s("li")]),t._v(" "),s("p",[t._v("class Foo(object):\ndef "),s("strong",[t._v("init")]),t._v('(self, a):\nprint "run in init"\nprint "a = ", a')]),t._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v('def __call__(self, *args):\n    print "run in call"\n    print "args = ", args\n')])])]),s("p",[t._v('f = Foo(\'ha\') # 调用 init\nf("hb","hd") # 调用 call\n可见，'),s("strong",[t._v("call")]),t._v(" 等价于重载了括号运算符。")]),t._v(" "),s("p",[t._v("中文字符串截取\nx.decode('utf8')[0:n].encode('utf8')")]),t._v(" "),s("p",[t._v("中文字符")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# -*- coding: utf-8 -*-")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" re\n\ns "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('u"中华人民333dffg"')]),t._v("\nmatch "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" re"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("match"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("u"),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('r"[\\u4e00-\\u9fa5]+"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" s"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" match"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),t._v(" match"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("group"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n")])])]),s("p",[t._v("二维数组遍历\n"),s("code",[t._v("','.join(str(item) for innerlist in a for item in innerlist)")])]),t._v(" "),s("h4",{attrs:{id:"动态类型"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#动态类型"}},[t._v("#")]),t._v(" 动态类型")]),t._v(" "),s("p",[t._v("对象是存储在内存中的实体，程序中用到的名称不过是对象的引用。"),s("strong",[t._v("引用和对象分离")]),t._v("\n乃是动态类型的核心。引用可以随时指向新的对象，各个引用之间互相独立。")]),t._v(" "),s("p",[t._v("可变数据类型，如列表，可以通过引用改变自身，而不可变元素，如字符串，不能改变引用对象本身，只能改变引用的指向。")]),t._v(" "),s("p",[t._v("函数的参数传递，本质上传递的是引用，因此，如果参数是不可变对象，则对参数的操作不会影响原对象，这类似于 C++中的值传递；但如果传递的是可变参数，则有可能改变原对象。")]),t._v(" "),s("h4",{attrs:{id:"lamda-函数"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#lamda-函数"}},[t._v("#")]),t._v(" lamda 函数")]),t._v(" "),s("p",[t._v("我最早是在 haskell 中见到匿名函数的，后来它被加入到了 python 以及 java 中。python 中定义 lamda 函数很简单：")]),t._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v("    func = lambda x,y: x+y\n")])])]),s("p",[t._v("其他函数式编程的经典函数如 map, filter, reduce 等，我最早也是在 haskell 中见到的。大多类似，不再赘述。")]),t._v(" "),s("h4",{attrs:{id:"迭代器"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#迭代器"}},[t._v("#")]),t._v(" 迭代器")]),t._v(" "),s("ul",[s("li",[t._v("循环对象 例如 open() 返回的就是一个循环对象，具有 "),s("code",[t._v("next()")]),t._v(" 方法，最终举出 "),s("code",[t._v("StopIteration")]),t._v(" 错误")]),t._v(" "),s("li",[t._v("迭代器 和循环对象没有差别装饰器")]),t._v(" "),s("li",[t._v("生成器 用以构建用户自定义的循环对象，这要用到神秘的"),s("code",[t._v("yield")]),t._v("关键字")])]),t._v(" "),s("h4",{attrs:{id:"装饰器"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#装饰器"}},[t._v("#")]),t._v(" 装饰器")]),t._v(" "),s("p",[t._v("函数装饰器接受一个可调用对象作为参数，并返回一个新的可调用对象。\n装饰器也可以带有参数，从而更为灵活。")]),t._v(" "),s("p",[t._v("类装饰器同理。")]),t._v(" "),s("p",[t._v("例如，上下文管理也可以用 "),s("em",[t._v("contexlib")]),t._v(" 模块用装饰器的方式实现。")]),t._v(" "),s("ul",[s("li",[s("p",[t._v("@statcimethod")])]),t._v(" "),s("li",[s("p",[t._v("@classmethod")])]),t._v(" "),s("li",[s("p",[t._v("@property")])])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("decorator")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("object")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" f"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"run in  decorator init"')]),t._v("\n        self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("f "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" f\n\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("__call__")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" a"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" b"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"calling deperator"')]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("f"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("a"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" b"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("12")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token decorator annotation punctuation"}},[t._v("@decorator")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("add")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("a"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" b"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"original a+b = "')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" a "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" b"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" a "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" b\n")])])]),s("p",[t._v("上面这个例子中，是 class 作为装饰器，装饰了函数\n反过来，函数也可以用来装饰 class，例如，下面的 singleton 机制：")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("singleton")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("cls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    instances "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("getinstance")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" cls "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("not")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" instances"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            instances"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("cls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" cls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" instances"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("cls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" getinstance\n\n"),s("span",{pre:!0,attrs:{class:"token decorator annotation punctuation"}},[t._v("@singleton")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("MyClass")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n\n")])])]),s("p",[t._v("更多装饰器，参见 "),s("a",{attrs:{href:"https://github.com/lord63/awesome-python-decorator",target:"_blank",rel:"noopener noreferrer"}},[t._v("awesome python decorator"),s("OutboundLink")],1)]),t._v(" "),s("p",[t._v("反射")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("m "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" import_module"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"src.modelzoo.ModelAnimal"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\ncls "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("getattr")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("m"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"ModelAnimal"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nc "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" cls"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nc"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("func"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n")])])]),s("p",[t._v("编码")]),t._v(" "),s("p",[t._v("已经是 unicode 的字符串，需要原生 unicode 转义")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("x "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"\\u676d\\u5dde"')]),t._v("\nx\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# '\\\\u676d\\\\u5dde'")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),t._v(" x\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# \\u676d\\u5dde")]),t._v("\nx "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("decode"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'raw_unicode_escape'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v('# 等价于赋值的时候前缀u,例如： x= u"\\u676d\\u5dde"')]),t._v("\nx\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# u'\\u676d\\u5dde'")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),t._v(" x\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 杭州")]),t._v("\n")])])]),s("p",[t._v("对于中文，正则")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" re\n\npattern "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" re"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("compile")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("u"),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("r'([道|路]+)([0-9]+)号'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" sys\n"),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("reload")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("sys"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nsys"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("setdefaultencoding"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'utf8'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\nline "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"清华道2号"')]),t._v("\nline "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("unicode")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("line"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nmatch "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pattern"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("match"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("line"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("h3",{attrs:{id:"宏"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#宏"}},[t._v("#")]),t._v(" 宏")]),t._v(" "),s("blockquote",[s("p",[t._v("MacroPy provides a mechanism for user-defined functions (macros) to perform transformations on the abstract syntax tree (AST) of a Python program at import time. This is an easy way to enhance the semantics of a Python program in ways which are otherwise impossible, for example providing an extremely concise way of declaring classes.")])]),t._v(" "),s("p",[t._v("下面这段代码是怎么实现的？")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" macropy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("tracing "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" macros"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" trace\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("with")]),t._v(" trace"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("sum")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" i "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("sum")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("sum")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),t._v("\n")])])]),s("p",[t._v("MacroPy")]),t._v(" "),s("h4",{attrs:{id:"闭式"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#闭式"}},[t._v("#")]),t._v(" 闭式")]),t._v(" "),s("p",[t._v("闭式可以减少定义函数时的参数，例如可以利用闭包来定义泛函。")]),t._v(" "),s("h3",{attrs:{id:"元类-metaclass"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#元类-metaclass"}},[t._v("#")]),t._v(" 元类(metaclass)")]),t._v(" "),s("p",[t._v("我们说过，python 中一切皆对象，就连类也是对象（Classes are objects too）！\n那么，类是哪个类的对象呢？答案是 元类")]),t._v(" "),s("p",[t._v("metaclass 是用来创建类对象的类。"),s("code",[t._v("__class__")]),t._v(" 的 "),s("code",[t._v("__class__")]),t._v(" 为 "),s("code",[t._v("type")]),t._v(", type 是 python 中内置的创建类的元类。\ntype is the metaclass Python uses to create all classes behind the scenes.")]),t._v(" "),s("p",[t._v("像 int，str，function ，其实都是 class\ntype 最常见的还是看类型，但其实还能创建类，可谓身兼数职！这也是 python 语言中比较少见的的一个令人难以相信的东西。")]),t._v(" "),s("p",[t._v("通过 type 创建类的一般格式")]),t._v(" "),s("p",[t._v("type(name of the class,\ntuple of the parent class (for inheritance, can be empty),\ndictionary containing attributes names and values)")]),t._v(" "),s("p",[t._v("因此，如下类的定义\nclass Foo(object):\n... bar = True\n其实可以写为\nFoo = type('Foo', (), {'bar':True})")]),t._v(" "),s("p",[t._v("这样的话，创建动态类简直易如反掌！")]),t._v(" "),s("p",[t._v("自定义元类： "),s("code",[t._v("__metaclass__")]),t._v("，可以被赋值为任意可调用对象")]),t._v(" "),s("p",[t._v("说了这么多，还是不明白什么是元类，元类有什么用？别着急，看"),s("a",{attrs:{href:"https://stackoverflow.com/questions/100003/what-are-metaclasses-in-python",target:"_blank",rel:"noopener noreferrer"}},[t._v("这里"),s("OutboundLink")],1)]),t._v(" "),s("p",[t._v("元类的创建：继承 type")]),t._v(" "),s("p",[t._v("A metaclass is most commonly used as a class-factory.")]),t._v(" "),s("h3",{attrs:{id:"鸭子类型"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#鸭子类型"}},[t._v("#")]),t._v(" 鸭子类型")]),t._v(" "),s("p",[t._v("例如")]),t._v(" "),s("p",[t._v("def echo(s):\nprint s.str")]),t._v(" "),s("p",[t._v("上面这个例子中，echo 函数的传入参数，只要可以取 str，就都可以，没有限制 s 的具体类型。")]),t._v(" "),s("h4",{attrs:{id:"描述符"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#描述符"}},[t._v("#")]),t._v(" 描述符")]),t._v(" "),s("p",[t._v("参考 "),s("a",{attrs:{href:"http://pyzh.readthedocs.io/en/latest/Descriptor-HOW-TO-Guide.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("python 描述器引导"),s("OutboundLink")],1)]),t._v(" "),s("p",[s("code",[t._v("__get__")]),t._v(", "),s("code",[t._v("__set__")]),t._v(" , "),s("code",[t._v("__delete__")]),t._v(" ： 实现这三个方法中任意一个的对象叫做描述器")]),t._v(" "),s("p",[t._v("默认对属性的访问控制是从对象的字典里面("),s("strong",[t._v("dict")]),t._v(")中获取(get), 设置(set)和删除(delete)它\n举例来说， a.x 的查找顺序是, a."),s("strong",[t._v("dict")]),t._v("['x'] , 然后 type(a)."),s("strong",[t._v("dict")]),t._v("['x'] , 然后找 type(a) 的父类(不包括元类(metaclass)).如果查找到的值是一个描述器, Python 就会调用描述器的方法来重写默认的控制行为")]),t._v(" "),s("p",[t._v("注意, 只有在新式类中时描述器才会起作用")]),t._v(" "),s("h4",{attrs:{id:"继承"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#继承"}},[t._v("#")]),t._v(" 继承")]),t._v(" "),s("ol",[s("li",[t._v("super()")]),t._v(" "),s("li",[t._v("MRO(method resolution order): 也就是通常说的继承顺序，并非简单的深度或者宽度优先，而是确保所有父类不会出现在子类之前")]),t._v(" "),s("li",[s("code",[t._v("self")]),t._v(", "),s("code",[t._v("cls")])])]),t._v(" "),s("h4",{attrs:{id:"协程"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#协程"}},[t._v("#")]),t._v(" 协程")]),t._v(" "),s("p",[t._v("协程，用户级线程，可以"),s("code",[t._v("让原来要使用异步+回调方式写的非人类代码,可以用看似同步的方式写出来")]),t._v("。具体地，它可以保留上一次调用的状态，和线程相比，没有了线程切换的开销。")]),t._v(" "),s("p",[s("code",[t._v("yield")]),t._v(", "),s("code",[t._v("next()")]),t._v(", "),s("code",[t._v(".send()")]),t._v(", "),s("code",[t._v(".close()")])]),t._v(" "),s("h4",{attrs:{id:"并行"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#并行"}},[t._v("#")]),t._v(" 并行")]),t._v(" "),s("h4",{attrs:{id:"自省-introspection"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#自省-introspection"}},[t._v("#")]),t._v(" 自省(introspection)")]),t._v(" "),s("p",[s("code",[t._v("dir")]),t._v(", "),s("code",[t._v("type")]),t._v(", "),s("code",[t._v("id")])]),t._v(" "),s("p",[s("code",[t._v("inspect")]),t._v(" 模块")]),t._v(" "),s("h4",{attrs:{id:"性能优化建议-debug"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#性能优化建议-debug"}},[t._v("#")]),t._v(" 性能优化建议/debug")]),t._v(" "),s("p",[t._v("debug： "),s("em",[t._v("pdb")]),t._v("模块")]),t._v(" "),s("p",[s("a",{attrs:{href:"https://wiki.python.org/moin/PythonSpeed/PerformanceTips",target:"_blank",rel:"noopener noreferrer"}},[t._v("官方 Performance Tips"),s("OutboundLink")],1)]),t._v(" "),s("h5",{attrs:{id:"c-接口"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#c-接口"}},[t._v("#")]),t._v(" C 接口")]),t._v(" "),s("ol",[s("li",[t._v("CDLL() 加载动态库，和 R 中的做法很相似。")]),t._v(" "),s("li",[t._v("C API")])]),t._v(" "),s("h3",{attrs:{id:"单元测试"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#单元测试"}},[t._v("#")]),t._v(" 单元测试")]),t._v(" "),s("p",[s("a",{attrs:{href:"http://andrewliu.in/2015/12/12/Python%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95%E5%92%8CMock%E6%B5%8B%E8%AF%95/",target:"_blank",rel:"noopener noreferrer"}},[t._v("Python 单元测试和　 Mock 测试"),s("OutboundLink")],1)]),t._v(" "),s("h4",{attrs:{id:"其他"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#其他"}},[t._v("#")]),t._v(" 其他")]),t._v(" "),s("ol",[s("li",[s("p",[t._v("使用 "),s("em",[t._v("LBYL")]),t._v("(look before you leap, 例如前置 if 条件判断) 还是 "),s("em",[t._v("EAFP")]),t._v("(easy to ask forgiveness than permission，例如 "),s("code",[t._v("try--catch")]),t._v(")? 这里给出了一个"),s("a",{attrs:{href:"http://stackoverflow.com/questions/5589532/try-catch-or-validation-for-speed/",target:"_blank",rel:"noopener noreferrer"}},[t._v("建议"),s("OutboundLink")],1)])]),t._v(" "),s("li",[s("p",[t._v("不定长无名参数 "),s("code",[t._v("\\*args")]),t._v("(元组) 和 不定长有名参数 "),s("code",[t._v("\\*\\*kwargs")]),t._v("（列表）")])]),t._v(" "),s("li",[s("p",[s("code",[t._v("python -m SimpleHTTPServer 8088")]),t._v(" 文件共享从此 so easy.")])]),t._v(" "),s("li",[s("p",[s("code",[t._v("self")]),t._v(" 并非关键字，而只是一个约定俗成的变量名")])])]),t._v(" "),s("p",[t._v("dict 的方法")]),t._v(" "),s("p",[t._v("shop_id = shops.main_poi_id.values\nn_pois= shops.n_pois.values\ntag_map = dict((name, value) for name, value in zip(shop_id, n_pois))")]),t._v(" "),s("p",[t._v("注意，字典是 dict， 不是 map")]),t._v(" "),s("p",[t._v('assert len(data.poi_id.unique()) == data.shape[0], "门店有重复！"')]),t._v(" "),s("p",[t._v("python3 环境\nconda create -n python3 python=3 anaconda\n(我的路径为 /Users/zhangxisheng/anaconda/envs/python3)")]),t._v(" "),s("p",[t._v("(报错： "),s("code",[t._v("(eval):5: parse error near")]),t._v("|'`，后来查明，是因为我在 sitecustomized.py 中的配置中的干扰，去掉配置之后就可以了)")]),t._v(" "),s("p",[t._v("启动环境: "),s("code",[t._v("source activate python3")]),t._v("\n这样激活之后，python3 才会打开 anaconda python3")]),t._v(" "),s("p",[t._v("jupyter\nipython notebook 即可启动")]),t._v(" "),s("p",[t._v("jupyterlab 搜索启动")]),t._v(" "),s("h1",{attrs:{id:"快速传入文件"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#快速传入文件"}},[t._v("#")]),t._v(" 快速传入文件")]),t._v(" "),s("p",[t._v("python -m SimpleHTTPServer 8000 注意 windows 上用 powershell")]),t._v(" "),s("p",[t._v("python3 中如下")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("python3 "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("m http"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("server "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("8000")]),t._v("\n")])])]),s("p",[t._v("json\n"),s("code",[t._v("print json.dumps({'4': 5, '6': 7}, sort_keys=True, indent=4)")])]),t._v(" "),s("h2",{attrs:{id:"pystyle"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#pystyle"}},[t._v("#")]),t._v(" pystyle")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("pycodestyle "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("max")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("line"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("length"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("100")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("exclude mlflow"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("protos"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("mlflow"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("server"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("js"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("mlflow"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("store"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("db_migrations"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("mlflow"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("temporary_db_migrations_for_pre_1_users "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" mlflow tests\npylint "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("msg"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("template"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"{path} ({line},{column}): [{msg_id} {symbol}] {msg}"')]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("rcfile"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"$FWDIR/pylintrc"')]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" mlflow tests\n")])])]),s("p",[s("code",[t._v("pycodestyle")]),t._v(" (formerly called pep8) , Python style guide checker")]),t._v(" "),s("p",[s("code",[t._v("pycodestyle --show-source")])]),t._v(" "),s("h1",{attrs:{id:"细节"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#细节"}},[t._v("#")]),t._v(" 细节")]),t._v(" "),s("p",[t._v("函数必须用 reutrn 返回返回值，如果想有返回值的话")]),t._v(" "),s("p",[t._v("unique 是 函数， unique() 才得到值")]),t._v(" "),s("p",[t._v('忽视警告\nimport warnings\nwarnings.filterwarnings("ignore")')]),t._v(" "),s("p",[t._v('import warnings\nwarnings.filterwarnings("ignore", category=DeprecationWarning)')]),t._v(" "),s("p",[t._v("忽略命令行下警告错误的输出\npython -W ignore yourscript.py")]),t._v(" "),s("p",[t._v("df.values dataFrame 转为 ndarray")]),t._v(" "),s("p",[t._v("y = pd.DataFrame({'poi': x['b'].values, 'deal': x['a'].values, 'bu': x['c'].values, 'prob': x['d'].values}, columns=['poi', 'deal', 'bu', 'prob'])")]),t._v(" "),s("p",[t._v("y.to_csv('/Users/zhangxisheng/Downloads/refund_reason_concerns_refuse_2017-10-19_new.txt', index=False)")]),t._v(" "),s("p",[t._v("Series.values 返回 ndarray")]),t._v(" "),s("p",[t._v("mask\n使用()括起筛选条件，多个筛选条件之间使用逻辑运算符&,|,~与或非进行连接，特别注意，和我们平常使用 Python 不同，这里用 and,or,not 是行不通的")]),t._v(" "),s("p",[t._v("data.ix[[231, 236]] 选取行\n对索引是整数的，按索引走；对索引是非整数的，按值走；因此，这个函数是应当摒弃的！")]),t._v(" "),s("p",[t._v("既查又改的正确方式\nf.loc[f['a'] <= 3, 'b'] = f.loc[f['a'] <= 3, 'b'] / 10")]),t._v(" "),s("p",[t._v("按照 index 列 选取的话，直接 loc 取就行\ndf.loc[ind].head(100)\n如果反向选择，即取反，还得\nbad_df = df.index.isin([3,5])\ndf[~bad_df]")]),t._v(" "),s("h1",{attrs:{id:"select-the-5-largest-max-delays"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#select-the-5-largest-max-delays"}},[t._v("#")]),t._v(" Select the 5 largest/max delays")]),t._v(" "),s("p",[t._v("delays.nlargest(5).sort_values()\n选取 N 最大并不需要全部排序！\ndelays.nsmallest(5).sort_values()")]),t._v(" "),s("p",[t._v("import heapq\na = [1, 3, 2, 4, 5]\nheapq.nlargest(3, range(len(a)), a."),s("strong",[t._v("getitem")]),t._v(")")]),t._v(" "),s("p",[t._v('实验记录\nsacred\nprint_config\nwith arg="value"')]),t._v(" "),s("p",[t._v("降维可视化\nhypertools\nhyp.plot(sample, n_clusters=2, explore=False, group='label')")]),t._v(" "),s("p",[t._v("时间和日期\ndatetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S')")]),t._v(" "),s("p",[t._v("计算天数差值\nd1 = datetime.datetime.strptime('2017-08-03', '%Y-%m-%d')\nd2 = datetime.datetime.strptime('2017-08-02', '%Y-%m-%d')\ndelta = d1 - d2\nprint delta.days")]),t._v(" "),s("div",{staticClass:"error"},[t._v("Only valid with DatetimeIndex, TimedeltaIndex or PeriodIndex, but got an instance of 'RangeIndex'")]),t._v(" "),s("p",[t._v("======================================== dataFrame／pandas\ndf = pd.DataFrame({'B': [4,5,6,7],'A': ['a','b', 'a', 'b'], 'C':['china', 'china', 'eglish', 'egnlish']})")]),t._v(" "),s("p",[t._v('pd.read_csv(data_location, sep=feat_separator, header=None, na_values="null")\n读入数据时指定 na 值')]),t._v(" "),s("p",[t._v("箱线图\ndata[['label', 'hcg_t']].boxplot(by='label')")]),t._v(" "),s("p",[t._v("Pandas 所支持的数据类型:")]),t._v(" "),s("ol",[s("li",[t._v("float")]),t._v(" "),s("li",[t._v("int")]),t._v(" "),s("li",[t._v("bool")]),t._v(" "),s("li",[t._v("datetime64[ns]")]),t._v(" "),s("li",[t._v("datetime64[ns, tz]")]),t._v(" "),s("li",[t._v("timedelta[ns]")]),t._v(" "),s("li",[t._v("category")]),t._v(" "),s("li",[t._v("object\n默认的数据类型是 int64,float64.")])]),t._v(" "),s("p",[t._v("from pandas.util import testing\ndf = testing.makeDataFrame()")]),t._v(" "),s("p",[t._v("df = pd.DataFrame()")]),t._v(" "),s("p",[t._v("pd.read_clipboard()")]),t._v(" "),s("p",[t._v("df_ta['delivery_duration'] = df_ta['delivery_duration'].astype['float']")]),t._v(" "),s("p",[t._v("df.sample(5) # 随机抽取 5 条数据查看\ndf.iloc[22] #利用 iloc 命令做行选取")]),t._v(" "),s("p",[t._v("df.loc[22:25] #利用 loc 命令做行选取\ndf.loc[[22,33,44]] #利用 loc 命令做指定行选取")]),t._v(" "),s("p",[t._v("df['open_int'] = np.nan\ndf['open_int'] = 999\ndf['test'] = df.company_type == '民营企业'\ndf.rename(index=str, columns={'test':'乱加的一列'}, inplace=True) #更改列名，并固化该操作")]),t._v(" "),s("p",[t._v("df_test.drop([2,5],axis=0) #删除行\ndf_test.drop(['列 1','列 2'], axis=1) #删除列\ndf.date = df.date.map(lambda x: x.strftime('%Y-%m-%d')) #将时间数据转换为字符串")]),t._v(" "),s("p",[t._v("df['fs_roe'].mean() #计算平均数\ndf['fs_roe'].idxmax() #返回最大值 index\ndf.loc[df['fs_roe'].idxmin()]")]),t._v(" "),s("p",[t._v("df.fs_net_profit.corr(df.volume) #求解相关系数\ndf.company_type.unique() #查看不重复的数据\ndf_test.dropna(axis= 1) #删除含有 NaN 值的列\nm = data.dropna(axis=0, subset=['toshopid']) # 删除某列为 Nan 的行\ndf_test.volume.fillna(method= 'ffill',limit= 1) #限制向前或者向后填充行数")]),t._v(" "),s("p",[t._v("tmp = tmp.set_index('order_unix_time') 设置索引")]),t._v(" "),s("p",[t._v("group by\n文档 http://pandas.pydata.org/pandas-docs/stable/groupby.html#group-by-split-apply-combine\ngrouped = df.groupby('A')\ngrouped = df.groupby(['A', 'B'])\ngrouped.get_group('bar')\n按照多列分组\ndf.groupby(['A', 'B']).get_group(('bar', 'one'))")]),t._v(" "),s("p",[t._v("组内排序\nMy_Frame['sort_id'] = My_Frame['salary'].groupby(My_Frame['dep_id']).rank(ascending=False)\n注意排序的时候，如果是按照多列分组，多列的写法和上面的不太一样：")]),t._v(" "),s("p",[t._v("如果写成 .groupby(samll_shopes['a', 'b']) 则会报错： ValueError: Grouper for '<class 'pandas.core.frame.DataFrame'>' not 1-dimensional")]),t._v(" "),s("p",[t._v("df['Data4'] = df['Data3'].groupby(df['Date']).transform('sum')")]),t._v(" "),s("h3",{attrs:{id:"滑动-卷-往后倒-求平均-周平均"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#滑动-卷-往后倒-求平均-周平均"}},[t._v("#")]),t._v(" 滑动 卷 往后倒 求平均 周平均")]),t._v(" "),s("p",[t._v("df['A'] = df.rolling(2).mean() 注意，针对的是 index，即 index 每隔两个。由于默认的 index 是从 0 开始的数字，因此和每隔多少行无异\n但是如果 index 是 时间戳这种，就可以按照 '2s' 这种间距\ndf.rolling(2, min_periods=1).sum()\ndf_re.groupby('A').rolling(4).B.mean() 直接 rolling 会无视索引的区别，但是线分组的话，rolling 也会被限制在索引中\nx.B.xs('a').xs('china')")]),t._v(" "),s("p",[t._v("累积\ndf_re.groupby('A').expanding().sum()")]),t._v(" "),s("p",[t._v("df['new_column'] = pd.Series 这种赋值方式似乎并不是我们想象的那样，例如，当我给一个 df 赋予随机值列的时候，可能因为 df 的 index 不连续，造成好多 NUll 随机数。")]),t._v(" "),s("p",[t._v("df_re.groupby('group').resample('1D').ffill()\nresample 等频抽样，D 为天，S 为秒，")]),t._v(" "),s("h3",{attrs:{id:"等频聚合，例如，按周聚合，"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#等频聚合，例如，按周聚合，"}},[t._v("#")]),t._v(" 等频聚合，例如，按周聚合，")]),t._v(" "),s("p",[t._v("df['count'].resample('D', how='sum')")]),t._v(" "),s("p",[t._v("过滤 sf.groupby(sf).filter(lambda x: x.sum() > 2)")]),t._v(" "),s("p",[t._v("df.groupby('g').boxplot()")]),t._v(" "),s("p",[t._v('二维表\npd.crosstab(data["Credit_History"],data["Loan_Status"],margins=True)')]),t._v(" "),s("p",[t._v("排序\ndata_sorted = data.sort_values(['ApplicantIncome','CoapplicantIncome'], ascending=False)")]),t._v(" "),s("p",[t._v('箱线图\ndata.boxplot(column="ApplicantIncome",by="Loan_Status")')]),t._v(" "),s("p",[t._v("mask 注意==判断时，注意每列的类型，不要混淆 int64 和 string")]),t._v(" "),s("p",[t._v("df.xs(index) 按索引选择\n如果列名是中文，则用.号取值会报错，用中括号的方式即可。")]),t._v(" "),s("p",[t._v("mask = (x['中国'] <=2 )")]),t._v(" "),s("p",[t._v("import pandas as pd\ntrain = pd.read_csv('train.csv')\ntrain.shape\ntrain.head()\ntrain.columnA.describe()")]),t._v(" "),s("p",[t._v("describe(include='all') 描述所有列（默认只数值列）")]),t._v(" "),s("p",[t._v("取 dataframe 的一列成为 array\nshop_id = shops.main_poi_id.values")]),t._v(" "),s("p",[t._v("日期加上一天\ndapan['partition_date'] + pd.DateOffset(1)")]),t._v(" "),s("p",[t._v("日期操作函数\ndf.first_cooperate_month.dt.year.value_counts()")]),t._v(" "),s("p",[t._v("选择和变换\nmask = (df_tr.hour.values == 11) | (df_tr.hour.values == 17) & (df_tr.day.values == 17)\n注意，.values 取数值， .str 取字符串，\n例如 (df.messgae.str.find('model_e') != -1) 此处 find 用于判断字符串查找，或者也可以用 candi[candi['ids'].str.contains(\"2015122\")] 这种方式")]),t._v(" "),s("p",[t._v('但是，但是，对于中文，好像又不能用 str 和 unicode 的 u\n例如，tmp = raw_data.loc[raw_data.name == "北京"] 可以，但是 tmp = raw_data.loc[raw_data.name.str == "北京"] 或者 tmp = raw_data.loc[raw_data.name == u"北京"] 都是不对的！！！')]),t._v(" "),s("p",[t._v("对于中文列名，如何处理？\nx[x['中国'] <= 2] 这种选择方法当时也是可以的")]),t._v(" "),s("p",[t._v("df['gen'] = df['gen'].mask(df['gen'] + df['cont'] < 0.01)")]),t._v(" "),s("p",[t._v("df.loc[df['First Season'] > 1990, 'First Season'] = 1\ndf['First Season'] = (df['First Season'] > 1990).astype(int)")]),t._v(" "),s("p",[t._v("选出数值特征\nnumerric_features = train.select_dtypes(includes=[np.number])\nnumerric_features.dtypes")]),t._v(" "),s("p",[t._v("筛选非数字特征\ncategorical = train.select_dtypes(exclude=[np.number])")]),t._v(" "),s("p",[t._v("merge\ncandi = pd.merge(data, mpoi, left_on=['city', 'geohash_kb'], right_on=['city_name', 'geohash_mt'], how='left')")]),t._v(" "),s("p",[t._v('geohash\nimport pygeohash as pgh\ndef geo_encode(x, n=4, by=1):\nreturn str(pgh.encode(x[0]/by, x[1]/by,n)) + "--" + x[2]')]),t._v(" "),s("p",[t._v("按照是否包含在集合中选取 dataframe 符合条件的数据\ndf = data.loc[data.biz_type.isin({200}) ]\n取反 用 ～")]),t._v(" "),s("p",[t._v("分组排序\ndf.groupby('img')['h'].rank(ascending=False).values")]),t._v(" "),s("p",[t._v("类似的，分组最大\nx.r.map(lambda i: x.groupby('r').n.max()[i])")]),t._v(" "),s("p",[t._v("分组计数\nx.r.map(lambda i: x.groupby('r').n.count()[i]).values")]),t._v(" "),s("p",[t._v("去重前先排序\ndf = df.sort_values('lifecycle', ascending=False).drop_duplicates('case_id').sort_index()")]),t._v(" "),s("p",[t._v("变量的协方差矩阵\ncorr = numerric_features.corr()\n协方差最大最小值查看\ncorr[columnA].sort_values(ascending=false)[:5]\ncoor[columnA].sort_values(ascending=false)[-5:]")]),t._v(" "),s("p",[t._v("去除重复值 去重\ntrain.columnA.unique()\ndrop_duplicates(subset=None, keep='first', inplace=False)")]),t._v(" "),s("p",[t._v("数据透视表\npivot = train.pivot_table(index = 某个分类变量, values=某个数值变量, aggfunc=np.mean)\npivot = df_tr.pivot_table(index = 'area_id', values='poi_id', aggfunc=lambda x: len(x.dropna().unique()))\nprint pivot\n还可以加上 columns")]),t._v(" "),s("p",[t._v("多列透视，这个厉害了\np = df_tr.groupby('area_id').aggregate({'delivery_duration':np.mean, 'poi_id':lambda x: len(x.dropna().unique())})")]),t._v(" "),s("p",[t._v("agg 方法将一个函数使用在一个数列上，然后返回一个标量的值。也就是说 agg 每次传入的是一列数据，对其聚合后返回标量。")]),t._v(" "),s("p",[t._v("apply 是一个更一般化的方法：将一个数据分拆-应用-汇总。而 apply 会将当前分组后的数据一起传入，可以返回多维数据。")]),t._v(" "),s("p",[t._v("data.groupby(['model', 'bu']).size()")]),t._v(" "),s("p",[t._v("concat\npandas.concat(objs, axis=0, join='outer', join_axes=None, ignore_index=False, keys=None, levels=None, names=None, verify_integrity=False, copy=True)[source]¶")]),t._v(" "),s("p",[t._v("一列构造多列\ndf.textcol.apply(lambda s: pd.Series({'feature1':s+1, 'feature2'😒-1}))")]),t._v(" "),s("p",[t._v("时间序列\n日期类型转换\nloandata['issue_d']=pd.to_datetime(loandata['issue_d'])")]),t._v(" "),s("p",[t._v("对此透视表，画条形图\npviot.plot(kind='bar', color='blue')")]),t._v(" "),s("p",[t._v("条件筛选\ntrain = train[train[columnA] < 100]")]),t._v(" "),s("p",[t._v("空值\nnulls = pd.dataFrame(train.isnull().sum().sort_values(ascending=false)[:25])\n统计每列的 nan 值 或者 缺失值\ndf.isnull().sum()\n缺失值填充\ndf['time3_category'] = df['time3_category'].fillna(1)\ndf.fillna(df.mean()['a', 'b']) 这种在数据量很大的时候似乎总是很慢不可用\ndf[['a', 'b']] = df[['a','b']].fillna(value=0)")]),t._v(" "),s("p",[t._v("fill_NaN = Imputer(missing_values=np.nan, strategy='mean', axis=1)\nimputed_DF = pd.DataFrame(fill_NaN.fit_transform(df))\nimputed_DF.columns = df.columns\nimputed_DF.index = df.index")]),t._v(" "),s("p",[t._v("理解 axis\n轴用来为超过一维的数组定义的属性，二维数据拥有两个轴：第 0 轴沿着行的垂直往下，第 1 轴沿着列的方向水平延伸。")]),t._v(" "),s("p",[t._v('df.mean(axis=1) 计算的是每一行的均值\ndf.drop("col4", axis=1) 删除的却是这一列的每一行')]),t._v(" "),s("p",[t._v("null 缺失值\nxtrain = df.loc[df['Survive'].notnull(), ['Age','Fare', 'Group_Size','deck', 'Pclass', 'Title' ]]\nxtrain")]),t._v(" "),s("p",[t._v("SettingWithCop")]),t._v(" "),s("blockquote",[s("p",[t._v("A value is trying to be set on a copy of a slice from a DataFrame")])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" pandas "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("\ndf "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" DataFrame"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"A"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"B"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3.125")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("4.12")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3.1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("6.2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("7")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nrow "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("loc"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\nrow"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"A"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("\n")])])]),s("p",[t._v("上面这段代码就会爆错： A value is trying to be set on a copy of a slice from a DataFrame")]),t._v(" "),s("p",[t._v("aggregate 聚合之后，如何重命名聚合列的名称呢？\ndf = data.groupby('Seed').agg(\n{'age':['sum'],\n'height':['mean', 'std']})\ndf.columns = [\"_\".join(x) for x in df.columns.ravel()]\n但是如果聚合函数是 lambda 函数呢？")]),t._v(" "),s("p",[t._v("df['new_column'] = np.multiply(df['A'], df['B'])")]),t._v(" "),s("p",[t._v("设置列名称\nnull.columns = ['name1']\n设置行索引名称\nnull.index.name = 'name2'")]),t._v(" "),s("p",[t._v("分类变量的值和频次\ny = train.columnA.value_counts()\nlen(y[y>1].unique()) 过滤频次\ny = tmp.kb_id.value_counts()\ny 这样就能打印出现次数 top 的了，默认排序，很好")]),t._v(" "),s("p",[t._v("value_counts()得到的是 series (with index)， series 转 dataframe:\ndata.day.value_counts().to_frame()")]),t._v(" "),s("p",[t._v("或者，分组后统计频次，并排序\nf = df[['STNAME','CTYNAME']].groupby(['STNAME'])['CTYNAME'] "),s("br"),t._v("\n.count() "),s("br"),t._v("\n.reset_index(name='count') "),s("br"),t._v("\n.sort_values(['count'], ascending=False) "),s("br"),t._v("\n.head(5)")]),t._v(" "),s("p",[t._v("哑变量 ont-hot 编码 注意，训练集和测试集要编码一致\ntrain['a_encode'] = pd.get_dummies(train.columnA, drop_first=true)\ntest['a_encode'] = pd.get_dummies(train.columnA, drop_first=true)")]),t._v(" "),s("p",[t._v("Apply 方法\ntrain.columnA.apply(func)")]),t._v(" "),s("p",[t._v("插补缺失值\ndata = train.select_dtypes(include=[np.number]).interpolate().dropna()")]),t._v(" "),s("p",[t._v("去除列\ndata.drop(['featureA'], axis=1)")]),t._v(" "),s("p",[t._v("创建 dataFrame\ndf = pd.dataFrame()\ndf['id'] = train.id")]),t._v(" "),s("p",[t._v("dataFrame 转化为 csv\ndf.to_csv('a.csv', index=False)\nd.to_csv('/Users/zhangxisheng/Downloads/big_meal_single_poi.csv', index=False, encoding='utf-8') 有时候有编码问题的时候，加上 encoding 能够解决\n存下来的文件，excel 读入之后为乱码的解决方案：encoding='utf_8_sig'")]),t._v(" "),s("p",[t._v('一个 to_csv 的疑难杂症：\n我的 df 只有一列，类型为字符串，当我用 to_csv 存储下来之后，发现有些行被加上了双引号，有些则没有。\n最后在轩哥的帮助下定位到原因： 这些加上了双引号的行，都是因为本身带有都好；解决办法就是，指明 sep="\\t"，这样就行了。')]),t._v(" "),s("p",[t._v("obj.combine_first(other)\n如果 obj 中有为 null 的值，用对应索引的 other 中的数据填充")]),t._v(" "),s("p",[t._v("某一列为 nan 的行\ndf = df[np.isfinite(df['EPS'])]")]),t._v(" "),s("p",[t._v("索引\nagg.index.values")]),t._v(" "),s("p",[t._v("列与索引之间可以相互转化\ndf.set_index('date', inplace=True)\nx['index'] = x.index.get_level_values('C')\ndf.reset_index(inplace=True, drop=False) 对多重索引，将会把每个索引变为列，新加索引为从 0 开始的自然数，非常赞")]),t._v(" "),s("p",[t._v("层次化索引\n所谓层次化索引，其实也是多重索引，只不过有些相同的被省略了。\ndata.unstack() 可以打平层次化索引中的部分索引，即变宽表")]),t._v(" "),s("p",[t._v("多重索引\nm_idx = pd.MultiIndex.from_tuples(zip(dates, labels), names=['date', 'label'])\ndata_dict = {'observation1':obs1, 'observation2':obs2}\ndf = pd.DataFrame(data_dict, index=m_idx)\n参见 http://www.jianshu.com/p/3ab1554fe6f3")]),t._v(" "),s("p",[t._v("类型转换\ndf_ta['delivery_duration'] = df_ta['delivery_duration'].astype('float')")]),t._v(" "),s("p",[t._v("斜度\ntrain.columnA.skew()")]),t._v(" "),s("p",[t._v("交叉表 cross table\npd.crosstab(df.E,df.group)")]),t._v(" "),s("p",[t._v("标签编码\nlbe = LabelEncoder()\nlbe.fit(df_tr['area_id'].values.reshape(-1, 1))\ndf_ta['area_id_le'] = lbe.transform(df_ta['area_id'].values.reshape(-1, 1))")]),t._v(" "),s("p",[t._v('宽表变窄表\ndf = pd.melt(df, id_vars=["date"], var_name="condition")')]),t._v(" "),s("p",[t._v("窄表边宽表\npartition_date bu n_poi\n0 2018-04-01 交易销售部 67719\n1 2018-04-01 渠道发展部 16774\n2 2018-04-01 电销及平台支持部 15683\n3 2018-04-02 交易销售部 67526\n4 2018-04-02 渠道发展部 16757\n5 2018-04-02 电销及平台支持部 15393\n6 2018-04-03 交易销售部 67154\n7 2018-04-03 渠道发展部 16871\n8 2018-04-03 电销及平台支持部 15333\n11 2018-04-04 电销及平台支持部 15938")]),t._v(" "),s("p",[t._v("如何转成下面这种表？\n日期 交易 渠道 电销\n20180401 67719 16774 15683")]),t._v(" "),s("p",[t._v("因为只有下面这种宽表，才能画堆叠的直方图。\n一种办法是在 sql 里，聚合一下分别统计成三列就好，这是简单的。\n第二种就是： 聚合。\ndata[['prob', 'model_tag', 'user_id']].pivot_table(values='prob', columns='model_tag', index=['user_id'])")]),t._v(" "),s("h4",{attrs:{id:"loc-assign-的用"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#loc-assign-的用"}},[t._v("#")]),t._v(" loc assign 的用")]),t._v(" "),s("p",[t._v("data.loc[data.penalty_end_day.isnull()]['punished'] = 0 这种不行的\n正确的写法： data.loc[data.penalty_end_day.isnull(), ['punished']] = 0")]),t._v(" "),s("p",[t._v("cut 切分和分组\nbins = [0, 5, 10, 15, 20]\ngroup_names = ['A', 'B', 'C', 'D']\nloandata['categories'] = pd.cut(loandata['open_acc'], bins, labels=group_names)")]),t._v(" "),s("p",[t._v("df['bin'] = pd.cut(df.discount, bins = [0,0.5, 0.7,0.9,1], labels=['low', 'middle', 'normal','high'])\ndf.groupby('bin').aggregate({'label': np.sum}).plot(kind='bar')")]),t._v(" "),s("p",[t._v("bins = pd.cut(df['Value'], [0, 100, 250, 1500])\ndf.groupby(bins)['Value'].agg(['count', 'sum'])")]),t._v(" "),s("p",[t._v("cut 和 qcut 的区别\npd.qcut(factors, 5).value_counts()\npd.cut(factors, 5).value_counts()")]),t._v(" "),s("p",[t._v("qcuts 一般会保证每个 bin 中的个数是一样的，而 cut 不会考虑频次，只是平均分值。")]),t._v(" "),s("p",[t._v("分列\ngrade_split = pd.DataFrame((x.split('-') for x in loandata.grade),\nindex=loandata.index,columns=['grade','sub_grade'])")]),t._v(" "),s("p",[t._v("df.info(memory_usage='deep') 该表的精确内存使用量，行列个数，以及对应的数据类型个数\n在底层，pandas 会按照 数据类型 将 列 分组形成数据块（blocks）\n对于包含数值型数据（比如整型和浮点型）的数据块，pandas 会合并这些列，并把它们存储为一个 Numpy 数组（ndarray）。\nNumpy 数组是在 C 数组的基础上创建的，其值在内存中是连续存储的。基于这种存储机制，对其切片的访问是相当快的。")]),t._v(" "),s("p",[t._v("看每种类型的块所占内存\nfor dtype in ['float', 'int', 'object']:\nselected = df.select_dtypes(include=[dtype])\nmean_useage = selected.memory_usage(deep=true).mean()\nmean_useage = mean_useage/1024**2")]),t._v(" "),s("p",[t._v("pandas 中的许多数据类型具有多个子类型，它们可以使用较少的字节去表示不同数据，比如，\nfloat 型就有 float16、float32 和 float64 这些子类型")]),t._v(" "),s("p",[t._v("参考 用 pandas 处理大数据——节省 90%内存消耗的小贴士\nhttps://mp.weixin.qq.com/s?__biz=MzAxNTc0Mjg0Mg==&mid=2653286198&idx=1&sn=f8f0ea4845586b1f9b645995aa07d8a0&open_source=weibo_search")]),t._v(" "),s("p",[t._v("simhash\nimport re\nfrom simhash import Simhash\ndef get_features(s):\nwidth = 3\ns = s.lower()\ns = re.sub(r'[^\\w]+', '', s)\nreturn [s[i:i + width] for i in range(max(len(s) - width + 1, 1))]")]),t._v(" "),s("p",[t._v("print '%x' % Simhash(get_features('How are you? I am fine. Thanks.')).value\nprint '%x' % Simhash(get_features('How are u? I am fine. Thanks.')).value\nprint '%x' % Simhash(get_features('How r you?I am fine. Thanks.')).value")]),t._v(" "),s("p",[t._v("print Simhash('aa').distance(Simhash('bb'))\nprint Simhash('aa').distance(Simhash('aa'))")]),t._v(" "),s("p",[t._v("5.主成分分析")]),t._v(" "),s("p",[t._v("主成分分析是由因子分析进化而来的一种降维的方法。")]),t._v(" "),s("p",[t._v("通过正交变换将原始特征转换为线性独立的特征，转换后得到的特征被称为主成分。主成分分析可以将原始维度降维到 n 个维度。")]),t._v(" "),s("p",[t._v("有一个特例情况，就是通过主成分分析将维度降低为 2 维，可以将多维数据转换为平面中的点，来达到多维数据可视化的目的。")]),t._v(" "),s("p",[t._v("from sklearn import decomposition")]),t._v(" "),s("p",[t._v("pca = decomposition.PCA(n_components=2)")]),t._v(" "),s("p",[t._v("X = pca.fit_transform(data.ix[:,:-1].values)")]),t._v(" "),s("p",[t._v("pos=pd.DataFrame()")]),t._v(" "),s("p",[t._v("pos['X'] =X[:, 0]")]),t._v(" "),s("p",[t._v("pos['Y'] =X[:, 1]")]),t._v(" "),s("p",[t._v("pos['species'] = data['species']")]),t._v(" "),s("p",[t._v("ax = pos.ix[pos['species']=='virginica'].")]),t._v(" "),s("p",[t._v("plot(kind='scatter', x='X', y='Y', color='blue', label='virginica')")]),t._v(" "),s("p",[t._v("ax = pos.ix[pos['species']=='setosa'].")]),t._v(" "),s("p",[t._v("plot(kind='scatter', x='X', y='Y', color='green', label='setosa', ax=ax)")]),t._v(" "),s("p",[t._v("pos.ix[pos['species']=='versicolor'].")]),t._v(" "),s("p",[t._v("plot(kind='scatter', x='X', y='Y', color='red', label='versicolor', ax=ax")]),t._v(" "),s("p",[t._v("需要注意，通过 PCA 降维实际上是损失了一些信息，我们也可以看一下保留的两个主成分可以解释原始数据的多少。")]),t._v(" "),s("p",[t._v("6.独立成分分析")]),t._v(" "),s("p",[t._v("独立成分分析将多源信号拆分成较大可能独立性的子成分，它最初不是用来降维，而是用于拆分重叠的信号。")]),t._v(" "),s("p",[t._v("from sklearn import decomposition")]),t._v(" "),s("p",[t._v("pca = decomposition.FastICA(n_components=2)")]),t._v(" "),s("p",[t._v("X = pca.fit_transform(data.ix[:,:-1].values)")]),t._v(" "),s("p",[t._v("pos=pd.DataFrame()")]),t._v(" "),s("p",[t._v("pos['X'] =X[:, 0]")]),t._v(" "),s("p",[t._v("pos['Y'] =X[:, 1]")]),t._v(" "),s("p",[t._v("pos['species'] = data['species']")]),t._v(" "),s("p",[t._v("ax = pos.ix[pos['species']=='virginica'].")]),t._v(" "),s("p",[t._v("plot(kind='scatter', x='X', y='Y', color='blue', label='virginica')")]),t._v(" "),s("p",[t._v("ax = pos.ix[pos['species']=='setosa'].")]),t._v(" "),s("p",[t._v("plot(kind='scatter', x='X', y='Y', color='green', label='setosa', ax=ax)")]),t._v(" "),s("p",[t._v("pos.ix[pos['species']=='versicolor'].")]),t._v(" "),s("p",[t._v("plot(kind='scatter', x='X', y='Y', color='red', label='versicolor', ax=ax)")]),t._v(" "),s("p",[t._v("Out[42]:")]),t._v(" "),s("p",[t._v("<matplotlib.axes._subplots.AxesSubplot at 0x7f47f274af28>")]),t._v(" "),s("p",[t._v("7.多维尺度分析")]),t._v(" "),s("p",[t._v("多维尺度分析试图寻找原始高维空间数据的距离的良好低维表征。")]),t._v(" "),s("p",[t._v("简单来说，多维尺度分析被用于数据的相似性，它试图用几何空间中的距离来建模数据的相似性，即用二维空间中的距离来表示高维空间的关系。")]),t._v(" "),s("p",[t._v("数据可以是物体之间的相似度、分子之间的交互频率或国家间交易指数，而且是基于欧式距离的距离矩阵。")]),t._v(" "),s("p",[t._v("多维尺度分析算法是一个不断迭代的过程，因此，需要使用 max_iter 来指定较大迭代次数，同时计算的耗时也是上面算法中较大的一个。")]),t._v(" "),s("p",[t._v("from sklearn import manifold")]),t._v(" "),s("p",[t._v("from sklearn.metrics import euclidean_distances")]),t._v(" "),s("p",[t._v("similarities = euclidean_distances(data.ix[:,:-1].values)")]),t._v(" "),s("p",[t._v('mds = manifold.MDS(n_components=2, max_iter=3000, eps=1e-9, dissimilarity="precomputed",')]),t._v(" "),s("p",[t._v("n_jobs=1)")]),t._v(" "),s("p",[t._v("X = mds.fit(similarities).embedding_")]),t._v(" "),s("p",[t._v("pos=pd.DataFrame(X, columns=['X', 'Y'])")]),t._v(" "),s("p",[t._v("pos['species'] = data['species']")]),t._v(" "),s("p",[t._v("ax = pos.ix[pos['species']=='virginica'].")]),t._v(" "),s("p",[t._v("plot(kind='scatter', x='X', y='Y', color='blue', label='virginica')")]),t._v(" "),s("p",[t._v("ax = pos.ix[pos['species']=='setosa'].")]),t._v(" "),s("p",[t._v("plot(kind='scatter', x='X', y='Y', color='green', label='setosa', ax=ax)")]),t._v(" "),s("p",[t._v("pos.ix[pos['species']=='versicolor'].")]),t._v(" "),s("p",[t._v("plot(kind='scatter', x='X', y='Y', color='red', label='versicolor', ax=ax)")]),t._v(" "),s("p",[t._v("=============================== 正则匹配统计\nimport re")]),t._v(" "),s("p",[t._v("pattern = re.compile(r'(."),s("em",[t._v(")_ck(.")]),t._v(")_(.*)')\ncks = {}")]),t._v(" "),s("p",[t._v('with open("hyclicktags.csv", "r") as f:\nfor line in f:\ntmp = int(line.split(",")[1])\nmatch = pattern.match(line)\nif match:\nif match.group(2) in cks:\nc = cks.get(match.group(2))\ncks[match.group(2)] = c + tmp\nelse:\ncks[match.group(2)] = tmp')]),t._v(" "),s("p",[t._v("count = len(cks)\nprint count")]),t._v(" "),s("p",[t._v("if count <= 200:\nfor i in cks:\nprint i, cks[i]")]),t._v(" "),s("p",[t._v("带有中文的正则\nimport re")]),t._v(" "),s("p",[t._v("pattern = re.compile(ur'([道|路]+)([0-9]+)号')\nline = u\"清华道 2 号\"\nmatch = pattern.match(line)")]),t._v(" "),s("p",[t._v("if match:\nprint match.group(0)\nprint match.group(3)")]),t._v(" "),s("p",[t._v("print pattern.search(line).group()")]),t._v(" "),s("p",[t._v("正则替换\n例如，替换括号中的内容\nimport re\nstr = '多摩君 1（英文版）c(ab)(34) '\nout = re.sub('（."),s("em",[t._v("?）|(.")]),t._v("?)', '', str)\nprint out")]),t._v(" "),s("p",[t._v("import re\nstr = '多摩君一区 '.decode('utf8')\nout = re.sub(u\"[一区]|[二区]\", '', str)\nprint out")]),t._v(" "),s("h3",{attrs:{id:"推荐这种方式"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#推荐这种方式"}},[t._v("#")]),t._v(" 推荐这种方式")]),t._v(" "),s("p",[t._v("import re\ndef remove_district(x):\nreturn re.sub(u\"[一区]|[二区]\", '', x).encode('utf8')\nprint remove_district(u'武汉一区')\n该函数可以直接用在 df 的列变换中")]),t._v(" "),s("p",[t._v('执行字符串语句\nx = eval("2+2")')]),t._v(" "),s("p",[t._v("加载模块的另一种方式")]),t._v(" "),s("p",[t._v("file, path_name, description = imp.find_module('env', [dir])")]),t._v(" "),s("h1",{attrs:{id:"这一步就是导入-env-这个模块，让-b-成为-a-类的别名"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#这一步就是导入-env-这个模块，让-b-成为-a-类的别名"}},[t._v("#")]),t._v(" 这一步就是导入 env 这个模块，让 B 成为 A 类的别名")]),t._v(" "),s("p",[t._v("B = imp.load_module('env', file,path_name, description).A")]),t._v(" "),s("p",[t._v("最频繁元素\nfrom collections import Counter")]),t._v(" "),s("p",[t._v("def Most_Common(lst):\ndata = Counter(lst)\nreturn data.most_common(1)[0][0]")]),t._v(" "),s("p",[t._v("在 python 中，True == 1, False == 0， 因此，在列表中是可以用 True/False 做下标取值的，例如：\n[exponential(12), exponential(2)][uniform() < 0.5] 相当于每次随机取一种分布的随机数")]),t._v(" "),s("h2",{attrs:{id:"numpy"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#numpy"}},[t._v("#")]),t._v(" numpy")]),t._v(" "),s("p",[t._v("numpy 101 题 https://www.machinelearningplus.com/101-numpy-exercises-python/")]),t._v(" "),s("p",[t._v("去重的时候顺便统计频次\n(values,counts) = np.unique(p,return_counts=True)\nind=np.argmax(counts)\nprint values[ind], counts[ind]")]),t._v(" "),s("p",[t._v("import numpy as np\nnp.log() 对序列做对数变换\nnp.exp()")]),t._v(" "),s("p",[t._v("每行的和\ndf['Col_sum'] = df.apply(lambda x: x.sum(), axis=1)\n注意： 对 series ， apply 的方法，不能写 axis=1")]),t._v(" "),s("p",[t._v("每列的和\ndf.loc['Row_sum'] = df.apply(lambda x: x.sum())")]),t._v(" "),s("p",[t._v('快速拼接两列\ndf["dh"] = df["partition_date"].map(str) + \'-\'+ df["hour"].map(str)')]),t._v(" "),s("p",[t._v("替换\narr[arr % 2 == 1] = -1")]),t._v(" "),s("p",[t._v("where 筛选\nx = np.where(arr % 2 == 1, -1, arr)")]),t._v(" "),s("p",[t._v("重复\nb = np.repeat(1, 10).reshape(2,-1)")]),t._v(" "),s("p",[t._v("np.r_[np.repeat(a, 3), np.tile(a, 3)]")]),t._v(" "),s("p",[t._v("公共元素\nnp.intersect1d(a,b)")]),t._v(" "),s("h1",{attrs:{id:"from-a-remove-all-of-b"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#from-a-remove-all-of-b"}},[t._v("#")]),t._v(" From 'a' remove all of 'b'")]),t._v(" "),s("p",[t._v("np.setdiff1d(a,b)")]),t._v(" "),s("p",[t._v("np.where(a == b)")]),t._v(" "),s("p",[t._v("index = np.where((a >= 5) & (a <= 10))\na[index]")]),t._v(" "),s("p",[t._v("标量函数向量化\npair_max = np.vectorize(maxx, otypes=[float])")]),t._v(" "),s("p",[t._v("boolean to int\ny_pred = (y_pred >= 0.5)*1")]),t._v(" "),s("h1",{attrs:{id:"scikit-learn"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#scikit-learn"}},[t._v("#")]),t._v(" scikit-learn")]),t._v(" "),s("p",[t._v("from sklearn.model_selection import train_test_split")]),t._v(" "),s("p",[t._v("x_train, x_test, y_train, y_test = train_test_split(x,y, random_state=42, test_size = 0.33)")]),t._v(" "),s("p",[t._v("线性模型\nfrom sklearn import linear_model\nlr = linear_model.LinearRegression()")]),t._v(" "),s("p",[t._v("训练模型\nmodel = lr.fit(x_train,y_train)")]),t._v(" "),s("p",[t._v("model.score() 返回被模型解释的方差的占比（即 R 方）")]),t._v(" "),s("p",[t._v("模型预测\npredicts = model.predict(x_test)")]),t._v(" "),s("p",[t._v("模型评估/评测\nfrom sklearn.metrics import mean_squared_error\nmean_squared_error(y_test, predicts)")]),t._v(" "),s("p",[t._v("岭回归\nridge = linear_model.Ridge(alpha=0.5)")]),t._v(" "),s("p",[t._v("ridge_model = ridge.fit(x_train, y_train)")]),t._v(" "),s("p",[t._v("one hot 编码\n单列 OneHotEncoder(sparse = False).fit_transform( testdata[['age']] )\n多列 OneHotEncoder(sparse = False).fit_transform( testdata['age', 'salary'])\nOneHotEncoder 不能对字符串型的值做处理, 因此需先用 LabelEncoder 对离散特征编码")]),t._v(" "),s("p",[t._v("LabelBinarizer().fit_transform(testdata['pet'])")]),t._v(" "),s("p",[t._v("==================== csv 文件处理")]),t._v(" "),s("p",[t._v("import pandas as pd")]),t._v(" "),s("p",[t._v("from datetime import datetime")]),t._v(" "),s("p",[t._v('data_location = "/Users/zhangxisheng/Documents/projects/商家不接待项目/refuse_samples.csv"')]),t._v(" "),s("p",[t._v("df = pd.read_csv(data_location, sep=',', header=0)")]),t._v(" "),s("p",[t._v('def str2date(date_str, formats="%Y-%m-%d %H:%M:%S"):\nreturn datetime.strptime(date_str, formats).date()')]),t._v(" "),s("p",[t._v("dt.weekday() 返回一周的第几天，注意，周一是 0")]),t._v(" "),s("p",[t._v('df["C"] = "" 空列')]),t._v(" "),s("p",[t._v("df['date'] = df['date'].map(lambda x: str2date(x))")]),t._v(" "),s("p",[t._v("print df.shape")]),t._v(" "),s("p",[t._v("print df.head(10)\nprint df['date'][:3]")]),t._v(" "),s("p",[t._v("print df['date'].value_counts()")]),t._v(" "),s("p",[t._v("聚合\nprint pd.pivot_table(df, values='fea#1', index=['fea#5'], columns='label', aggfunc=len).reset_index()")]),t._v(" "),s("p",[t._v("多列转一列 由多列合成一列\ndf['Value'] = df.apply(lambda row: my_test(row['a'], row['c']), axis=1)")]),t._v(" "),s("p",[t._v("列求和\nsmall.sales.sum()")]),t._v(" "),s("p",[t._v("shuffle 混洗\nfrom sklearn.utils import shuffle\ndf = shuffle(df)")]),t._v(" "),s("p",[t._v("划分的时候不要混洗\ntrain_test_split(y, shuffle=False)")]),t._v(" "),s("h3",{attrs:{id:"sklern"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#sklern"}},[t._v("#")]),t._v(" ============= sklern")]),t._v(" "),s("p",[t._v("有时候将 dataFrame 用 df.values 转为 ndarray 进行训练的时候，爆出如下错误\nInput contains NaN, infinity or a value too large for dtype('float32')")]),t._v(" "),s("p",[t._v('可以如下检查\nif np.any(np.isnan(train_feat)):\nprint "存在含有 null 值 的列"')]),t._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",[s("code",[t._v('    if np.all(np.isfinite(mat)):\n        print "存在非有限值的列"\n')])])]),s("p",[t._v("单列作为 ndarray， 要重塑\ndf.col.values.reshape(-1,1)")]),t._v(" "),s("p",[t._v("排序 pair，指定排序 key\n"),s("code",[t._v("sorted(model_bst.get_fscore().items(), key=lambda x: x[1], reverse=True)")])]),t._v(" "),s("p",[t._v("救急、自省\n查看包路径\nfrom distutils.sysconfig import get_python_lib\nprint(get_python_lib())")]),t._v(" "),s("p",[t._v("列出 object 及其内存\nimport sys\nipython"),s("em",[t._v("vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\nsorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('")]),t._v("') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)")]),t._v(" "),s("p",[t._v("data.faq.str.startswith('联系', na=False)")]),t._v(" "),s("p",[t._v("按重复次数过滤\ndata.groupby('case_id').filter(lambda x: len(x) >= 2)")]),t._v(" "),s("p",[t._v("np.meshgrid\nhttps://stackoverflow.com/questions/36013063/what-is-purpose-of-meshgrid-in-python-numpy 看了这个解释你还不知道这个函数是干啥的你打我！")]),t._v(" "),s("p",[t._v("从 url 读取图片")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("detect_and_draw")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("img_url"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    url "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"http://10.20.42.6:8415/TextDetect"')]),t._v("\n    r "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" requests"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("post"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("url"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" data"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("img_url"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" headers"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Content-Type'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'application/x-www-form-urlencoded'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    result "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" r"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("json"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'text'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#     img = Image.open(StringIO(urllib.urlopen(img_url).read()))")]),t._v("\n    img_file "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" cStringIO"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("StringIO"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("urllib"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("urlopen"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("img_url"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("read"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    img "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Image"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("open")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("img_file"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    img_data "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("array"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("img"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" dtype"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("np"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("uint8"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    fig"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("ax "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" plt"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("subplots"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    ax"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("imshow"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("img_data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"%d boxs detected"')]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("r"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("json"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'text'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" i "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" r"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("json"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'text'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        x "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("int")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("i"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'x0'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        y "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("int")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("i"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'y0'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        w "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("int")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("i"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'x1'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("int")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("i"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'x0'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        h "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("int")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("i"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'y1'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("int")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("i"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'y0'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        rect "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" patches"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Rectangle"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("y"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("w"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("h"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("linewidth"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" edgecolor"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'r'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("facecolor"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'none'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        ax"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add_patch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("rect"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    plt"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("show"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("假设检验")]),t._v(" "),s("p",[t._v("from scipy.stats import binom_test\nbinom_test(2, 8, 11/2364, alternative='greater')")]),t._v(" "),s("h1",{attrs:{id:"动画"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#动画"}},[t._v("#")]),t._v(" 动画")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" numpy "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" np\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" matplotlib "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" pyplot "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" plt\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" matplotlib "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" animation\n\nfig "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" plt"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("figure"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nfig"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("set_dpi"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("100")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nfig"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("set_size_inches"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("7")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("6.5")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nax "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" plt"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("axes"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("xlim"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" ylim"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\npatch "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" plt"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Circle"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.75")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" fc"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'y'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("init")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    patch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("center "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    ax"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add_patch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("patch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" patch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("animate")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("i"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" patch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("center\n    x "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" np"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sin"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("np"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("radians"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("i"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    y "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v(" np"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cos"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("np"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("radians"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("i"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    patch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("center "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" patch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n\nanim "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" animation"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("FuncAnimation"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("fig"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" animate"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                               init_func"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("init"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                               frames"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("360")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                               interval"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                               blit"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nplt"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("show"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 如果要在 jupyter notebook 中显示，则改为 `HTML(anim.to_jshtml())` 即可；不过，在 jupyter notebook 中显示动效真的很慢！")]),t._v("\n")])])]),s("p",[t._v("动画甚至可以保存为 mp4 动画")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("anim"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("save"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'/Users/zhangxisheng/Downloads/animation.mp4'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" fps"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n          extra_args"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'-vcodec'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'h264'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                      "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'-pix_fmt'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'yuv420p'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("h1",{attrs:{id:"输入框"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#输入框"}},[t._v("#")]),t._v(" 输入框")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" Tkinter "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("printData")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("firstName"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" lastName"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("firstName"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("lastName"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    root"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("destroy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("get_input")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n\n    firstName "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" entry1"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("get"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    lastName "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" entry2"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("get"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    printData"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("firstName"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" lastName"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\nroot "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Tk"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#Label 1")]),t._v("\nlabel1 "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Label"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("root"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("text "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'First Name'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nlabel1"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("pack"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nlabel1"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("config"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("justify "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" CENTER"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nentry1 "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Entry"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("root"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" width "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nentry1"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("pack"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nlabel3 "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Label"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("root"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" text"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Last Name"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nlabel3"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("pack"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nlabel1"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("config"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("justify "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" CENTER"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nentry2 "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Entry"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("root"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" width "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nentry2"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("pack"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nbutton1 "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Button"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("root"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" text "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'submit'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nbutton1"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("pack"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nbutton1"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("config"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("command "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" get_input"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nroot"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("mainloop"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n")])])]),s("p",[t._v("关于 tkinter")]),t._v(" "),s("p",[t._v("pack(side=TOP) 从上往下排列，而 side=LEFT 是从左往右排列")]),t._v(" "),s("p",[t._v("单选和复选框")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tkinter "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Radiobar")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Frame"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" parent"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" picks"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" side"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("TOP"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" anchor"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("W"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n       Frame"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("__init__"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" parent"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n       var "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" IntVar"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n       self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("var "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" var\n       i "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\n       "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" pick "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" picks"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        i "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" i"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\n        chk "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Radiobutton"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" text"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("pick"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" variable"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("var"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" value "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" i"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        chk"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("pack"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("side"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("side"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" anchor"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("anchor"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" expand"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("YES"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("state")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n      "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("var"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("get"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Checkbar")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Frame"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n   "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("__init__")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" parent"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" picks"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" side"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("LEFT"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" anchor"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("W"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n      Frame"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("__init__"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" parent"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n      self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("vars")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n      "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" pick "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" picks"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n         var "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" IntVar"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n         chk "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Checkbutton"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" text"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("pick"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" variable"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("var"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n         chk"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("pack"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("side"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("side"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" anchor"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("anchor"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" expand"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("YES"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n         self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("vars")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("var"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n   "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("state")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n      "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("map")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("lambda")]),t._v(" var"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" var"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("get"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" self"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("vars")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" __name__ "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'__main__'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n   root "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Tk"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n   "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# lng = Checkbar(root, ['Python', 'Ruby', 'Perl', 'C++'],side=TOP)")]),t._v("\n   "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# lng.pack(side=TOP,  fill=X)")]),t._v("\n   "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# lng.config(relief=GROOVE, bd=2)")]),t._v("\n   labels "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("u'类别'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("u'菜名和价格'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("u'菜名'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("u'价格'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("u'标题'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("u'标签'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("u'规格'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("u'其他文字'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("u'非文字'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n   lng "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Radiobar"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("root"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" picks "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n   lng"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("pack"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("side"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("TOP"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("  fill"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("X"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n   lng"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("config"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("relief"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("GROOVE"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" bd"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n   "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("allstates")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n      "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("lng"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("state"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n   Button"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("root"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" text"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Quit'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" command"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("root"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("quit"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("pack"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("side"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("RIGHT"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n   Button"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("root"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" text"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Peek'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" command"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("allstates"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("pack"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("side"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("RIGHT"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n   root"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("mainloop"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n   "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"down"')]),t._v("\n\n")])])]),s("p",[t._v("图像可以只用用 Image.open 读取； 如果要用 cve.read, 注意要转换 rgb 分量")]),t._v(" "),s("h1",{attrs:{id:""}},[s("a",{staticClass:"header-anchor",attrs:{href:"#"}},[t._v("#")]),t._v(" ======================================================")]),t._v(" "),s("p",[t._v("最地道的 python\n"),s("code",[t._v("def ecode(x): return 1 if x == 'a' else 0")])]),t._v(" "),s("p",[t._v("python3 中合并字典的方式")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("x "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'1'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\ny "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'6'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("6")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("**")]),t._v("x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("**")]),t._v("y"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# {'1': 4, '6': 6}")]),t._v("\n")])])]),s("p",[t._v('字符串倒序\nx = "abcd"\nx[::-1]')]),t._v(" "),s("p",[t._v("出现次数最多的元素")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("t "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("12")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("6")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("7")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("max")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("set")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("test"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" key "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" test"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("count"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("多个条件判断")]),t._v(" "),s("p",[t._v("math,English,computer =90,59,88\nif any([math<60,English<60,computer<60]):\nprint('not pass')")]),t._v(" "),s("h1",{attrs:{id:"通过程序，执行-shell-命令"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#通过程序，执行-shell-命令"}},[t._v("#")]),t._v(" 通过程序，执行 shell 命令")]),t._v(" "),s("p",[t._v('from subprocess import check_output\nprint(check_output(["ls", ".."]).decode("utf8"))')]),t._v(" "),s("p",[t._v("dataframe 通过旧列为新列赋值的方法\ndataset['IsAlone'].loc[dataset['FamilySize'] > 1] = 0")]),t._v(" "),s("p",[t._v('"v{:04d}".format(x)\n四位数字，不足的用 0 补全，格式化')]),t._v(" "),s("p",[t._v("利用装饰器实现统计函数运行时间注解")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" time\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("timer")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("fn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    job_name "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" fn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("__name__\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("new_fn")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("args"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        t1 "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" time"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("time"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'job:'")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" job_name "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("' start'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        result "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" fn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("args"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        t2 "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" time"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("time"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'job:'")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" job_name "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("' end, it costs '")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("str")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("t2 "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" t1"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("' s'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" result\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" new_fn\n\n"),s("span",{pre:!0,attrs:{class:"token decorator annotation punctuation"}},[t._v("@timer")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("f")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" x"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("\n\n")])])]),s("p",[t._v("pandas 性能提升")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" pandas "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" pd\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" dask"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("dataframe "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" dd\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" dask"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("multiprocessing "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" get\n\n\ndata "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("DataFrame"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ndata"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'col1'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("random"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("normal"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("size "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1500000")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ndata"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'col2'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("random"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("normal"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("size "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1500000")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nddata "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" dd"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("from_pandas"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" npartitions"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" timeit\n\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("myfunc")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("y"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" y"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("**")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("apply_myfunc_to_DF")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("apply")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("lambda")]),t._v(" row"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" myfunc"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("row"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" axis"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("pandas_apply")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" apply_myfunc_to_DF"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("dask_apply")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" ddata"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("map_partitions"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("apply_myfunc_to_DF"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("compute"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("get"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("get"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("vectorized")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" myfunc"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'col1'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'col2'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nt_pds "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" timeit"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Timer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("lambda")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" pandas_apply"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("t_pds"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("timeit"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("number"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 52.0947")]),t._v("\n\nt_dsk "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" timeit"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Timer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("lambda")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" dask_apply"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("t_dsk"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("timeit"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("number"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#19.0225")]),t._v("\n\nt_vec "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" timeit"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Timer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("lambda")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" vectorized"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("t_vec"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("timeit"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("number"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 0.025")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" swifter\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("apply_myfunc_to_DF_swiftly")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("swifter"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("apply")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("lambda")]),t._v(" row"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" myfunc"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("row"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" axis"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("pandas_apply_swiftly")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" apply_myfunc_to_DF_swiftly"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nt_swift "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" timeit"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Timer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("lambda")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" pandas_apply_swiftly"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("t_swift"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("timeit"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("number"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 74")]),t._v("\n\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("myfunc2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'new'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("apply")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("lambda")]),t._v(" row"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" myfunc"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("row"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" axis"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" df\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" numpy "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" np\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" multiprocessing "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" cpu_count"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("Pool\ncores "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" cpu_count"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#Number of CPU cores on your system")]),t._v("\npartitions "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" cores "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#Define as many partitions as you want")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("parallelize")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" func"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    data_split "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("array_split"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" partitions"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    pool "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Pool"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("cores"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    data "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("concat"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pool"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("map")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("func"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" data_split"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    pool"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("close"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    pool"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("join"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" data\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("apply_parall")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" parallelize"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("myfunc2"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nt_parall "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" timeit"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Timer"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("lambda")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" apply_parall"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("t_parall"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("timeit"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("number"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 18.73")]),t._v("\n\n\n")])])]),s("p",[t._v("可以看到，最快的还是向量化（在这个例子中，是 pandas 的 2000 倍）。所以，只要是能用向量化的地方，都尽量用向量化！\n对不太好向量化的函数，用 dask 要比 pandas 快 3 倍; 手动并行化几乎和 dask 差不多的效率。\n至于 swift, 看起来效果并不是很好")]),t._v(" "),s("p",[t._v("其他向量化的例子")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("\ndefdef  gt_5_bikes_vectorizedgt_5_bi "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" np"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("where"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("h1",{attrs:{id:"pythonic"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#pythonic"}},[t._v("#")]),t._v(" pythonic")]),t._v(" "),s("p",[t._v("python 3 中，合并字典的方式\nx={'a':2,'b':4}\ny={'a':5,'c':1}\nz = {**x,**y}\nz\n{'a': 5, 'b': 4, 'c': 1}\n注意，相同键，值用的是后一个。")]),t._v(" "),s("h1",{attrs:{id:"线性回归"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#线性回归"}},[t._v("#")]),t._v(" 线性回归")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" numpy "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" np\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" pandas "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" pd\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" matplotlib "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" pyplot "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" plt\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" seaborn "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" sns\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" sklearn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("linear_model "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" LinearRegression\n\n\n\nmodel2 "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" LinearRegression"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nmodel2"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("wdd"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("values"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("reshape"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("27")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("values"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("reshape"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("27")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nmatplotlib"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("rcParams"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'figure.figsize'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("6")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\nplt"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("scatter"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("wdd"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("values"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("values"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("color"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'g'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nplt"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("plot"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("wdd"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("values"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("reshape"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("27")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" model2"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("predict"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("wdd"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("values"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("reshape"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("27")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("color"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'k'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nplt"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("title"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('u"处理逃单量和逃单万订单之间的关系"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nplt"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("xlabel"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("u'逃单万订单'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nplt"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ylabel"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("u'处理逃单门店量'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nplt"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("show"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n")])])]),s("h2",{attrs:{id:"python-环境管理"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#python-环境管理"}},[t._v("#")]),t._v(" python 环境管理")]),t._v(" "),s("p",[t._v("Pipfile 与 Pipfile.lock 是社区拟定的依赖管理文件，用于替代过于简陋的 requirements.txt")]),t._v(" "),s("p",[t._v("pipenv: 结合了 Pipfile 、pip 和 virtualenv")]),t._v(" "),s("p",[t._v("conda 本身也可以创建")]),t._v(" "),s("div",{staticClass:"language-shell extra-class"},[s("pre",{pre:!0,attrs:{class:"language-shell"}},[s("code",[t._v("conda create -n tf "),s("span",{pre:!0,attrs:{class:"token assign-left variable"}},[t._v("python")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3.6")]),t._v(" anaconda\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 安装包既可以用 conda，可以激活环境之后用pip3安装")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# conda install -n tf -c https://conda.binstar.org/menpo opencv")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# pip3 install tensorflow==2.1")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# conda install -n tf pytorch torchvision -c pytorch")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# python3 -m pip install mxnet==1.4.1 gluonts")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#  Pycharm 等 IDE 在执行的时候，可以设置将 Source Root 加入到 PYTHONPATH，但如果是在 Shell 等运行，则我们的一些本地代码可能找不到")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 例如，我们执行 paradise 库下的一些代码，就会抱怨说找不到  paradise")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 这可以用如下方式解决： /Users/zhangxisheng/anaconda/envs/tf/lib/python3.6/site-packages/ 下创建一个 paradise.pth 文件，里面下上路径  `/Users/zhangxisheng/mtdp/paradise/` ,这样就可以了。")]),t._v("\n\n\n"),s("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v("source")]),t._v(" activate tf\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 我们在这个环境下安装了 tensorflow 最新版本,以及 simple tensorflow serving")]),t._v("\n/Users/zhangxisheng/anaconda/envs/tf/bin/tensorboard --logdir"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("''")]),t._v("\n\n\n\n"),s("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v("source")]),t._v(" deactivate\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 删除环境！")]),t._v("\nconda remove -n yourenvname --all\n\n")])])]),s("h2",{attrs:{id:"高级语法"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#高级语法"}},[t._v("#")]),t._v(" 高级语法")]),t._v(" "),s("ol",[s("li",[s("p",[t._v("jupyter 中，shift tab 一般是回退一个 tab，但是如果光标在函数上，会显示函数的注释")])]),t._v(" "),s("li",[s("p",[t._v("python3 中的 f-string")])])]),t._v(" "),s("p",[t._v("高级拆包")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("a"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("b "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# a")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 1")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# b")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# [2, 3]")]),t._v("\n\n\na"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("b"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("c "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("6")]),t._v("\n\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# in 代替 or")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" x "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("or")]),t._v(" x "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("or")]),t._v(" x "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n     "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("pass")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" x "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n     "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("pass")]),t._v("\n\n\n\n")])])]),s("h2",{attrs:{id:"常用"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#常用"}},[t._v("#")]),t._v(" 常用")]),t._v(" "),s("h3",{attrs:{id:"启动脚本个性化"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#启动脚本个性化"}},[t._v("#")]),t._v(" 启动脚本个性化")]),t._v(" "),s("p",[t._v("import site\nsite.getusersitepackages()")]),t._v(" "),s("p",[t._v("eg. /Users/zhangxisheng/.local/lib/python2.7/site-packages")]),t._v(" "),s("p",[t._v("然后建立 sitecustomized.py 脚本，里面的语句将在 python 启动的时候自动执行。")]),t._v(" "),s("p",[t._v("但是，sitecustomized.py 脚本中 import 的包，还是不能找到。")]),t._v(" "),s("p",[t._v("应该可以 用 PYTHONSTARTUP 变量指向，但是确实这样是不好的做法，所以还是放弃自动引入包。")]),t._v(" "),s("h3",{attrs:{id:"pep8"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#pep8"}},[t._v("#")]),t._v(" PEP8")]),t._v(" "),s("p",[t._v("有时候抱怨 "),s("code",[t._v("module level import not at top of file")]),t._v(", 可以注释 # noqa, 就不会被检查了")]),t._v(" "),s("h2",{attrs:{id:"疑难点"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#疑难点"}},[t._v("#")]),t._v(" 疑难点")]),t._v(" "),s("p",[t._v("df.to_csv('foo.txt',index=False,header=False, quoting=csv.QUOTE_NONE)")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("lst "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("6")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("7")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("8")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" x "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" lst"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" x "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("6")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        lst"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("remove"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nlst\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# [2, 4, 6, 7, 8]")]),t._v("\n")])])]),s("p",[t._v("因为迭代器这里是按照索引去得带的，当把 1 删除之后，2 其实成了列表 lst 的首个元素，但是迭代器还是取看第二个位置的，因此 2 被跳过了。")]),t._v(" "),s("p",[t._v("正确的写法")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("x "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" x "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" lst "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" x "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("6")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])])]),s("p",[t._v("Another solution would be to iterate in reverse. That way, no elements can be skipped since removing an item from the list will only affect the indexes of elements that were already handled. This can either be done manually using index-based iteration starting from the end, or using reversed():")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("lst "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("6")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("7")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("8")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" x "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("reversed")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("lst"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" x "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("6")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        lst"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("remove"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("Finally, one can also iterate over a copy of the list, so when removing elements from the original list, the iterator is not affected.")]),t._v(" "),s("div",{staticClass:"c-callout c-callout–warning"},[s("strong",{staticClass:"c-callout__title"},[t._v("Warning")]),t._v(" "),s("p",{staticClass:"c-callout__paragraph"},[t._v("\n不要在循环中用 remove 删除元素列表！\n")])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" i "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("i"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("break")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Not executed as there is a break")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"No Break"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("foo 循环后直接跟 else 的用法可还行？")]),t._v(" "),s("h3",{attrs:{id:"函数的默认参数"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#函数的默认参数"}},[t._v("#")]),t._v(" 函数的默认参数")]),t._v(" "),s("blockquote",[s("p",[t._v("Default parameter values are always evaluated when, and only when, the “def” statement they belong to is executed")])]),t._v(" "),s("p",[t._v("函数定义中绑定默认参数，而不是在函数执行时\n合理性： 函数也是对象，在定义时被执行得到的对象")]),t._v(" "),s("p",[s("strong",[t._v("def")]),t._v(" 是一个可执行语句！")]),t._v(" "),s("h2",{attrs:{id:"多进程并行-parallel-multiprocessing"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#多进程并行-parallel-multiprocessing"}},[t._v("#")]),t._v(" 多进程并行 Parallel Multiprocessing")]),t._v(" "),s("p",[t._v("GIL 是计算机程序设计语言解释器用于同步线程的一种机制，它使得任何时刻仅有一个线程在执行。即便在多核心处理器上，使用 GIL 的解释器也只允许同一时间执行一个线程。")]),t._v(" "),s("p",[t._v("因为 GIL 的存在，多线程看来是不可能的了，好在，还可以用 multiprocessing 开启多进程。")]),t._v(" "),s("p",[t._v("IO 密集型任务选择 multiprocessing.dummy，CPU 密集型任务选择 multiprocessing")]),t._v(" "),s("h3",{staticStyle:{color:"inherit","line-height":"inherit","margin-top":"1.6em","margin-bottom":"1.6em","font-weight":"bold","border-bottom":"2px solid rgb(239, 112, 96)","font-size":"1.3em"}},[s("span",{staticStyle:{"font-size":"inherit","line-height":"inherit",display:"inline-block","font-weight":"normal",background:"rgb(239, 112, 96)",color:"rgb(255, 255, 255)",padding:"3px 10px 1px","border-top-right-radius":"3px","border-top-left-radius":"3px","margin-right":"3px"}},[t._v("第一种")])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),t._v("time\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" joblib "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Parallel"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" delayed\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" multiprocessing "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" cpu_count\n\nexecutor "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Parallel"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("n_jobs"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("cpu_count"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" backend"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'multiprocessing'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntasks "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("delayed"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("preds_11"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("poiid"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" poiid "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("poiid"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("unique"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nresults "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" executor"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tasks"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n")])])]),s("h3",{staticStyle:{color:"inherit","line-height":"inherit","margin-top":"1.6em","margin-bottom":"1.6em","font-weight":"bold","border-bottom":"2px solid rgb(239, 112, 96)","font-size":"1.3em"}},[s("span",{staticStyle:{"font-size":"inherit","line-height":"inherit",display:"inline-block","font-weight":"normal",background:"rgb(239, 112, 96)",color:"rgb(255, 255, 255)",padding:"3px 10px 1px","border-top-right-radius":"3px","border-top-left-radius":"3px","margin-right":"3px"}},[t._v("第二种")])]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" multiprocessing "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Pool\npool "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Pool"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nresults "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pool"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("map")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("preds_11"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("poiid"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("unique"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\npool"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("close"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\npool"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("join"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("h2",{attrs:{id:"应用"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#应用"}},[t._v("#")]),t._v(" 应用")]),t._v(" "),s("h3",{attrs:{id:"替换目录下所有文件中的某个关键词"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#替换目录下所有文件中的某个关键词"}},[t._v("#")]),t._v(" 替换目录下所有文件中的某个关键词")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" os"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" re\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("change")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("s"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n\n    match "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" re"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("match"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'.*{% asset_img (.*) %}'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" s"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" match"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        url "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" match"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("group"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),t._v(" url\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'![]('")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" url "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("')\\n'")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" s\n\n\nroot "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"/Users/zhangxisheng/Documents/personal/grocery-store-of-lancezhange/section8/"')]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" dirpath"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" dirnames"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" filenames "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" os"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("walk"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("root"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" filepath "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" filenames"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        file_name "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" os"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("path"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("join"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dirpath"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" filepath"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" file_name"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("endswith"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'.md'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),t._v(" file_name\n            "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("with")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("open")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("file_name"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'r'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" f"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                lines "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" f"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("readlines"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("with")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("open")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("file_name"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v("'w'")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" f"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" line "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" lines"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                    line "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" change"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("line"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n                    f"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("write"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("line"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n")])])]),s("p",[t._v("cpython\n.pyx 文件")]),t._v(" "),s("p",[t._v("@cython.boundscheck(False) # Deactivate bounds checking\n@cython.wraparound(False) # Deactivate negative indexing.")]),t._v(" "),s("p",[t._v("series.dtype == np.float64:\nfi16 = np.finfo(np.float16)")]),t._v(" "),s("p",[t._v("series.dtype == np.int64: Machine limits for integer types.\nii8 = np.iinfo(np.int8)")]),t._v(" "),s("p",[t._v("ntol = npos + nneg")]),t._v(" "),s("h2",{attrs:{id:"python3-8"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#python3-8"}},[t._v("#")]),t._v(" python3.8")]),t._v(" "),s("h2",{attrs:{id:"参考资料"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#参考资料"}},[t._v("#")]),t._v(" 参考资料")]),t._v(" "),s("ul",[s("li",[s("p",[s("a",{attrs:{href:"http://pyzh.readthedocs.org/en/latest/python-questions-on-stackoverflow.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("stackoverflow 上一些 python 问题合集 "),s("OutboundLink")],1)])]),t._v(" "),s("li",[s("p",[s("a",{attrs:{href:"http://book.pythontips.com/en/latest/index.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("python tips/intermediate python"),s("OutboundLink")],1)])]),t._v(" "),s("li",[s("p",[s("a",{attrs:{href:"https://github.com/taizilongxu/interview_python",target:"_blank",rel:"noopener noreferrer"}},[t._v("关于 python 的面试题"),s("OutboundLink")],1)])])]),t._v(" "),s("ul",[s("li",[s("p",[s("a",{attrs:{href:"https://gist.github.com/sloria/7001839",target:"_blank",rel:"noopener noreferrer"}},[t._v("The Best of the Best Practices(BOBP) GUide for Python"),s("OutboundLink")],1)])]),t._v(" "),s("li",[s("p",[s("a",{attrs:{href:"https://github.com/shik3519/machine-learning/blob/master/tutorials/003-python-basics-numpy-regex.ipynb",target:"_blank",rel:"noopener noreferrer"}},[t._v("python-basics-numpy-regex"),s("OutboundLink")],1)])])])])}),[],!1,null,null,null);a.default=e.exports}}]);