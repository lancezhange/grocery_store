(window.webpackJsonp=window.webpackJsonp||[]).push([[47],{354:function(e,t,r){e.exports=r.p+"assets/img/2020-03-23-20-55-42.233d9f20.png"},556:function(e,t,r){"use strict";r.r(t);var a=r(17),n=Object(a.a)({},(function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("h1",{attrs:{id:"问答系统专题"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#问答系统专题"}},[e._v("#")]),e._v(" 问答系统专题")]),e._v(" "),a("p",[e._v("本章包含的问题，严格来说并不都是问答相关的，但均为自然语言处理和计算机视觉的交叉领域。")]),e._v(" "),a("p",[e._v("问题的三种类型：what(who, when, where 等实体事实型), how, why")]),e._v(" "),a("p",[e._v("参考")]),e._v(" "),a("p",[a("a",{attrs:{href:"http://mp.weixin.qq.com/s?__biz=MzIxNzE2MTM4OA==&mid=413066638&idx=1&sn=64f9469badfc5de2f3c59a4c9ad176d7&scene=2&srcid=01201tw7IA8ae8fXl9xyoKV4&from=timeline&isappinstalled=0#wechat_redirect",target:"_blank",rel:"noopener noreferrer"}},[e._v("智能问答技术综述"),a("OutboundLink")],1),e._v(" by 何世柱，et al.　文章对问答技术的发展和涉及的问题做了一个简要综述。")]),e._v(" "),a("h3",{attrs:{id:"图片描述-image-caption-专题"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#图片描述-image-caption-专题"}},[e._v("#")]),e._v(" 图片描述(Image Caption)专题")]),e._v(" "),a("p",[e._v("图片描述相比图片分类，物体识别等其他问题，是较为困难的，因为一方面要对图片中的场景做准确的理解，不仅要识别物体，还要识别物体之间的关联以及所属的活动，并且需要对时空关系等做一定的推断，一方面还需要结合自然语言处理对信息做一定的归纳之后生成文本描述，属于"),a("em",[e._v("交叉问题")]),e._v("（类似的还有图片问题回答）。")]),e._v(" "),a("ul",[a("li",[a("p",[a("a",{attrs:{href:"http://mscoco.org/dataset/#captions-challenge2015",target:"_blank",rel:"noopener noreferrer"}},[e._v("MS COCO Captioning Challenge"),a("OutboundLink")],1)]),e._v(" "),a("p",[e._v("比赛于 2015 年 4 月开始，5 月结束（但测评服务目前仍然开放），7 月份在 CVPR 2015 Large-scale Scene Understanding workshop 宣布结果。采用人工裁判，排在第一的还是人，后面两名则分别来自谷歌和微软（打了个平手）。")])]),e._v(" "),a("li",[a("p",[a("a",{attrs:{href:"https://github.com/ryankiros/neural-storyteller",target:"_blank",rel:"noopener noreferrer"}},[e._v("Neural Storyteller"),a("OutboundLink")],1)]),e._v(" "),a("p",[e._v("所谓的看图说话。相关示例参考博文"),a("a",{attrs:{href:"https://medium.com/@samim/generating-stories-about-images-d163ba41e4ed#.6heufs6ms",target:"_blank",rel:"noopener noreferrer"}},[e._v("Generating Stories About Images"),a("OutboundLink")],1)])]),e._v(" "),a("li",[a("p",[a("a",{attrs:{href:"http://www.cs.toronto.edu/~rkiros/adv_L.html",target:"_blank",rel:"noopener noreferrer"}},[e._v("Neural Artistic Captions"),a("OutboundLink")],1)]),e._v(" "),a("p",[e._v("诸多图片故事的例子。")])])]),e._v(" "),a("ul",[a("li",[a("a",{attrs:{href:"http://arxiv.org/abs/1411.4555",target:"_blank",rel:"noopener noreferrer"}},[e._v("Show and tell: A Neural image Caption Generator, by Oriol Vinyals, et al., Google, 2014"),a("OutboundLink")],1),e._v("\n借鉴机器翻译中 RNN-RNN 这样端到端的方式，将编码的 RNN 替换为 CNN（因为 CNN 久经考验，已经被公认能够为输入图片产生丰富的表示，然后只要将此表示嵌入一个定长向量，后面能做的事情就非常多了），即成 CNN - RNN, 称此模型为 NIC(neural iamge caption).")])]),e._v(" "),a("p",[e._v("代码实现\nkarpathy 大神的"),a("a",{attrs:{href:"https://github.com/karpathy/neuraltalk",target:"_blank",rel:"noopener noreferrer"}},[e._v("neuraltalk"),a("OutboundLink")],1),e._v(" 以及其进化版"),a("a",{attrs:{href:"https://github.com/karpathy/neuraltalk2",target:"_blank",rel:"noopener noreferrer"}},[e._v("neuraltalk2"),a("OutboundLink")],1),e._v(". neuraltalk 是用 python 实现的，而 neuraltalk2 用 torch 并运行在 GPU 上, 因此后者的速度比前者有显著的提升。新图片的 CNN feature 用 VGG 架构提取（得到顶层的 4096 维的激活子）。")]),e._v(" "),a("ul",[a("li",[a("a",{attrs:{href:"http://arxiv.org/abs/1502.03044",target:"_blank",rel:"noopener noreferrer"}},[e._v("Show, Attend and Tell: Neural Image Caption Generation with Visual Attention"),a("OutboundLink")],1)])]),e._v(" "),a("ul",[a("li",[a("p",[a("a",{attrs:{href:"http://arxiv.org/abs/1512.01337",target:"_blank",rel:"noopener noreferrer"}},[e._v("Neural Generative Question Answering by Jun YIn, et al., 2015"),a("OutboundLink")],1)]),e._v(" "),a("p",[e._v("仍然是标配：端到端的编码和解码　＋　注意机制")])])]),e._v(" "),a("p",[e._v("数据")]),e._v(" "),a("p",[e._v("Flicker8k, Flicker30k, MS-COCO")]),e._v(" "),a("hr"),e._v(" "),a("h3",{attrs:{id:"视频描述"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#视频描述"}},[e._v("#")]),e._v(" 视频描述")]),e._v(" "),a("p",[e._v("视频内容描述是图片内容描述的自然延伸，在视频搜索，人机交互，为视力缺陷者讲述内容等场景中均有切实的需求。")]),e._v(" "),a("p",[e._v("图片的内容描述，最后生成的句子长度是不定长的，在视频描述中，多了一个不定长的变量：图片帧的个数。此外，现实视频中的物体繁多，场景多样，动作各异，这为视频内容描述带来了不小的困难。如何识别出最主要的内容也是难点，因为我们只想描述视频中最主要的部分，而不是面面俱到。")]),e._v(" "),a("p",[e._v("LSTM 模型的成功。")]),e._v(" "),a("p",[e._v("后来又加入了 attension(注意机制)")]),e._v(" "),a("ul",[a("li",[a("p",[a("a",{attrs:{href:"http://shikharsharma.com/projects/action-recognition-attention/",target:"_blank",rel:"noopener noreferrer"}},[e._v("action recognition using visual attension"),a("OutboundLink")],1)]),e._v(" "),a("p",[e._v("基于 soft attension 模型的视频中动作识别。")])]),e._v(" "),a("li",[a("p",[a("a",{attrs:{href:"http://arxiv.org/abs/1505.00487v3",target:"_blank",rel:"noopener noreferrer"}},[e._v("Sentence to Sequence -- Video to Text"),a("OutboundLink")],1)]),e._v(" "),a("p",[e._v("2015.10. 基于 (video, sentence) pair 训练出 LSTM 模型，称为 S2VT，video 包括像素值和相邻帧的光流。")])])]),e._v(" "),a("p",[e._v("基准视频数据集")]),e._v(" "),a("ol",[a("li",[e._v("MSVD (microsoft video description corpus)")]),e._v(" "),a("li",[e._v("MPII-MD (MPII movie description dataset)")]),e._v(" "),a("li",[e._v("M-VAD (montreal video annotation dataset)")])]),e._v(" "),a("ul",[a("li",[a("p",[a("a",{attrs:{href:"https://sites.google.com/site/describingmovies/",target:"_blank",rel:"noopener noreferrer"}},[e._v("LSMDC 2015"),a("OutboundLink")],1)]),e._v(" "),a("p",[e._v("The Large Scale MOvie Description Challenge, at ICCV 2015.")])])]),e._v(" "),a("h3",{attrs:{id:"vqa-visual-question-answering"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#vqa-visual-question-answering"}},[e._v("#")]),e._v(" VQA(Visual Question Answering)")]),e._v(" "),a("p",[e._v("基于视觉的问题回答，包括回答一些关于图片或者视频的问题，对实现人机交互的智能梦想具有重要意义。本文中我们只考虑图片的问题，视频的以后再说。")]),e._v(" "),a("p",[e._v("一般而言，要回答提问者关于图片的问题，首先需要对图片中的场景和物体做出准确的识别，这就需要机器视觉方面的技术，例如物体检测，图像分割(image segmentation)等；其次，提问和回答一般都是以文字的形式（如果是语音，还要依赖语音识别技术转化为文字），因此还需要自然语言处理相关的技术。由于需要"),a("strong",[e._v("机器视觉")]),e._v("和"),a("strong",[e._v("自然语言处理")]),e._v("两大技术协同作战，VQA 需要的智能程度是相当高的，因此也被认为可以取代图灵测试来衡量智能水平。")]),e._v(" "),a("p",[e._v("问题的类型大致包括：主体(waht)，数量，颜色，位置")]),e._v(" "),a("p",[e._v("进一步，如果问题的回答需要一定程度的逻辑推理呢？")]),e._v(" "),a("ul",[a("li",[a("p",[a("a",{attrs:{href:"http://www.visualqa.org/",target:"_blank",rel:"noopener noreferrer"}},[e._v("VQA 挑战"),a("OutboundLink")],1)]),e._v(" "),a("p",[e._v("超过 25 万张图片，75 万个问题，值得关注！下面是一个例子")])])]),e._v(" "),a("p",[a("img",{attrs:{src:r(354),alt:""}})]),e._v(" "),a("ul",[a("li",[a("a",{attrs:{href:"http://www.visualqa.org/VQA_ICCV2015.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("VQA_ICCV2015"),a("OutboundLink")],1)])]),e._v(" "),a("p",[e._v("DAQUAR 数据集，大约含 1500 张图片，关于 37 类物体的约 7000 个问题。")]),e._v(" "),a("ul",[a("li",[a("p",[a("a",{attrs:{href:"http://arxiv.org/abs/1505.02074",target:"_blank",rel:"noopener noreferrer"}},[e._v("Exploring Models and Data for Image Question Answering, by Mengye Ren, et al., NIPS 2015(to appear)"),a("OutboundLink")],1)]),e._v(" "),a("p",[e._v("一篇综述文章,，代码"),a("a",{attrs:{href:"https://github.com/renmengye/imageqa-publi",target:"_blank",rel:"noopener noreferrer"}},[e._v("在此"),a("OutboundLink")],1),e._v("\n利用视觉语义嵌入(visual semantic embeddings) 连接 CNN 和 RNN.\n还附带了一个将原有的图片描述转换为 QA 的算法，由此可以生成大量的数据集供使用，例如文中将 MS-COCO 数据集扩展为 MS-COCO-QA.\n假定回答只是一个单词，并将问题理解为分类问题，这是比较局限的地方。")]),e._v(" "),a("p",[e._v("起初我觉得这种方式的回答是有问题的：当算法返回说沙发上是枕头的时候，它其实并没有对枕头做出识别，而仅仅是因为训练数据中，沙发上有枕头的比较多！可是后来一想：算法是怎么知道沙发上的枕头比较多的呢，不还是因为它对沙发和枕头有一定的识别能力吗！")])])]),e._v(" "),a("ul",[a("li",[a("p",[a("a",{attrs:{href:"http://arxiv.org/abs/1512.02167",target:"_blank",rel:"noopener noreferrer"}},[e._v("Simple Baseline for Visual Question Answering by Bolei Zhou, et al., 2015"),a("OutboundLink")],1)]),e._v(" "),a("p",[a("a",{attrs:{href:"https://github.com/metalbubble/VQAbaseline",target:"_blank",rel:"noopener noreferrer"}},[e._v("代码在此"),a("OutboundLink")],1)])])]),e._v(" "),a("ul",[a("li",[a("a",{attrs:{href:"http://www.cs.toronto.edu/~mbweb/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Aligning Books and Movies:\nTowards Story-like Visual Explanations by Watching Movies and Reading Books 　 by YuKun Zhu, et al., 2015"),a("OutboundLink")],1)])]),e._v(" "),a("h2",{attrs:{id:"开放域问答"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#开放域问答"}},[e._v("#")]),e._v(" 开放域问答")])])}),[],!1,null,null,null);t.default=n.exports}}]);