(window.webpackJsonp=window.webpackJsonp||[]).push([[100],{513:function(e,r,n){"use strict";n.r(r);var t=n(17),o=Object(t.a)({},(function(){var e=this,r=e.$createElement,n=e._self._c||r;return n("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[n("h1",{attrs:{id:"前沿"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#前沿"}},[e._v("#")]),e._v(" 前沿")]),e._v(" "),n("ul",[n("li",[n("p",[n("a",{attrs:{href:"http://arxiv.org/abs/1511.09249",target:"_blank",rel:"noopener noreferrer"}},[e._v("On Learning to Think: Algorithmic Information Theory for Novel Combinations of Reinforcement Learning Controllers and Recurrent Neural World Models by Juergen Schmidhuber, 2015"),n("OutboundLink")],1)]),e._v(" "),n("p",[e._v("learning to think.")])])]),e._v(" "),n("ul",[n("li",[n("p",[n("a",{attrs:{href:"http://www.sciencemag.org/content/350/6266/1332.full.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("Human-level concept learning through probabilistic program induction"),n("OutboundLink")],1)]),e._v(" "),n("p",[e._v("发表在《科学》上的文章，通过概率推理来学习概念，在这到处都是深度学习的万花丛中，真是一抹别样红啊。"),n("a",{attrs:{href:"https://github.com/brendenlake/BPL",target:"_blank",rel:"noopener noreferrer"}},[e._v("matlab 代码在此"),n("OutboundLink")],1)]),e._v(" "),n("blockquote",[n("p",[e._v("People learning new concepts can often generalize successfully from just a single example, yet machine learning algorithms typically require tens or hundreds of examples to perform with similar accuracy. People can also use learned concepts in richer ways than conventional algorithms—for action, imagination, and explanation.")])]),e._v(" "),n("p",[e._v("作者指出了两点人类学习和机器学习新概念的区别：人类学习新的概念是小样本学习(one-shot learning)，而现在的机器学习是大样本的；人类对学习到的概念能够更为灵活和广泛地使用。")]),e._v(" "),n("p",[e._v("文中提出的新框架：BPL(Bayesian Program Learning, 贝叶斯程式学习)，宣称结合了三个关键想法：组合、因果、学习如何学习。核心在于，用概率程式表示概念，而概率程式由低级概念的程式结合空间关系等组合出来。生成模型的生成模型。")])])]),e._v(" "),n("ul",[n("li",[n("p",[n("a",{attrs:{href:"http://illustration2vec.net/papers/illustration2vec-main.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("Illustration2Vec: A Semantic Vector Representation of Illustrations"),n("OutboundLink")],1)]),e._v(" "),n("p",[e._v("漫画的语义向量表示。日本东北大学（Tohoku University，简称东北大）和东京大学的研究人员联合出品。")]),e._v(" "),n("p",[e._v("难点：漫画相比于普通的图片，其形态更加丰富；缺少相关数据集")]),e._v(" "),n("p",[e._v("还提出了一种语义形变(semantic morphing)算法用以在大量漫画数据中中搜寻相关漫画。")])])]),e._v(" "),n("ul",[n("li",[n("p",[n("a",{attrs:{href:"https://github.com/ZhengyaoJiang/PGPortfolio",target:"_blank",rel:"noopener noreferrer"}},[e._v("A Deep Reinforcement Learning Framework for the Financial Portfolio Management Problem"),n("OutboundLink")],1),e._v(" (利用深度强化学习框架解决金融投资组合管理问题)")])]),e._v(" "),n("li",[n("p",[n("a",{attrs:{href:"https://www.muglife.com/",target:"_blank",rel:"noopener noreferrer"}},[e._v("mug life"),n("OutboundLink")],1),e._v("\nMug Life is a new innovative mobile app which leverages deep learning to let you instantly create 3D animations from any uploaded photo. 图片也能动起来了，效果惊人")])])]),e._v(" "),n("p",[e._v("toread\nQuora 重复问题检测 https://www.linkedin.com/pulse/duplicate-quora-question-abhishek-thakur")]),e._v(" "),n("ul",[n("li",[n("a",{attrs:{href:"https://deepmind.com/blog/producing-flexible-behaviours-simulated-environments/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Producing flexible behaviours in simulated environments"),n("OutboundLink")],1)])]),e._v(" "),n("p",[e._v("人偶模仿，真的太厉害了。\n"),n("img",{attrs:{src:"https://storage.googleapis.com/deepmind-live-cms/documents/ezgif.com-resize.gif",alt:""}})]),e._v(" "),n("ul",[n("li",[n("a",{attrs:{href:"https://github.com/ZheC/Realtime_Multi-Person_Pose_Estimation",target:"_blank",rel:"noopener noreferrer"}},[e._v("实时多人姿态估计"),n("OutboundLink")],1),e._v("\n太炫酷了。")])]),e._v(" "),n("p",[e._v("问题难点 1.人数是不确定的，并且他们所处 location 及 scale 也是不确定的。 2.人们之间的互动产生的遮挡、四肢变化、肢体重叠等有着复杂的空间关系 3.实时性，很多算法随着人数的增多而增加")]),e._v(" "),n("p",[e._v("关键点\nPart Affinity Fields\n没有采用 top-down 的方式（先检测到人，再估计人的姿态），而是部位检测并关联，属于 bottom up，并且，对部位的检测和关联是联合学习的")]),e._v(" "),n("p",[e._v("点的关联通过计算一个 部位仿射分")]),e._v(" "),n("ul",[n("li",[e._v("normalizing flow\n标准化流\nNormalizing flows transform simple densities (like Gaussians) into rich complex distributions that can be used for generative models, RL, and variational inference")])]),e._v(" "),n("p",[e._v("local reparameterization 局部再参数化")]),e._v(" "),n("p",[e._v("NF 乃是对一个原始分布的一系列可逆变换")]),e._v(" "),n("p",[n("a",{attrs:{href:"https://github.com/pymc-devs/pymc3/blob/master/docs/source/notebooks/normalizing_flows_overview.ipynb",target:"_blank",rel:"noopener noreferrer"}},[e._v("Normalizing Flow in pymc3"),n("OutboundLink")],1)]),e._v(" "),n("ul",[n("li",[e._v("Dynamic Routing Between Capsules\n胶囊网络\n胶囊之间的动态路由")])]),e._v(" "),n("p",[e._v("所谓胶囊，就是向量（区别于神经元为标量）")]),e._v(" "),n("ul",[n("li",[e._v("active learning 主动学习")])]),e._v(" "),n("p",[e._v("参考")]),e._v(" "),n("ol",[n("li",[n("a",{attrs:{href:"https://www.jianshu.com/p/42801f031cfa",target:"_blank",rel:"noopener noreferrer"}},[e._v("Active Learning: 一个降低深度学习时间，空间，经济成本的解决方案"),n("OutboundLink")],1)])]),e._v(" "),n("p",[e._v("Fine-tuning Convolutional Neural Networks for Biomedical Image Analysis: Actively and Incrementally”。\n问题：如何使用尽可能少的标签数据来训练一个效果 promising 的分类器\n第一种情况：标签数据太少，标注数据的成本太高\n第二种情况：数据太多，无法一次性处理")]),e._v(" "),n("p",[e._v("分类器的性能随着样本量的增多，先提高，到一个阈值之后几乎稳定。那么问题是，如果有效地降低这个阈值？\n解决办法：主动学习。去主动学习那些信息量大的样本。")]),e._v(" "),n("p",[e._v("哪些是这样的样本呢？很容易想到的选择标准是，1.对熵大的（例如二分类问题中，预测概率在 0.5 附近的），2.多样性（对于来自同一幅 image 的增广 patch 集，如果它们的分类结果高度不统一了，那么这个 image 就是 Important 的，或者 hard sample）")]),e._v(" "),n("p",[e._v('The way to create something beautiful is often to make subtle tweaks to something that already exists, or to combine existing ideas in a slightly new way.\n-- "Hackers & Painters"')]),e._v(" "),n("p",[n("a",{attrs:{href:"https://github.com/google/active-learning",target:"_blank",rel:"noopener noreferrer"}},[e._v("Active Learning Playground"),n("OutboundLink")],1)]),e._v(" "),n("ul",[n("li",[e._v("弱监督学习\nweak Supervision\n当你的标注数据噪音很大，质量很差，")])]),e._v(" "),n("p",[e._v("参考\n"),n("a",{attrs:{href:"https://hazyresearch.github.io/snorkel/blog/ws_blog_post.html",target:"_blank",rel:"noopener noreferrer"}},[e._v("Weak Supervision: The New Programming Paradigm for Machine Learning"),n("OutboundLink")],1)]),e._v(" "),n("ul",[n("li",[e._v("AVB(Adversarial Variational Bayes)")])]),e._v(" "),n("p",[e._v("对抗式变分贝叶斯\n统一自编码器和 GANs")]),e._v(" "),n("p",[e._v("变分自编码对编码器添加了约束，就是强迫它产生服从单位高斯分布的潜在变量。正是这种约束，把 VAE 和标准自编码器给区分开来了。\n相当于我们又了两个目标，一是重建误差要小，二是压缩表示变量和单位高斯分布之间的差异要小。\n两个目标之间需要权衡。")]),e._v(" "),n("p",[e._v("我们可以让网络自己去决定这种权衡。对于我们的损失函数，我们可以把这两方面进行加和。一方面，是图片的重构误差，我们可以用平均平方误差来度量，另一方面。我们可以用 KL 散度（KL 散度介绍）来度量我们潜在变量的分布和单位高斯分布的差异。")]),e._v(" "),n("p",[e._v("为了优化 KL 散度，我们需要应用一个简单的参数重构技巧：不像标准自编码器那样产生实数值向量，VAE 的编码器会产生两个向量:一个是均值向量，一个是标准差向量。")]),e._v(" "),n("p",[e._v("参考\n"),n("a",{attrs:{href:"https://arxiv.org/pdf/1701.04722.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("Adversarial Variational Bayes: Unifying Variational Autoencoders and Generative Adversarial Networks"),n("OutboundLink")],1)]),e._v(" "),n("ul",[n("li",[e._v("量子计算")])]),e._v(" "),n("p",[n("a",{attrs:{href:"https://www.technologyreview.com/s/610250/hello-quantum-world/",target:"_blank",rel:"noopener noreferrer"}},[e._v("MIT 评论： Serious quantum computers are finally here. What are we going to do with them?"),n("OutboundLink")],1)]),e._v(" "),n("h3",{attrs:{id:"其他待看"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#其他待看"}},[e._v("#")]),e._v(" 其他待看")]),e._v(" "),n("p",[n("a",{attrs:{href:"https://github.com/jparkhill/TensorMol",target:"_blank",rel:"noopener noreferrer"}},[e._v("TensorMol"),n("OutboundLink")],1)]),e._v(" "),n("p",[n("a",{attrs:{href:"https://modeldepot.io/",target:"_blank",rel:"noopener noreferrer"}},[e._v("ModelDepot"),n("OutboundLink")],1),e._v("\n一系列模型")]),e._v(" "),n("p",[n("a",{attrs:{href:"https://github.com/greenelab/deep-review",target:"_blank",rel:"noopener noreferrer"}},[e._v("deep review"),n("OutboundLink")],1),e._v("\n论文协作")]),e._v(" "),n("p",[n("a",{attrs:{href:"https://github.com/kentsyx/Neural-IMage-Assessment",target:"_blank",rel:"noopener noreferrer"}},[e._v("Neural IMage Assessment"),n("OutboundLink")],1)]),e._v(" "),n("p",[n("a",{attrs:{href:"https://github.com/geek-ai/Texygen/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Texygen"),n("OutboundLink")],1),e._v("\n文本生成基线平台")]),e._v(" "),n("p",[n("a",{attrs:{href:"https://github.com/geek-ai/MAgent",target:"_blank",rel:"noopener noreferrer"}},[e._v("MAgent"),n("OutboundLink")],1),e._v("\n多玩家强化学习平台")]),e._v(" "),n("p",[n("a",{attrs:{href:"https://github.com/Featuretools/predict_next_purchase",target:"_blank",rel:"noopener noreferrer"}},[e._v("predict next purchase"),n("OutboundLink")],1)]),e._v(" "),n("p",[e._v("precision medicine")]),e._v(" "),n("p",[n("a",{attrs:{href:"https://en.wikipedia.org/wiki/FLAME_clustering",target:"_blank",rel:"noopener noreferrer"}},[e._v("FLAME Clustering"),n("OutboundLink")],1),e._v("\nFuzzy clustering by Local Approximation of MEmberships")]),e._v(" "),n("p",[e._v("模糊聚类: 一个元素可以属于多个类")]),e._v(" "),n("p",[e._v("局部的成员估计法\n做法：")]),e._v(" "),n("ol",[n("li",[e._v("先抽取 KNN 关系，然后根据 KNN 关系，为每一个元素赋予一个密度值。根据这个密度值，将元素分成三类：\n"),n("ol",[n("li",[e._v("密度值大于其所有近邻的，称为类的支持点")]),e._v(" "),n("li",[e._v("异常值：密度值小于所有近邻切低于预设的阈值的")]),e._v(" "),n("li",[e._v("其他")])])]),e._v(" "),n("li",[e._v("每个支持点独为一类；所有异常值为一类；")])])])}),[],!1,null,null,null);r.default=o.exports}}]);