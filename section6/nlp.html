<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>自然语言处理专题 {ignore=true} | Lancezhange Vuepress Book</title>
    <meta name="description" content="Learner">
    <meta name="generator" content="VuePress 1.3.1">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/github-markdown-css/2.10.0/github-markdown.min.css">
  <link rel="stylesheet" href="/grocery_store/css/index.css">
    
    <link rel="preload" href="/grocery_store/assets/css/0.styles.892fb305.css" as="style"><link rel="preload" href="/grocery_store/assets/js/app.47d3b9e2.js" as="script"><link rel="preload" href="/grocery_store/assets/js/3.e15ba04f.js" as="script"><link rel="preload" href="/grocery_store/assets/js/45.9345a9b8.js" as="script"><link rel="prefetch" href="/grocery_store/assets/js/10.666a773f.js"><link rel="prefetch" href="/grocery_store/assets/js/100.2b4d646e.js"><link rel="prefetch" href="/grocery_store/assets/js/101.de9d1a6c.js"><link rel="prefetch" href="/grocery_store/assets/js/102.85a31c0d.js"><link rel="prefetch" href="/grocery_store/assets/js/103.4d080f12.js"><link rel="prefetch" href="/grocery_store/assets/js/104.54bedf13.js"><link rel="prefetch" href="/grocery_store/assets/js/105.60e9e099.js"><link rel="prefetch" href="/grocery_store/assets/js/106.46204467.js"><link rel="prefetch" href="/grocery_store/assets/js/107.bd63530e.js"><link rel="prefetch" href="/grocery_store/assets/js/108.85a7fb4a.js"><link rel="prefetch" href="/grocery_store/assets/js/109.b3cce710.js"><link rel="prefetch" href="/grocery_store/assets/js/11.a878cb18.js"><link rel="prefetch" href="/grocery_store/assets/js/110.b71213ca.js"><link rel="prefetch" href="/grocery_store/assets/js/111.88e3c009.js"><link rel="prefetch" href="/grocery_store/assets/js/112.672c0e9c.js"><link rel="prefetch" href="/grocery_store/assets/js/113.9f626878.js"><link rel="prefetch" href="/grocery_store/assets/js/114.e971f677.js"><link rel="prefetch" href="/grocery_store/assets/js/115.32931221.js"><link rel="prefetch" href="/grocery_store/assets/js/116.7d7bb8e9.js"><link rel="prefetch" href="/grocery_store/assets/js/117.d1d64952.js"><link rel="prefetch" href="/grocery_store/assets/js/118.371e7dde.js"><link rel="prefetch" href="/grocery_store/assets/js/119.7187d3ac.js"><link rel="prefetch" href="/grocery_store/assets/js/12.7971495d.js"><link rel="prefetch" href="/grocery_store/assets/js/120.3f89d6c5.js"><link rel="prefetch" href="/grocery_store/assets/js/121.78a87a30.js"><link rel="prefetch" href="/grocery_store/assets/js/122.d2df534e.js"><link rel="prefetch" href="/grocery_store/assets/js/123.c95b11ab.js"><link rel="prefetch" href="/grocery_store/assets/js/124.0fb28081.js"><link rel="prefetch" href="/grocery_store/assets/js/125.ff76a45e.js"><link rel="prefetch" href="/grocery_store/assets/js/126.9328300b.js"><link rel="prefetch" href="/grocery_store/assets/js/127.a6a2f599.js"><link rel="prefetch" href="/grocery_store/assets/js/128.4f59b6b2.js"><link rel="prefetch" href="/grocery_store/assets/js/129.7ad23ac2.js"><link rel="prefetch" href="/grocery_store/assets/js/13.c108b008.js"><link rel="prefetch" href="/grocery_store/assets/js/130.ff450918.js"><link rel="prefetch" href="/grocery_store/assets/js/131.0c23ac4e.js"><link rel="prefetch" href="/grocery_store/assets/js/132.39985100.js"><link rel="prefetch" href="/grocery_store/assets/js/133.97d11e72.js"><link rel="prefetch" href="/grocery_store/assets/js/134.f7f5be44.js"><link rel="prefetch" href="/grocery_store/assets/js/135.8d032916.js"><link rel="prefetch" href="/grocery_store/assets/js/136.5f8428a0.js"><link rel="prefetch" href="/grocery_store/assets/js/137.68ec6b04.js"><link rel="prefetch" href="/grocery_store/assets/js/138.594feaf1.js"><link rel="prefetch" href="/grocery_store/assets/js/139.712936f1.js"><link rel="prefetch" href="/grocery_store/assets/js/14.c8cecf72.js"><link rel="prefetch" href="/grocery_store/assets/js/140.e462485b.js"><link rel="prefetch" href="/grocery_store/assets/js/141.f7e3b64c.js"><link rel="prefetch" href="/grocery_store/assets/js/142.a1f358ef.js"><link rel="prefetch" href="/grocery_store/assets/js/143.7bd36db2.js"><link rel="prefetch" href="/grocery_store/assets/js/144.cfa08961.js"><link rel="prefetch" href="/grocery_store/assets/js/145.8d25c175.js"><link rel="prefetch" href="/grocery_store/assets/js/146.8205975e.js"><link rel="prefetch" href="/grocery_store/assets/js/147.cf326d2a.js"><link rel="prefetch" href="/grocery_store/assets/js/148.0d0d0e35.js"><link rel="prefetch" href="/grocery_store/assets/js/149.f913ee12.js"><link rel="prefetch" href="/grocery_store/assets/js/15.b2a89722.js"><link rel="prefetch" href="/grocery_store/assets/js/150.b5d4816c.js"><link rel="prefetch" href="/grocery_store/assets/js/151.3e654989.js"><link rel="prefetch" href="/grocery_store/assets/js/152.0ed89a9b.js"><link rel="prefetch" href="/grocery_store/assets/js/153.8e227ca1.js"><link rel="prefetch" href="/grocery_store/assets/js/154.a2831255.js"><link rel="prefetch" href="/grocery_store/assets/js/155.10ccf9ac.js"><link rel="prefetch" href="/grocery_store/assets/js/156.380d2724.js"><link rel="prefetch" href="/grocery_store/assets/js/157.c81733eb.js"><link rel="prefetch" href="/grocery_store/assets/js/158.cbfc3908.js"><link rel="prefetch" href="/grocery_store/assets/js/159.a6d3ae98.js"><link rel="prefetch" href="/grocery_store/assets/js/16.fd35c607.js"><link rel="prefetch" href="/grocery_store/assets/js/160.7255e3b5.js"><link rel="prefetch" href="/grocery_store/assets/js/161.904010dd.js"><link rel="prefetch" href="/grocery_store/assets/js/162.4dec98b6.js"><link rel="prefetch" href="/grocery_store/assets/js/163.37a5ff7f.js"><link rel="prefetch" href="/grocery_store/assets/js/164.88a4ed0a.js"><link rel="prefetch" href="/grocery_store/assets/js/165.e5de4c12.js"><link rel="prefetch" href="/grocery_store/assets/js/166.acb3ac67.js"><link rel="prefetch" href="/grocery_store/assets/js/167.922fe947.js"><link rel="prefetch" href="/grocery_store/assets/js/168.9b5decdc.js"><link rel="prefetch" href="/grocery_store/assets/js/169.774f7b4f.js"><link rel="prefetch" href="/grocery_store/assets/js/17.94989432.js"><link rel="prefetch" href="/grocery_store/assets/js/170.74cc7593.js"><link rel="prefetch" href="/grocery_store/assets/js/171.f22f02e1.js"><link rel="prefetch" href="/grocery_store/assets/js/172.795a7607.js"><link rel="prefetch" href="/grocery_store/assets/js/173.6a9852b2.js"><link rel="prefetch" href="/grocery_store/assets/js/174.24096d24.js"><link rel="prefetch" href="/grocery_store/assets/js/175.e1f80709.js"><link rel="prefetch" href="/grocery_store/assets/js/176.89e20247.js"><link rel="prefetch" href="/grocery_store/assets/js/177.90f78411.js"><link rel="prefetch" href="/grocery_store/assets/js/178.7cad688f.js"><link rel="prefetch" href="/grocery_store/assets/js/179.869c8720.js"><link rel="prefetch" href="/grocery_store/assets/js/18.dd69d970.js"><link rel="prefetch" href="/grocery_store/assets/js/180.c18e296f.js"><link rel="prefetch" href="/grocery_store/assets/js/19.e3521f6f.js"><link rel="prefetch" href="/grocery_store/assets/js/2.6f311729.js"><link rel="prefetch" href="/grocery_store/assets/js/20.cdd37c03.js"><link rel="prefetch" href="/grocery_store/assets/js/21.74481991.js"><link rel="prefetch" href="/grocery_store/assets/js/22.13aba632.js"><link rel="prefetch" href="/grocery_store/assets/js/23.faf985dd.js"><link rel="prefetch" href="/grocery_store/assets/js/24.7790407e.js"><link rel="prefetch" href="/grocery_store/assets/js/25.cc831358.js"><link rel="prefetch" href="/grocery_store/assets/js/26.5b918371.js"><link rel="prefetch" href="/grocery_store/assets/js/27.1fb985ed.js"><link rel="prefetch" href="/grocery_store/assets/js/28.4f194089.js"><link rel="prefetch" href="/grocery_store/assets/js/29.0e71c38a.js"><link rel="prefetch" href="/grocery_store/assets/js/30.9f05173d.js"><link rel="prefetch" href="/grocery_store/assets/js/31.5e341171.js"><link rel="prefetch" href="/grocery_store/assets/js/32.a550c1a0.js"><link rel="prefetch" href="/grocery_store/assets/js/33.8a828405.js"><link rel="prefetch" href="/grocery_store/assets/js/34.f7e62a92.js"><link rel="prefetch" href="/grocery_store/assets/js/35.711fbf92.js"><link rel="prefetch" href="/grocery_store/assets/js/36.b9fe8ec6.js"><link rel="prefetch" href="/grocery_store/assets/js/37.a924be53.js"><link rel="prefetch" href="/grocery_store/assets/js/38.e8f0d4cf.js"><link rel="prefetch" href="/grocery_store/assets/js/39.4cc4d0f7.js"><link rel="prefetch" href="/grocery_store/assets/js/4.2b8a6a17.js"><link rel="prefetch" href="/grocery_store/assets/js/40.39fd33ca.js"><link rel="prefetch" href="/grocery_store/assets/js/41.aa2d5c38.js"><link rel="prefetch" href="/grocery_store/assets/js/42.2e653320.js"><link rel="prefetch" href="/grocery_store/assets/js/43.60b064bc.js"><link rel="prefetch" href="/grocery_store/assets/js/44.b9efc4b4.js"><link rel="prefetch" href="/grocery_store/assets/js/46.58671855.js"><link rel="prefetch" href="/grocery_store/assets/js/47.e8dced2f.js"><link rel="prefetch" href="/grocery_store/assets/js/48.623a8708.js"><link rel="prefetch" href="/grocery_store/assets/js/49.3da43528.js"><link rel="prefetch" href="/grocery_store/assets/js/5.41bb00f7.js"><link rel="prefetch" href="/grocery_store/assets/js/50.e8b509c0.js"><link rel="prefetch" href="/grocery_store/assets/js/51.57dea248.js"><link rel="prefetch" href="/grocery_store/assets/js/52.2a653b34.js"><link rel="prefetch" href="/grocery_store/assets/js/53.7e9a38b7.js"><link rel="prefetch" href="/grocery_store/assets/js/54.a6d6a228.js"><link rel="prefetch" href="/grocery_store/assets/js/55.6589895a.js"><link rel="prefetch" href="/grocery_store/assets/js/56.f2fcde50.js"><link rel="prefetch" href="/grocery_store/assets/js/57.8fbc4361.js"><link rel="prefetch" href="/grocery_store/assets/js/58.e5afed15.js"><link rel="prefetch" href="/grocery_store/assets/js/59.2c8ca963.js"><link rel="prefetch" href="/grocery_store/assets/js/6.31d3db0c.js"><link rel="prefetch" href="/grocery_store/assets/js/60.79a70c57.js"><link rel="prefetch" href="/grocery_store/assets/js/61.9fef88bd.js"><link rel="prefetch" href="/grocery_store/assets/js/62.e375e524.js"><link rel="prefetch" href="/grocery_store/assets/js/63.5952f2d6.js"><link rel="prefetch" href="/grocery_store/assets/js/64.2695a163.js"><link rel="prefetch" href="/grocery_store/assets/js/65.96a95c08.js"><link rel="prefetch" href="/grocery_store/assets/js/66.82c501b5.js"><link rel="prefetch" href="/grocery_store/assets/js/67.c3dfe33d.js"><link rel="prefetch" href="/grocery_store/assets/js/68.8d61e8ca.js"><link rel="prefetch" href="/grocery_store/assets/js/69.ffb2c059.js"><link rel="prefetch" href="/grocery_store/assets/js/7.390fea20.js"><link rel="prefetch" href="/grocery_store/assets/js/70.487b0cd7.js"><link rel="prefetch" href="/grocery_store/assets/js/71.698c9666.js"><link rel="prefetch" href="/grocery_store/assets/js/72.baf6aad0.js"><link rel="prefetch" href="/grocery_store/assets/js/73.a28c801a.js"><link rel="prefetch" href="/grocery_store/assets/js/74.fb3ba6d3.js"><link rel="prefetch" href="/grocery_store/assets/js/75.acb4412a.js"><link rel="prefetch" href="/grocery_store/assets/js/76.3bd634b6.js"><link rel="prefetch" href="/grocery_store/assets/js/77.db356fda.js"><link rel="prefetch" href="/grocery_store/assets/js/78.c7912e34.js"><link rel="prefetch" href="/grocery_store/assets/js/79.2144dc16.js"><link rel="prefetch" href="/grocery_store/assets/js/8.aee987bd.js"><link rel="prefetch" href="/grocery_store/assets/js/80.d6fbf3d8.js"><link rel="prefetch" href="/grocery_store/assets/js/81.e21a47d6.js"><link rel="prefetch" href="/grocery_store/assets/js/82.7f4636b3.js"><link rel="prefetch" href="/grocery_store/assets/js/83.440942f4.js"><link rel="prefetch" href="/grocery_store/assets/js/84.f0d9a8fe.js"><link rel="prefetch" href="/grocery_store/assets/js/85.39c6b930.js"><link rel="prefetch" href="/grocery_store/assets/js/86.75218340.js"><link rel="prefetch" href="/grocery_store/assets/js/87.b103ab9e.js"><link rel="prefetch" href="/grocery_store/assets/js/88.ac661b1d.js"><link rel="prefetch" href="/grocery_store/assets/js/89.92e470b3.js"><link rel="prefetch" href="/grocery_store/assets/js/9.ad4b2757.js"><link rel="prefetch" href="/grocery_store/assets/js/90.b2e04d90.js"><link rel="prefetch" href="/grocery_store/assets/js/91.8a00aaec.js"><link rel="prefetch" href="/grocery_store/assets/js/92.f7d023f2.js"><link rel="prefetch" href="/grocery_store/assets/js/93.9128c36a.js"><link rel="prefetch" href="/grocery_store/assets/js/94.6280de17.js"><link rel="prefetch" href="/grocery_store/assets/js/95.ea00692f.js"><link rel="prefetch" href="/grocery_store/assets/js/96.a140f30e.js"><link rel="prefetch" href="/grocery_store/assets/js/97.ea5254e3.js"><link rel="prefetch" href="/grocery_store/assets/js/98.9e168926.js"><link rel="prefetch" href="/grocery_store/assets/js/99.c5f861f8.js">
    <link rel="stylesheet" href="/grocery_store/assets/css/0.styles.892fb305.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/grocery_store/" class="home-link router-link-active"><!----> <span class="site-name">Lancezhange Vuepress Book</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/grocery_store/" class="nav-link">
  Home
</a></div><div class="nav-item"><a href="/grocery_store/SUMMARY/" class="nav-link">
  Contents
</a></div><div class="nav-item"><a href="http://www.lancezhange.com/" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Blog
  <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/grocery_store/" class="nav-link">
  Home
</a></div><div class="nav-item"><a href="/grocery_store/SUMMARY/" class="nav-link">
  Contents
</a></div><div class="nav-item"><a href="http://www.lancezhange.com/" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Blog
  <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></div> <!----></nav>  <ul class="sidebar-links"><li><a href="/grocery_store/section6/" class="sidebar-link">机器学习</a></li><li><a href="/grocery_store/section6/ads.html" class="sidebar-link">广告系统技术 {ignore=true}</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/grocery_store/section6/ai-system.html" class="sidebar-link">AI 系统 {ignore=true}</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/grocery_store/section6/ai-system.html#系统设计" class="sidebar-link">系统设计</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/ai-system.html#online-deep-learning" class="sidebar-link">Online Deep Learning</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/ai-system.html#模型工程" class="sidebar-link">模型工程</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/ai-system.html#ai-芯片" class="sidebar-link">Ai 芯片</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/ai-system.html#参考" class="sidebar-link">参考</a></li></ul></li><li><a href="/grocery_store/section6/ai.html" class="sidebar-link">人工智能专题</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/grocery_store/section6/ai.html#应用" class="sidebar-link">应用</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/ai.html#资料" class="sidebar-link">资料</a></li></ul></li><li><a href="/grocery_store/section6/ai_with_symbolism.html" class="sidebar-link">符号化 AI {ignore=true}</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/grocery_store/section6/ai_with_symbolism.html#参考" class="sidebar-link">参考</a></li></ul></li><li><a href="/grocery_store/section6/anomaly_detection.html" class="sidebar-link">异常检测</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/grocery_store/section6/anomaly_detection.html#深度学习方法" class="sidebar-link">深度学习方法</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/anomaly_detection.html#资料" class="sidebar-link">资料</a></li></ul></li><li><a href="/grocery_store/section6/attension.html" class="sidebar-link">Attension 专题 {ignore=true}</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/grocery_store/section6/attension.html#attension" class="sidebar-link">Attension</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/attension.html#adversarial-attention" class="sidebar-link">Adversarial Attention</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/attension.html#transformer" class="sidebar-link">Transformer</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/attension.html#工具和应用" class="sidebar-link">工具和应用</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/attension.html#参考" class="sidebar-link">参考</a></li></ul></li><li><a href="/grocery_store/section6/backendArchitecture.html" class="sidebar-link">后端架构专题</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/grocery_store/section6/backendArchitecture.html#参考资料" class="sidebar-link">参考资料</a></li></ul></li><li><a href="/grocery_store/section6/bayesian.html" class="sidebar-link">贝叶斯统计</a></li><li><a href="/grocery_store/section6/bigdata.html" class="sidebar-link">大数据专题</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/grocery_store/section6/bigdata.html#常见题目" class="sidebar-link">常见题目</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/bigdata.html#参考" class="sidebar-link">参考</a></li></ul></li><li><a href="/grocery_store/section6/capsnet.html" class="sidebar-link">CapsNet</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/grocery_store/section6/capsnet.html#原理" class="sidebar-link">原理</a></li></ul></li><li><a href="/grocery_store/section6/cluster.html" class="sidebar-link">聚类算法专题</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/grocery_store/section6/cluster.html#常用算法" class="sidebar-link">常用算法</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/cluster.html#参考" class="sidebar-link">参考</a></li></ul></li><li><a href="/grocery_store/section6/cnn.html" class="sidebar-link">CNN {ignore=true}</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/grocery_store/section6/cnn.html#基本原理" class="sidebar-link">基本原理</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/cnn.html#经典结构" class="sidebar-link">经典结构</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/cnn.html#octconv" class="sidebar-link">OctConv</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/cnn.html#dorefa-net" class="sidebar-link">Dorefa-Net</a></li></ul></li><li><a href="/grocery_store/section6/competition.html" class="sidebar-link">比赛</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/grocery_store/section6/competition.html#bagging" class="sidebar-link">bagging</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/competition.html#satcking" class="sidebar-link">satcking</a></li></ul></li><li><a href="/grocery_store/section6/computerVision.html" class="sidebar-link">传统计算机视觉专题</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/grocery_store/section6/crf.html" class="sidebar-link">CRF</a></li><li><a href="/grocery_store/section6/cryptocurrency.html" class="sidebar-link">加密货币 {ignore=true}</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/grocery_store/section6/cryptocurrency.html#数字现金的梦碎" class="sidebar-link">数字现金的梦碎</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/cryptocurrency.html#比特币" class="sidebar-link">比特币</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/cryptocurrency.html#共识算法" class="sidebar-link">共识算法</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/cryptocurrency.html#以太坊-ethereum-eth" class="sidebar-link">以太坊</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/cryptocurrency.html#libra" class="sidebar-link">Libra</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/cryptocurrency.html#交易" class="sidebar-link">交易</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/cryptocurrency.html#参考" class="sidebar-link">参考</a></li></ul></li><li><a href="/grocery_store/section6/ctr.html" class="sidebar-link">CTR {ignore=true}</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/grocery_store/section6/ctr.html#传统方法-前深度学习时代" class="sidebar-link">传统方法 - 前深度学习时代</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/ctr.html#深度学习-ctr" class="sidebar-link">深度学习 CTR</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/ctr.html#参考" class="sidebar-link">参考</a></li></ul></li><li><a href="/grocery_store/section6/cuttingEdge.html" class="sidebar-link">前沿</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/grocery_store/section6/dataAnalysis.html" class="sidebar-link">数据分析专题</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/grocery_store/section6/dataAnalysis.html#参考" class="sidebar-link">参考</a></li></ul></li><li><a href="/grocery_store/section6/dataScrapy.html" class="sidebar-link">数据爬取</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/grocery_store/section6/deepgbm.html" class="sidebar-link">DeepGBM</a></li><li><a href="/grocery_store/section6/deeplearning.html" class="sidebar-link">深度学习专题 {ignore=true}</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/grocery_store/section6/deeplearning.html#理论基础" class="sidebar-link">理论基础</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/deeplearning.html#cnn-专题" class="sidebar-link">CNN 专题</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/deeplearning.html#rnn-专题" class="sidebar-link">RNN 专题</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/deeplearning.html#seq2seq-专题" class="sidebar-link">Seq2Seq 专题</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/deeplearning.html#gnn-专题" class="sidebar-link">GNN 专题</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/deeplearning.html#attension-专题" class="sidebar-link">Attension 专题</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/deeplearning.html#模型压缩和精简" class="sidebar-link">模型压缩和精简</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/deeplearning.html#待分类" class="sidebar-link">待分类</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/deeplearning.html#贝叶斯深度学习" class="sidebar-link">贝叶斯深度学习</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/deeplearning.html#深度学习的应用" class="sidebar-link">深度学习的应用</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/deeplearning.html#工具" class="sidebar-link">工具</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/deeplearning.html#参考" class="sidebar-link">参考</a></li></ul></li><li><a href="/grocery_store/section6/deeplearning_tricks.html" class="sidebar-link">Deep Learning Tricks</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/grocery_store/section6/deeplearning_tricks.html#deep-learning-tricks" class="sidebar-link">Deep Learning Tricks</a></li></ul></li><li><a href="/grocery_store/section6/em.html" class="sidebar-link">EM 算法</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/grocery_store/section6/em.html#原理" class="sidebar-link">原理</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/em.html#em-算法的应用" class="sidebar-link">EM 算法的应用</a></li></ul></li><li><a href="/grocery_store/section6/feature_enginerring.html" class="sidebar-link">特征工程</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/grocery_store/section6/few_shot_small_data_learning.html" class="sidebar-link">小样本学习</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/grocery_store/section6/few_shot_small_data_learning.html#少样本学习（few-shot-learning）" class="sidebar-link">少样本学习（Few-Shot Learning）</a></li></ul></li><li><a href="/grocery_store/section6/fm.html" class="sidebar-link">FM {ignore=true}</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/grocery_store/section6/fm.html#fm-及其延伸方法" class="sidebar-link">FM 及其延伸方法</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/fm.html#代码实现" class="sidebar-link">代码实现</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/fm.html#参考" class="sidebar-link">参考</a></li></ul></li><li><a href="/grocery_store/section6/frontend.html" class="sidebar-link">前端技术专题</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/grocery_store/section6/gan.html" class="sidebar-link">生成对抗网络</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/grocery_store/section6/gan.html#_2-1-原生-gan" class="sidebar-link">2.1 原生 GAN</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/gan.html#_2-2-cgan：-随心所欲" class="sidebar-link">2.2 CGAN： 随心所欲</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/gan.html#_2-3-wgan" class="sidebar-link">2.3 WGAN</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/gan.html#_2-4-improved-wgans-wgan-gp" class="sidebar-link">2.4 Improved WGANs (WGAN-GP)</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/gan.html#_2-5-ls-gan" class="sidebar-link">2.5 LS-GAN</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/gan.html#_2-6-其他变种" class="sidebar-link">2.6 其他变种</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/gan.html#_4-0-工具" class="sidebar-link">4.0 工具</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/gan.html#_4-1-数据生成" class="sidebar-link">4.1 数据生成</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/gan.html#_4-2-域转换-图片翻译" class="sidebar-link">4.2 域转换-图片翻译</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/gan.html#_4-4-图片超分辨率重建" class="sidebar-link">4.4 图片超分辨率重建</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/gan.html#nlp-领域" class="sidebar-link">NLP 领域</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/gan.html#信息检索-搜索推荐领域" class="sidebar-link">信息检索/搜索推荐领域</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/gan.html#_4-5-和深度强化学习的结合" class="sidebar-link">4.5 和深度强化学习的结合</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/gan.html#_4-4-依葫芦画瓢：模仿学习" class="sidebar-link">4.4 依葫芦画瓢：模仿学习</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/gan.html#ai-隐身术" class="sidebar-link">AI 隐身术</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/gan.html#黑盒攻击" class="sidebar-link">黑盒攻击</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/gan.html#可迁移攻击" class="sidebar-link">可迁移攻击</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/gan.html#textual-adversarial-attack-and-defense" class="sidebar-link">Textual Adversarial Attack and Defense</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/gan.html#生成方法" class="sidebar-link">生成方法</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/gan.html#比赛" class="sidebar-link">比赛</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/gan.html#_6-1-深度学习知识点" class="sidebar-link">6.1 深度学习知识点</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/gan.html#_6-2-关于生成模型" class="sidebar-link">6.2 关于生成模型</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/gan.html#_6-3-一些必要的数学基础知识回顾" class="sidebar-link">6.3 一些必要的数学基础知识回顾</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/gan.html#_6-4-一些数据集" class="sidebar-link">6.4 一些数据集</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/gan.html#_6-5-tensorflow-知识点" class="sidebar-link">6.5 tensorflow 知识点</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/gan.html#参考资料" class="sidebar-link">参考资料</a></li></ul></li><li><a href="/grocery_store/section6/gan2.html" class="sidebar-link">GAN (part2) {ignore=true}</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/grocery_store/section6/gan2.html#最新进展" class="sidebar-link">最新进展</a></li></ul></li><li><a href="/grocery_store/section6/gbdt_xgboost.html" class="sidebar-link">Tree Boosting {ignore}</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/grocery_store/section6/gbdt_xgboost.html#gbdt" class="sidebar-link">GBDT</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/gbdt_xgboost.html#xgboost" class="sidebar-link">XGBoost</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/gbdt_xgboost.html#lightgbm" class="sidebar-link">LightGBM</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/gbdt_xgboost.html#参考" class="sidebar-link">参考</a></li></ul></li><li><a href="/grocery_store/section6/gnn.html" class="sidebar-link">Graph Neural Networks {ignore=true}</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/grocery_store/section6/gnn.html#什么是图神经网络" class="sidebar-link">什么是图神经网络</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/gnn.html#原始-gnn" class="sidebar-link">原始 GNN</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/gnn.html#图卷积网络-gcn" class="sidebar-link">图卷积网络 GCN</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/gnn.html#图注意力网络（graph-attention-networks-gat）" class="sidebar-link">图注意力网络（Graph Attention Networks, GAT）</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/gnn.html#图自编码器（-graph-autoencoders）" class="sidebar-link">图自编码器（ Graph Autoencoders）</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/gnn.html#图生成网络（-graph-generative-networks）" class="sidebar-link">图生成网络（ Graph Generative Networks）</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/gnn.html#图时空网络（graph-spatial-temporal-networks）" class="sidebar-link">图时空网络（Graph Spatial-temporal Networks）</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/gnn.html#图神经网络的应用" class="sidebar-link">图神经网络的应用</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/gnn.html#参考文献" class="sidebar-link">参考文献</a></li></ul></li><li><a href="/grocery_store/section6/gradient_free.html" class="sidebar-link">无梯度优化 (Gradient-Free)</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/grocery_store/section6/gradient_free.html#进化策略-es" class="sidebar-link">进化策略 ES</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/gradient_free.html#遗传算法-ge" class="sidebar-link">遗传算法 GE</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/gradient_free.html#粒子群算法-pso" class="sidebar-link">粒子群算法 PSO</a></li></ul></li><li><a href="/grocery_store/section6/hmm.html" class="sidebar-link">隐马尔可夫 {ignore=true}</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/grocery_store/section6/hmm.html#基本原理" class="sidebar-link">基本原理</a></li></ul></li><li><a href="/grocery_store/section6/industry.html" class="sidebar-link">产品工业实践 {ignore=true}</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/grocery_store/section6/industry.html#模型的快速线上评估方法" class="sidebar-link">模型的快速线上评估方法</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/industry.html#产品分析" class="sidebar-link">产品分析</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/industry.html#思考视野" class="sidebar-link">思考视野</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/industry.html#参考" class="sidebar-link">参考</a></li></ul></li><li><a href="/grocery_store/section6/interview-questions.html" class="sidebar-link">问题 {ignore=true}</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/grocery_store/section6/interview-questions.html#数学基础" class="sidebar-link">数学基础</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/interview-questions.html#机器学习" class="sidebar-link">机器学习</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/interview-questions.html#深度学习" class="sidebar-link">深度学习</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/interview-questions.html#数据结构与算法" class="sidebar-link">数据结构与算法</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/interview-questions.html#编程" class="sidebar-link">编程</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/interview-questions.html#大数据" class="sidebar-link">大数据</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/interview-questions.html#工业" class="sidebar-link">工业</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/interview-questions.html#务虚" class="sidebar-link">务虚</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/interview-questions.html#参考资料" class="sidebar-link">参考资料</a></li></ul></li><li><a href="/grocery_store/section6/knn.html" class="sidebar-link">knn</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/grocery_store/section6/knn.html#决策规则" class="sidebar-link">决策规则</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/knn.html#求解" class="sidebar-link">求解</a></li></ul></li><li><a href="/grocery_store/section6/knowledge_graph.html" class="sidebar-link">知识图谱(knowledge graph)</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/grocery_store/section6/knowledge_graph.html#参考" class="sidebar-link">参考</a></li></ul></li><li><a href="/grocery_store/section6/lda.html" class="sidebar-link">LDA 主题模型</a></li><li><a href="/grocery_store/section6/linearmodel.html" class="sidebar-link">线性模型 {ignore=True}</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/grocery_store/section6/linearmodel.html#线性回归" class="sidebar-link">线性回归</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/linearmodel.html#广义线性模型" class="sidebar-link">广义线性模型</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/linearmodel.html#逻辑回归" class="sidebar-link">逻辑回归</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/linearmodel.html#广义线性混合模型" class="sidebar-link">广义线性混合模型</a></li></ul></li><li><a href="/grocery_store/section6/lstm.html" class="sidebar-link">LSTM {ignore=ture}</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/grocery_store/section6/lstm.html#lstm-原理" class="sidebar-link">LSTM 原理</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/lstm.html#双向-lstm" class="sidebar-link">双向 LSTM</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/lstm.html#参考资料" class="sidebar-link">参考资料</a></li></ul></li><li><a href="/grocery_store/section6/ltr.html" class="sidebar-link">Learning to Rank (LTR) {ignore=true}</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/grocery_store/section6/ltr.html#基本原理" class="sidebar-link">基本原理</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/ltr.html#效果评价" class="sidebar-link">效果评价</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/ltr.html#参考" class="sidebar-link">参考</a></li></ul></li><li><a href="/grocery_store/section6/mab.html" class="sidebar-link">多臂老虎机</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/grocery_store/section6/mab.html#contextual-bandit" class="sidebar-link">Contextual Bandit</a></li></ul></li><li><a href="/grocery_store/section6/mcmc.html" class="sidebar-link">蒙特卡洛方法与 MCMC 采样</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/grocery_store/section6/mcmc.html#蒙特卡洛模拟" class="sidebar-link">蒙特卡洛模拟</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/mcmc.html#采样方法" class="sidebar-link">采样方法</a></li></ul></li><li><a href="/grocery_store/section6/mf.html" class="sidebar-link">矩阵分解 {ignore=true}</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/grocery_store/section6/mf.html#显式矩阵分解" class="sidebar-link">显式矩阵分解</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/mf.html#隐式矩阵分解" class="sidebar-link">隐式矩阵分解</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/mf.html#增量矩阵分解" class="sidebar-link">增量矩阵分解</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/mf.html#矩阵分解方法的可解释性" class="sidebar-link">矩阵分解方法的可解释性</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/mf.html#svd" class="sidebar-link">SVD</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/mf.html#svd-2" class="sidebar-link">SVD ++</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/mf.html#矩阵分解应用" class="sidebar-link">矩阵分解应用</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/mf.html#参考" class="sidebar-link">参考</a></li></ul></li><li><a href="/grocery_store/section6/ml_engineering.html" class="sidebar-link">机器学习工业实践</a></li><li><a href="/grocery_store/section6/mlconcepts.html" class="sidebar-link">机器学习常见概念 {ignore=true}</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/grocery_store/section6/mlconcepts.html#基础概念" class="sidebar-link">基础概念</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/mlconcepts.html#参考" class="sidebar-link">参考</a></li></ul></li><li><a href="/grocery_store/section6/mobile.html" class="sidebar-link">移动互联网专题</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/grocery_store/section6/model_compress_simplify.html" class="sidebar-link">模型压缩精简 {ignore=true}</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/grocery_store/section6/model_compress_simplify.html#压缩精简方法" class="sidebar-link">压缩精简方法</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/model_compress_simplify.html#参考" class="sidebar-link">参考</a></li></ul></li><li><a href="/grocery_store/section6/multimodel.html" class="sidebar-link">多模型多任务专题 {ignore=ture}</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/grocery_store/section6/multimodel.html#多模态" class="sidebar-link">多模态</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/multimodel.html#多任务" class="sidebar-link">多任务</a></li></ul></li><li><a href="/grocery_store/section6/nlp.html" class="active sidebar-link">自然语言处理专题 {ignore=true}</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/grocery_store/section6/nlp.html#传统自然语言处理" class="sidebar-link">传统自然语言处理</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/nlp.html#深度学习文本模型" class="sidebar-link">深度学习文本模型</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/nlp.html#神经网络语言模型（nnlm）" class="sidebar-link">神经网络语言模型（NNLM）</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/nlp.html#学习资料" class="sidebar-link">学习资料</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/nlp.html#附录" class="sidebar-link">附录</a></li></ul></li><li><a href="/grocery_store/section6/nlp_reading_comprehension.html" class="sidebar-link">阅读理解</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/grocery_store/section6/nlp_reading_comprehension.html#工具" class="sidebar-link">工具</a></li></ul></li><li><a href="/grocery_store/section6/nlp_word2vec_embedding.html" class="sidebar-link">Embeding 专题 {ignore=true}</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/grocery_store/section6/nlp_word2vec_embedding.html#lsa" class="sidebar-link">LSA</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/nlp_word2vec_embedding.html#word2vec" class="sidebar-link">Word2vec</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/nlp_word2vec_embedding.html#doc2vec" class="sidebar-link">Doc2Vec</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/nlp_word2vec_embedding.html#item2vec" class="sidebar-link">Item2Vec</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/nlp_word2vec_embedding.html#离散特征嵌入" class="sidebar-link">离散特征嵌入</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/nlp_word2vec_embedding.html#graph-embedding" class="sidebar-link">Graph Embedding</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/nlp_word2vec_embedding.html#动态词向量" class="sidebar-link">动态词向量</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/nlp_word2vec_embedding.html#其他" class="sidebar-link">其他</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/nlp_word2vec_embedding.html#参考" class="sidebar-link">参考</a></li></ul></li><li><a href="/grocery_store/section6/onlineLearning.html" class="sidebar-link">在线学习</a></li><li><a href="/grocery_store/section6/paper2018.html" class="sidebar-link">2018年阅读的论文</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/grocery_store/section6/paper2019.html" class="sidebar-link">2019 论文精读 {ignore=true}</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/grocery_store/section6/paper2020.html" class="sidebar-link">2020 论文精读 {ignore=true}</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/grocery_store/section6/pca.html" class="sidebar-link">降维 (dimensionality reduction) {ignore=ture}</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/grocery_store/section6/pca.html#pca" class="sidebar-link">pca</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/pca.html#t-sne" class="sidebar-link">t-SNE</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/pca.html#ivis" class="sidebar-link">ivis</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/pca.html#keplermapper" class="sidebar-link">KeplerMapper</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/pca.html#umap" class="sidebar-link">UMAP</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/pca.html#附录-降维可视化" class="sidebar-link">附录-降维可视化</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/pca.html#参考" class="sidebar-link">参考</a></li></ul></li><li><a href="/grocery_store/section6/pgm.html" class="sidebar-link">概率图模型 {ignore=true}</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/grocery_store/section6/pgm.html#hmm" class="sidebar-link">HMM</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/pgm.html#马尔可夫随机场-mrf" class="sidebar-link">马尔可夫随机场 MRF</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/pgm.html#条件随机场" class="sidebar-link">条件随机场</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/pgm.html#参考" class="sidebar-link">参考</a></li></ul></li><li><a href="/grocery_store/section6/qa.html" class="sidebar-link">问答系统专题</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/grocery_store/section6/qa.html#开放域问答" class="sidebar-link">开放域问答</a></li></ul></li><li><a href="/grocery_store/section6/rank.html" class="sidebar-link">排序专题</a></li><li><a href="/grocery_store/section6/rbm.html" class="sidebar-link">RBM</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/grocery_store/section6/rbm.html#dbn" class="sidebar-link">DBN</a></li></ul></li><li><a href="/grocery_store/section6/recommender.html" class="sidebar-link">推荐系统专题 {ignore=true}</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/grocery_store/section6/recommender.html#传统方法" class="sidebar-link">传统方法</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/recommender.html#深度学习在推荐领域的应用" class="sidebar-link">深度学习在推荐领域的应用</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/recommender.html#开辟新赛道：超越单体模型" class="sidebar-link">开辟新赛道：超越单体模型</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/recommender.html#开辟新赛道：结合强化学习" class="sidebar-link">开辟新赛道：结合强化学习</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/recommender.html#开辟新赛道：其他思路" class="sidebar-link">开辟新赛道：其他思路</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/recommender.html#可解释性" class="sidebar-link">可解释性</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/recommender.html#应用" class="sidebar-link">应用</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/recommender.html#附录-关于样本" class="sidebar-link">附录-关于样本</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/recommender.html#附录-paper-阅读札记" class="sidebar-link">附录-paper 阅读札记</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/recommender.html#参考资料" class="sidebar-link">参考资料</a></li></ul></li><li><a href="/grocery_store/section6/recommender_practical_work.html" class="sidebar-link">推荐系统实战</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/grocery_store/section6/recommender_practical_work.html#" class="sidebar-link"></a></li></ul></li><li><a href="/grocery_store/section6/rl.html" class="sidebar-link">强化学习(reinforcement learning) {ignore=true}</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/grocery_store/section6/rl.html#基本方法演进" class="sidebar-link">基本方法演进</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/rl.html#分布式强化学习" class="sidebar-link">分布式强化学习</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/rl.html#参考" class="sidebar-link">参考</a></li></ul></li><li><a href="/grocery_store/section6/rnn.html" class="sidebar-link">RNN {ignore-true}</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/grocery_store/section6/rnn.html#rnn-原理" class="sidebar-link">RNN 原理</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/rnn.html#lstm" class="sidebar-link">LSTM</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/rnn.html#工具和应用" class="sidebar-link">工具和应用</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/rnn.html#参考" class="sidebar-link">参考</a></li></ul></li><li><a href="/grocery_store/section6/search_and_rank.html" class="sidebar-link">搜索排序 {ignore=true}</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/grocery_store/section6/search_and_rank.html#传统方法" class="sidebar-link">传统方法</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/search_and_rank.html#机器学习排序方法" class="sidebar-link">机器学习排序方法</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/search_and_rank.html#深度学习方法" class="sidebar-link">深度学习方法</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/search_and_rank.html#评价方法" class="sidebar-link">评价方法</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/search_and_rank.html#数据获取" class="sidebar-link">数据获取</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/search_and_rank.html#召回" class="sidebar-link">召回</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/search_and_rank.html#参考" class="sidebar-link">参考</a></li></ul></li><li><a href="/grocery_store/section6/seq2seq.html" class="sidebar-link">Seq2seq</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/grocery_store/section6/seq2seq.html#经典网络" class="sidebar-link">经典网络</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/seq2seq.html#参考" class="sidebar-link">参考</a></li></ul></li><li><a href="/grocery_store/section6/socialnetworks.html" class="sidebar-link">社交网络专题</a></li><li><a href="/grocery_store/section6/spatialDataMining.html" class="sidebar-link">空间数据挖掘专题</a><ul class="sidebar-sub-headers"></ul></li><li><a href="/grocery_store/section6/speech.html" class="sidebar-link">语音识别专题 {ignore=True}</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/grocery_store/section6/speech.html#基础知识" class="sidebar-link">基础知识</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/speech.html#模型" class="sidebar-link">模型</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/speech.html#工具" class="sidebar-link">工具</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/speech.html#demo" class="sidebar-link">Demo</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/speech.html#参考资料" class="sidebar-link">参考资料</a></li></ul></li><li><a href="/grocery_store/section6/svm.html" class="sidebar-link">SVM {ignore=True}</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/grocery_store/section6/svm.html#线性可分-svm" class="sidebar-link">线性可分 SVM</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/svm.html#非线性可分-核技巧" class="sidebar-link">非线性可分-核技巧</a></li></ul></li><li><a href="/grocery_store/section6/timeseries.html" class="sidebar-link">时间序列分析专题 {ignore=true}</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/grocery_store/section6/timeseries.html#平稳性" class="sidebar-link">平稳性</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/timeseries.html#传统方法" class="sidebar-link">传统方法</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/timeseries.html#神经网络方法" class="sidebar-link">神经网络方法</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/timeseries.html#模型评估和选择" class="sidebar-link">模型评估和选择</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/timeseries.html#高阶话题" class="sidebar-link">高阶话题</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/timeseries.html#工具" class="sidebar-link">工具</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/timeseries.html#应用" class="sidebar-link">应用</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/timeseries.html#参考" class="sidebar-link">参考</a></li></ul></li><li><a href="/grocery_store/section6/tricks.html" class="sidebar-link">Tricks {ignore=True}</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/grocery_store/section6/tricks.html#常用-tricks" class="sidebar-link">常用 Tricks</a></li></ul></li><li><a href="/grocery_store/section6/vae.html" class="sidebar-link">VAE</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/grocery_store/section6/vae.html#vae" class="sidebar-link">VAE</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/vae.html#神经变分推断" class="sidebar-link">神经变分推断</a></li></ul></li><li><a href="/grocery_store/section6/vi-vae.html" class="sidebar-link">Variational Inference 变分推断方法</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/grocery_store/section6/vi-vae.html#背景" class="sidebar-link">背景</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/vi-vae.html#基本框架" class="sidebar-link">基本框架</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/vi-vae.html#平均场理论" class="sidebar-link">平均场理论</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/vi-vae.html#参考" class="sidebar-link">参考</a></li></ul></li><li><a href="/grocery_store/section6/vi.html" class="sidebar-link">Variational Inference 变分推断 {ignore=true}</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/grocery_store/section6/vi.html#背景" class="sidebar-link">背景</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/vi.html#基本框架" class="sidebar-link">基本框架</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/vi.html#平均场理论" class="sidebar-link">平均场理论</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/vi.html#参考" class="sidebar-link">参考</a></li></ul></li><li><a href="/grocery_store/section6/visualization.html" class="sidebar-link">可视化专题</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/grocery_store/section6/visualization.html#story-maps" class="sidebar-link">Story Maps</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/visualization.html#palettable" class="sidebar-link">palettable</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/visualization.html#web-based-interactive-storytelling" class="sidebar-link">Web-Based Interactive Storytelling</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/visualization.html#数据大屏" class="sidebar-link">数据大屏</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/visualization.html#机构／人物" class="sidebar-link">机构／人物</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/visualization.html#工具-2" class="sidebar-link">工具</a></li></ul></li><li><a href="/grocery_store/section6/wide_learning.html" class="sidebar-link">宽度学习 {ignore=true}</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/grocery_store/section6/wide_learning.html#基础理论" class="sidebar-link">基础理论</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/wide_learning.html#工具" class="sidebar-link">工具</a></li><li class="sidebar-sub-header"><a href="/grocery_store/section6/wide_learning.html#应用" class="sidebar-link">应用</a></li></ul></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="自然语言处理专题-ignore-true"><a href="#自然语言处理专题-ignore-true" class="header-anchor">#</a> 自然语言处理专题 {ignore=true}</h1> <p>[TOC]</p> <div align="center"><figure align="center"><img src="img-gan/2019-05-09-05-01.jpg" style="width:800px;"> <figcaption>图片来自微博@唐杰THU</figcaption></figure></div> <h2 id="传统自然语言处理"><a href="#传统自然语言处理" class="header-anchor">#</a> 传统自然语言处理</h2> <p>传统的自然语言处理方法是从语法分析，句法分析到语义分析的完整链条，每个步骤都需要人工设计（而深度学习则是‘端到端’的，这个我们后面会讲到）。</p> <p>正如计算机视觉领域所经受的那样，NLP 也经历了传统方法的兴盛（所谓的理性时代），后来 Deep Learning 成功入侵了语音和视觉领域，并最终在自然语言处理方面也展现了它强大的存在，使得传统方法逐渐走向没落（当然，或许有朝一日，理性主义会卷土重来）。</p> <p>参考　<a href="http://blog.sina.com.cn/s/blog_729574a00102wf63.html" target="_blank" rel="noopener noreferrer">穿越乔家大院寻找“毛毛虫”<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> by 白硕</p> <h3 id="n-元模型"><a href="#n-元模型" class="header-anchor">#</a> n 元模型</h3> <p>n 元模型的缺陷
无法建模更远的关系，语料的不足使得无法训练更高阶的语言模型。
无法建模出词之间的相似度。
训练语料里面有些 n 元组没有出现过,其对应的条件概率就是 0,导致计算一整句话的概率为 0。解决这个问题有两种常用方法： 平滑法和回退法。</p> <h5 id="语法分析"><a href="#语法分析" class="header-anchor">#</a> 语法分析</h5> <p>首先将句子分词，并做词性标注、去除停用词等工作。英文单词孤立，不涉及分词，而中文就需要分词了。常用的分词方法，有基于语法规则的，有基于词典的（如最大正向/反向匹配），也有基于统计的（比如 HMM, CRF）。</p> <p>将词语作为单元，可以研究词语之间的关系，包括　 synset 同义词，hypernymy 上位词，例如 person 是 woman 的上位词.</p> <p>词形变化关系，例如，</p> <blockquote><p>老虎是一种凶猛的动物，狮子是一种凶猛的动物</p></blockquote> <p>则(老虎，狮子)就是词形变化关系，体现在相似的上下文中</p> <p>syntagmatic relation
组合关系，比如，在前面的例子中，(老虎，凶猛)就是组合关系，它体现在</p> <p>分词</p> <p>词性标注</p> <h5 id="句法分析-systactic-parsing"><a href="#句法分析-systactic-parsing" class="header-anchor">#</a> 句法分析(systactic parsing)</h5> <h5 id="远程监督"><a href="#远程监督" class="header-anchor">#</a> 远程监督</h5> <p>远监督</p> <p>Distant Supervision</p> <h5 id="语义分析"><a href="#语义分析" class="header-anchor">#</a> 语义分析</h5> <ul><li>语义依存分析</li></ul> <p>在句子结构中分析实词和实词之间的语义关系，这种关系是一种事实上或逻辑上的关系，且只有当词语进入到句子时才会存在。语义依存分析的目的即回答句子的”Who did what to whom when and where”的问题。
<img src="http://mmbiz.qpic.cn/mmbiz/58FUuNaBUjrhHeeViaJ7BibPxDSDlRcI4ZqlIxhFKU7m81PcdVM2LsExgCRzbgTtEhsEJVttdWyc3hALp1ctU6kg/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1%22%3E" alt=""></p> <p>语言被认为具有树形结构</p> <h3 id="序列模型-vs-树结构"><a href="#序列模型-vs-树结构" class="header-anchor">#</a> 序列模型 VS 树结构</h3> <blockquote><p>在以下两种情况下，我们无需使用树结构模型</p></blockquote> <blockquote><p>1、我们面对的是简单问题，其较少考虑长距离的语义依赖关系；</p> <p>2、即使面对的是复杂问题，只要我们能够获得足够的训练数据；</p> <p>by 车万翔</p></blockquote> <ul><li>Recognise Textual Entailment 文本蕴含关系识别</li></ul> <p>所谓文本蕴含关系，就是蕴含关系在文本中的体现，比如‘我因为喜欢读书所以去上了大学’就蕴含‘’我喜欢读书，也蕴含‘我上过大学’，但并不蕴含‘我上过火星’。</p> <ul><li>word sense disambiguation 语义消歧</li></ul> <p>目标：确定一个多义词在特定语境中的使用的是哪一个语义。</p> <p>在词性标注那里，通常用的是邻近的结构信息，比如副词后面紧跟着动词，但语义消歧这里往往依赖的是长距离信息，比如，<em>苹果是一款非常受人们欢迎的手机</em>，<em>苹果</em>一词的语义其实跟最右的<em>手机</em>一词相关(比较：<em>苹果是一种非常受人们欢迎的水果</em>)，因此，语义消歧要使用 <strong><em>更为广泛</em></strong> 的上下文信息。</p> <p>常用的语义消歧方法包括基于标注训练集的有监督消歧、</p> <ul><li><p><a href="http://arxiv.org/abs/1511.06388" target="_blank" rel="noopener noreferrer">sense2vec - A Fast and Accurate Method for Word Sense Disambiguation In Neural Word Embeddings by Andrew Trask, et al., ICLR 2016, underreview<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <p>词嵌入的时候，多义词不应该只有一种向量表示。</p></li></ul> <p>@import &quot;nlp_word2vec.md&quot;</p> <h3 id="主题模型"><a href="#主题模型" class="header-anchor">#</a> 主题模型</h3> <p>最为常用的算法可能要属于 LDA 了。
LDA 的前身为概率潜在语义分析(pLSA)</p> <p><a href="http://arxiv.org/abs/1510.08628v1" target="_blank" rel="noopener noreferrer">WarpLDA: a Simple and Efficient O(1) Algorithm for Latent Dirichlet Allocation by Jianfei Chen, et al., 2015<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <h3 id="文本摘要专题-text-summarization"><a href="#文本摘要专题-text-summarization" class="header-anchor">#</a> 文本摘要专题(Text Summarization)</h3> <p>中等长度的摘要其实要比短摘要和长摘要都要难，因为，短摘要，例如一句话总结，容易遗漏要点，而长摘要涉及语句之间的逻辑关系（就好比写一篇小作文，语文不好的童鞋可头疼了），中等长度的摘要集合了上述所有难点。</p> <p>A Neural Attention Model for Abstractive Sentence Summarization</p> <h3 id="命名实体识别-ner-named-entity-recognition"><a href="#命名实体识别-ner-named-entity-recognition" class="header-anchor">#</a> 命名实体识别(NER, Named Entity Recognition)</h3> <p>Sequential Learning
给序列中的每个元素打上标签集合中的某个标签。
例如，对分词，标签集合 = [B,M,E,S],其中 B 代表这个汉字是词汇的开始字符，M 代表这个汉字是词汇的中间字符，E 代表这个汉字是词汇的结束字符，而 S 代表单字词。
这样打完之后，将 BME 组合起来就是词汇。
从这个角度看，分词、命名体识别、词性标注、依存分析等，都是序列标注问题。</p> <p>命名实体识别(Named EntitiesRecognition, NER)是自然语言处理的一个基础任务。其目的是识别语料中人名、地名、组织机构名等命名实体。由于这些命名实体数量不断增加，通常不可能在词典中穷尽列出，且其构成方法具有各自的一些规律性，因而,通常把对这些词的识别从词汇形态处理(如汉语切分)任务中独立处理，称为命名实体识别。命名实体识别技术是信息抽取、信息检索、机器翻译、问答系统等多种自然语言处理技术必不可少的组成部分。 命名实体是命名实体识别的研究主体，一般包括 3 大类(实体类、时间类和数字类)和 7 小类(人名、地名、机构名、时间、日期、货币和百分比)命名实体。评判一个命名实体是否被正确识别包括两个方面：实体的边界是否正确；实体的类型是否标注正确。主要错误类型包括文本正确，类型可能错误；反之，文本边界错误,而其包含的主要实体词和词类标记可能正确。</p> <p>命名实体识别的主要技术方法分为：基于规则和词典的方法、基于统计的方法、二者混合的方法等。</p> <p>1.基于规则和词典的方法
基于规则的方法多采用语言学专家手工构造规则模板,选用特征包括统计信息、标点符号、关键字、指示词和方向词、位置词(如尾字)、中心词等方法，以模式和字符串相匹配为主要手段，这类系统大多依赖于知识库和词典的建立。基于规则和词典的方法是命名实体识别中最早使用的方法，一般而言，当提取的规则能比较较精确地反映语言现象时，基于规则的方法性能要优于基于统计的方法。但是这些规则往往依赖于具体语言、领域和文本风格，编制过程耗时且难以涵盖所有的语言现象，特别容易产生错误，系统可移植性不好，对于不同的系统需要语言学专家重新书写规则。基于规则的方法的另外一个缺点是代价太大，存在系统建设周期长、移植性差而且需要建立不同领域知识库作为辅助以提高系统识别能力等问题。</p> <p>2.基于统计的方法
基于统计机器学习的方法主要包括：隐马尔可夫模型(HiddenMarkovMode,HMM)、较大熵(MaxmiumEntropy,ME)、支持向量机(Support VectorMachine,SVM)、条件随机场( ConditionalRandom Fields,CRF)等。</p> <p>在这几种学习方法中，较大熵模型结构紧凑，具有较好的通用性，主要缺点是训练时间复杂性非常高，有时甚至导致训练代价难以承受，另外由于需要明确的归一化计算，导致开销比较大。
而条件随机场为命名实体识别提供了一个特征灵活、全局最优的标注框架，但同时存在收敛速度慢、训练时间长的问题。一般说来，较大熵和支持向量机在正确率上要比隐马尔可夫模型高一些，
但是隐马尔可夫模型在训练和识别时的速度要快一些，主要是由于在利用 Viterbi 算法求解命名实体类别序列的效率较高。隐马尔可夫模型更适用于一些对实时性有要求以及像信息检索这样需要处理大量文本的应用,如短文本命名实体识别。
基于统计的方法对特征选取的要求较高，需要从文本中选择对该项任务有影响的各种特征，并将这些特征加入到特征向量中。依据特定命名实体识别所面临的主要困难和所表现出的特性，考虑选择能有效反映该类实体特性的特征集合。主要做法是通过对训练语料所包含的语言信息进行统计和分析，从训练语料中挖掘出特征。有关特征可以分为具体的单词特征、上下文特征、词典及词性特征、停用词特征、核心词特征以及语义特征等。
基于统计的方法对语料库的依赖也比较大，而可以用来建设和评估命名实体识别系统的大规模通用语料库又比较少。</p> <ol><li>基于深度学习的方法
双向 LSTM （即 BILSTM）</li></ol> <p>4.混合方法
自然语言处理并不完全是一个随机过程,单独使用基于统计的方法使状态搜索空间非常庞大，必须借助规则知识提前进行过滤修剪处理。
目前几乎没有单纯使用统计模型而不使用规则知识的命名实体识别系统，在很多情况下是使用混合方法：
3.1 统计学习方法之间或内部层叠融合。
3.2 规则、词典和机器学习方法之间的融合，其核心是融合方法技术。 在基于统计的学习方法中引入部分规则，将机器学习和人工知识结合起来。
3.3 将各类模型、算法结合起来，将前一级模型的结果作为下一级的训练数据，并用这些训练数据对模型进行训练，得到下一级模型。 这种方法在具体实现过程中需要考虑怎样高效地将两种方法结合起来，采用什么样的融合技术。由于命名实体识别在很大程度上依赖于分类技术,在分类方面可以采用的融合技术主要包括如 Voting, XVoting,GradingVa,l Grading 等。</p> <p>BILSTM + CRF，这是目前比较主流的算法。</p> <p>BILSTM 学习到的是每个词映射到 tag 的概率值，最后再用 CRF 学习一个最优序列。</p> <p>https://github.com/deepmipt/ner</p> <ul><li><p><a href="http://arxiv.org/abs/1103.0398" target="_blank" rel="noopener noreferrer">Natural Language Processing (almost) from Scratch by Ronan Collobert, et al., 2011<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <p>很喜欢这篇文章的文笔。
提出的架构可以通用于词性标注(POS)、语块标注(CHUNK)、NER、语义角色标注(SRL)等诸多问题（文章中对此四个问题当时的　 state-of-the-art 模型方法有一个汇总说明，注意到，这四个问题的难度是依次递增的），因此无需特定领域的先验知识。
从大量无标注的数据上学习中间表示</p></li> <li><p><a href="https://github.com/xmb-cipher/fofe-ner" target="_blank" rel="noopener noreferrer">FOFE NER<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p></li></ul> <hr> <h3 id="情感分析-意见挖掘-sentiment-analysis-opinion-mining"><a href="#情感分析-意见挖掘-sentiment-analysis-opinion-mining" class="header-anchor">#</a> 情感分析/意见挖掘(sentiment analysis/opinion mining)</h3> <p>问题总结为一句话：　针对什么表达了什么倾向的意见。</p> <p>其实，最准确的，还应该考虑：　谁针对什么表达了什么倾向的意见。不同的主体其权重不同，因此主体是谁这一点也应该注意；不过一般主体信息较为明确且容易获得，因此不是问题。</p> <p>针对什么就是评价对象抽取问题。所谓评价对象，即评论的维度，例如，对一件产品的评论可能涉及到该产品的某个组件或者某个功能，一篇电影评论中可能涉及剧本、特技、演员等。</p> <p>什么倾向就是要判断评论的情感极别是正面的还是负面的。</p> <p>情感分析并不是最终的目的，它是更上层建筑的基石。例如，它可以用在观点问答系统中，回答诸如“人们喜不喜欢这部电影的特效？”这样的问题，也可以用在推荐系统中，向用户推荐在某个维度上获得好评的产品（例如特技被认为超级牛叉的电影），以及用在观点总结系统中（典型的如淘宝上产品的评价汇总）</p> <h4 id="评价对象抽取"><a href="#评价对象抽取" class="header-anchor">#</a> 评价对象抽取</h4> <p>参考 <a href="http://www.cnblogs.com/siegfang/p/3455160.html" target="_blank" rel="noopener noreferrer">评价对象抽取综述<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>　一文。</p> <hr> <h3 id="意见挖掘"><a href="#意见挖掘" class="header-anchor">#</a> 意见挖掘</h3> <p>基于方面的意见挖掘
就是先把意见分为几种类型，例如，对服务人员态度的意见，多菜品的意见，对装修的意见，等等。</p> <p>OER : Opinion Element Recognition, 意见要素挖掘
ORE : Opinion Relation Extraction, 意见关系抽取
EPC : Emotion polarity Classification, 情感极性</p> <p>语义依存图是近年来提出的对树结构句法或语义表示的扩展，它与树结构的主要区别是允许一些词拥有多个父节点，从而使其成为有向无环图（direct edacyclic graph，DAG）</p> <p>目前依存树分析领域两大主流方法分别是基于转移（Transition-based）和基于图（Graph-based）的依存分析。基于图的算法将依存分析建模为在有向完全图中求解最大生成树的问题。基于转移的依存分析算法将句子的解码过程建模为一个有限自动机问题（类比编译器中的句法分析组件）。</p> <h4 id="情感计算"><a href="#情感计算" class="header-anchor">#</a> 情感计算</h4> <p>统计数据（暂时没有找到出处）指出一个语言的情感信息 10%来自于语言本身的内容，20%来自于语言的语调、语气，70%来自于表情。</p> <p>情感一致性：用户在一段时间内对同一话题的观点较为一致</p> <p>情感传播：沿着关系网络传播，并且，同一社区内的观点较为一致</p> <p><a href="https://github.com/rainarch/SentiBridge" target="_blank" rel="noopener noreferrer">中文实体情感知识库<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <hr> <h3 id="机器翻译-mt-machine-translation"><a href="#机器翻译-mt-machine-translation" class="header-anchor">#</a> 机器翻译(MT, Machine Translation)</h3> <p>本文介绍机器翻译。</p> <p><a href="https://github.com/stanfordnlp/phrasal" target="_blank" rel="noopener noreferrer">phrasal<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>: 斯坦福自然语言处理小组出品的统计机器翻译系统。</p> <ul><li><p>对齐(alignment)</p> <p>对齐，分段落、句子、短语、词语等不同级别</p> <p><a href="http://nlp.ict.ac.cn/~liuyang/papers/acl05_chn.pdf" target="_blank" rel="noopener noreferrer">词语对齐的对数线性模型<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p></li></ul> <ul><li><p><em><a href="http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf" target="_blank" rel="noopener noreferrer">Sequence to Sequence Learning with Neural Networks<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></em></p> <p>Google 出品.传统的 DNN 需要的输入和输出都是固定维度的，但在机器翻译这种情景中，输入和输出都是序列(sequence, 不定长)，因此提出了一个通用的序列到序列的方法，直接粗暴。具体地，用多层 LSTM 将序列映射到一个定长向量，然后用另外一个深层 LSTM 将该向量解码为序列，好比请了两个翻译。其实请翻译的做法之前就已经有人做了，只不过这里作者请的翻译是 RNN 的一个变种，即 LSTM. 而之所以请 LSTM 做翻译，是因为 LSTM 比较擅长处理长距离的时序依赖（体现在句子中就是长距离的单词之间的语义依赖），使得在语言翻译问题中能够得到好的效果。
trick：输入序列逆序化能够将效果提升很多，作者认为这是他们论文最大的贡献之一。可能的解释是，反序之后，输入序列和输出序列的平均距离虽然没有变，但最小距离减小很多。</p></li></ul> <ul><li><p>PBMT(phrase based machine translation)</p></li> <li><p>统计机器翻译(SMT, statistical machine translation)</p> <p>包括基于单词，基于短语，基于形态分析（词形变化），基于句法（不再将句子视为无结构的单词序列）</p> <p>奠基之作：<em>《A statistical approach to machine translation》</em>(简称 Brown90)</p></li></ul> <p>First Order Probablistic Logic
一阶概率逻辑</p> <p><a href="http://cogcomp.cs.illinois.edu/papers/BrazAmRo08.pdf" target="_blank" rel="noopener noreferrer">A survey of First-Order Probablistic Models<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> <a href="http://dl.acm.org/citation.cfm?id=1073462" target="_blank" rel="noopener noreferrer">Statistical Phrase-based Translation <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <hr> <h3 id="自然语言生成-nlg-natural-language-generation"><a href="#自然语言生成-nlg-natural-language-generation" class="header-anchor">#</a> 自然语言生成(NLG,natural Language Generation)</h3> <p>自然语言生成是高阶话题了，因为它涉及的能力比较全面，就好像语文考试中最重要的作文一样。
图片描述和视频描述是自然语言生成的重要应用，这个我们放到<a href="/grocery_store/section6/qa.html">问答系统专题</a>去讲。</p> <p><a href="http://www.cs.utoronto.ca/~ilya/pubs/2011/LANG-RNN.pdf" target="_blank" rel="noopener noreferrer">Generating Text with RNN<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <p><a href="https://zhuanlan.zhihu.com/p/29168803" target="_blank" rel="noopener noreferrer">Role of RL in Text Generation by GAN<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <ul><li><p><a href="https://arxiv.org/pdf/1804.02596.pdf" target="_blank" rel="noopener noreferrer">Simple Models for Word Formation in English Slang<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <p>提出了英语俚语中三种类型（混合, 缩略 和 叠词）的生成模型。
俚语作为一种超语法构词现象，伴随着互联网的兴起，更为年轻群体所广泛使用，对俚语的研究因此很有必要。</p> <p>俚语的非标准型特点。</p> <p>俚语举例：
Double Income No Kids 丁克一族——有双薪收入而没有小孩的夫妇
lambortini, lamborghini 和 martini 的混合</p> <p>insight:</p> <ol><li>混合型的俚语可以视为一个序列标记的问题，而不是 seq2seq.</li></ol></li></ul> <h3 id="语料资源"><a href="#语料资源" class="header-anchor">#</a> 语料资源</h3> <ul><li><p><a href="http://wordnet.princeton.edu/" target="_blank" rel="noopener noreferrer">WordNet<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <p>普林斯顿大学的科研人员设计的一个基于认知语言学的英语字典，它将单词按照意义组合成不同的同义词集合，并记录了这些集合之间的层次关系。</p></li> <li><p><a href="http://conceptnet5.media.mit.edu/" target="_blank" rel="noopener noreferrer">ConceptNet<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <p>语义网络，出自 MIT</p></li> <li><p><a href="https://framenet.icsi.berkeley.edu/fndrupal/home" target="_blank" rel="noopener noreferrer">FrameNet<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <p>伯克利出品。按照情景(frame)组织，每种情景由其触发词汇。缺点：frame 那么多，我们永远也弄不全的！</p></li> <li><p>HowNet</p></li></ul> <h2 id="深度学习文本模型"><a href="#深度学习文本模型" class="header-anchor">#</a> 深度学习文本模型</h2> <p>TextCNN
Convolutional Neural Networks for Sentence Classification</p> <p>做法： 先定长（长的截断，短的补 0）</p> <p>TextRNN</p> <hr> <h3 id="机器阅读理解"><a href="#机器阅读理解" class="header-anchor">#</a> 机器阅读理解</h3> <p><a href="https://mp.weixin.qq.com/s/vAj7vUkvPS7jqHzewb5AuQ" target="_blank" rel="noopener noreferrer">2018 机器阅读理解技术竞赛<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>
2018 机器阅读理解技术竞赛由中国中文信息学会（CIPS）和中国计算机学会（CCF）联合主办，
百度公司、中国中文信息学会评测工作委员会和计算机学会中文信息技术专委会联合承办</p> <p>竞赛数据集包含 30 万来自百度搜索的真实问题，每个问题对应 5 个候选文档文本，以及人工撰写的优质答案。数据集划分为 28 万的训练集，1 万开发集和 1 万测试集。该数据集中包含了 DuReader(https://arxiv.org/abs/1711.05073)中已发布的 20 万问题数据，可自由下载（下载地址:(http://ai.baidu.com/broad/introduction?dataset=dureader)）用于预训练和测试</p> <hr> <h3 id="常用工具"><a href="#常用工具" class="header-anchor">#</a> 常用工具</h3> <ul><li><p><a href="">NLTK</a></p></li> <li><p><a href="http://spacy.io/" target="_blank" rel="noopener noreferrer">spaCy<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>
号称具有工业强度，用 Cython 编写，比 NLTK 快 400 倍</p> <p>仅支持英语，且不能在 windows 下使用（这倒没啥大不了的）</p></li> <li><p>Stanford Core NLP</p> <p>还有<a href="https://github.com/dasmith/stanford-corenlp-python" target="_blank" rel="noopener noreferrer">python 接口<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p></li></ul> <ul><li>语言技术平台（Language Technology Platfor，LTP）</li> <li>结巴分词</li></ul> <h2 id="神经网络语言模型（nnlm）"><a href="#神经网络语言模型（nnlm）" class="header-anchor">#</a> 神经网络语言模型（NNLM）</h2> <p>第一篇提出神经网络语言模型的论文是 Bengio 在 2003 年发表的「A Neural Probabilistic Language Model」</p> <blockquote><p>预训练语言模型的成功，证明了我们可以从海量的无标注文本中学到潜在的语义信息，而无需为每一项下游 NLP 任务单独标注大量训练数据。此外，预训练语言模型的成功也开创了 NLP 研究的新范式，即首先使用大量无监督语料进行语言模型预训练（Pre-training），再使用少量标注语料进行微调（Fine-tuning）来完成具体 NLP 任务（分类、序列标注、句间关系判断和机器阅读理解等）.</p></blockquote> <h3 id="transformer"><a href="#transformer" class="header-anchor">#</a> transformer</h3> <p><a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html" target="_blank" rel="noopener noreferrer">Transformer: A Novel Neural Network Architecture for Language Understanding<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <p>预训练（掩词猜测和下句预测）</p> <p><a href="https://towardsdatascience.com/deconstructing-bert-distilling-6-patterns-from-100-million-parameters-b49113672f77" target="_blank" rel="noopener noreferrer">从 bert 中提取 6 种注意力模式<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>
Pattern 1: Attention to next word
Pattern 2: Attention to previous word
Pattern 3: Attention to identical/related words
Pattern 4: Attention to identical/related words in other sentence
Pattern 5: Attention to other words predictive of word
Pattern 6: Attention to delimiter tokens</p> <p>Transformer 在 2017 年由 Google 在题为 <font color="red">Attention Is All You Need</font> 的论文中提出。</p> <blockquote><p>Transformer 是一个完全基于注意力机制的编解码器模型，它完全抛弃了之前其它模型引入注意力机制后仍然保留的循环与卷积结构，而采用了自注意力（Self-attention）机制，在任务表现、并行能力和易于训练性方面都有大幅的提高。
在 Transformer 出现之前，基于神经网络的机器翻译模型多数都采用了 RNN 的模型架构，它们依靠循环功能进行有序的序列操作。虽然 RNN 架构有较强的序列建模能力，但是存在训练速度慢，训练质量低等问题。
与基于 RNN 的方法不同，Transformer 模型中没有循环结构，而是把序列中的所有单词或者符号并行处理，同时借助自注意力机制对句子中所有单词之间的关系直接进行建模，而无需考虑各自的位置。具体而言，如果要计算给定单词的下一个表征，Transformer 会将该单词与句子中的其他单词一一对比，并得出这些单词的注意力分数。注意力分数决定其他单词对给定词汇的语义影响。之后，注意力分数用作所有单词表征的平均权重，这些表征输入全连接网络，生成新表征。</p></blockquote> <p>Transformer 自从提出以来，俨然已经是 RNN 的一个有竞争力的替代品了</p> <p>They address a significant shortcoming of RNNs, i.e. their inherently sequential computation which prevents parallelization across elements of the input sequence, whilst still addressing the vanishing gradients problem through its self-attention mechanism</p> <p>依赖自注意机制</p> <p>优点</p> <ol><li><strong>时序无关可并行</strong>
There is no connections in time as with RNNs, allowing one to fully parallelize per-symbol computations.</li> <li><strong>全局感知域</strong>
Each symbol’s representation is directly informed by all other symbols’ representations (in contrast to e.g. convolutional architectures which typically have a limited receptive field).
缺点</li></ol> <p><a href="https://arxiv.org/pdf/1807.03819.pdf" target="_blank" rel="noopener noreferrer">Universal Transformer<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>
Published as a conference paper at ICLR 2019</p> <p>Universal Transformer 的产生是因为 Transformer 在实践上和理论上的两个缺点。
universal 代表的是 computationally universal，即图灵完备</p> <p>Transition 层？</p> <ul><li>[ ] <a href="http://mostafadehghani.com/2019/05/05/universal-transformers/" target="_blank" rel="noopener noreferrer">Universal Transformers 解读<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></li></ul> <h4 id="elmo（embeddings-from-language-models）"><a href="#elmo（embeddings-from-language-models）" class="header-anchor">#</a> ELMO（Embeddings from Language Models）</h4> <p>来自 2018 年的论文 <code>Deep contextualized word representations</code></p> <p>我们知道，词向量一经训练，就是固定的了，但是像多义词这种，用固定的向量并不足以表达多义。
也就是说，词向量不灵活！</p> <p>ELMo 的做法是我们只预训练 language model, 而 word embedding 是通过输入的句子实时输出的，这样单词的意思就是上下文相关的了, 这样就很大程度上缓解了歧义的发生.</p> <h4 id="bert"><a href="#bert" class="header-anchor">#</a> Bert</h4> <p>参见 <a href="/grocery_store/section6/bert.html">bert 走读</a></p> <h4 id="gpt-2"><a href="#gpt-2" class="header-anchor">#</a> GPT-2</h4> <p><a href="https://openai.com/blog/better-language-models/#update" target="_blank" rel="noopener noreferrer">GPT_2<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <p>来自 OpenAI 的语言模型全能王！</p> <p>作为一个没有经过任何领域数据专门训练的模型，它的表现比那些专为特定领域打造的模型还要好</p> <p>简单来说，GPT-2 就是基于 Transformer 架构的大规模模型。</p> <p>GPT-2 是 GPT 算法“进化版”，比 GPT 参数扩大 10 倍，达到了 15 亿个，数据量扩大 10 倍，使用了包含 800 万个网页的数据集，共有 40 GB。</p> <p>这个庞大的算法使用语言建模作为训练信号，以无监督的方式在大型数据集上训练一个 Transformer，然后在更小的监督数据集上微调这个模型，以帮助它解决特定任务</p> <p>https://github.com/BrikerMan/Kashgari</p> <h2 id="学习资料"><a href="#学习资料" class="header-anchor">#</a> 学习资料</h2> <ul><li><p><a href="https://class.coursera.org/nlp/lecture" target="_blank" rel="noopener noreferrer">coursera 课程： 自然语言处理<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <p>老师为斯坦福大学的 Dan Jurafsky 和 Christopher Manning</p></li> <li><p><a href="http://icml.cc/2015/tutorials/icml2015-nlu-tutorial.pdf" target="_blank" rel="noopener noreferrer">Natural Language Understanding: Fundations and State-of-the-Art<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>
须深读之。</p> <p>ICML 教程之一，待看</p></li> <li><p><a href="https://www.cs.colorado.edu/~martin/slp.html" target="_blank" rel="noopener noreferrer">Speech and Language Processing --- An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p></li> <li><p><a href="https://github.com/niangaotuantuan/Publications-of-Deep-Learning-in-NLP/blob/master/neural_lm.md" target="_blank" rel="noopener noreferrer">Publications of deep Learning in NLP<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p></li> <li><p><a href="http://computing.dcu.ie/~qliu/Talks/20180321%20What%20does%20deep%20learning%20bring%20to%20natural%20language%20processing%20v2.pdf" target="_blank" rel="noopener noreferrer">What has Deep Learning brought to NLP<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <p>毫无疑问，DL 为 NLP 带来的是深刻的变化。几乎所有的 NLP 任务的最好结果都被刷新；诸如图片标题生成等之前比较困难的问题变得容易了；而类似中间语等之前貌似不可能的问题也变得可能了。</p> <p>本文试图回答的几个问题：</p> <ol><li>DL 为 NLP 带来了哪些基础性的变化？</li> <li>这些基础性的变化有什么影响？</li> <li>NLP 中的 DL 有什么弱项？</li> <li>NLP 的未来方向在哪儿？</li></ol> <p>针对第一个问题的回答：
NLP 问题的定义空间变了，从符号离散空间变到了数值连续空间。随着这个趋势，方法也从基于规则的，到统计方法，再到神经网络。 其他变化还包括：更细粒度（不再是词和短语这样的硬性粒度划分，不同粒度之间的知识也能更好迁移），更好的泛化，打破模型之间的边界（多模态和跨模型方法变得更直接也更容易，类似 VQA 这样的问题能够得以有效解决），整体性的 NLP 模型（不再子模型分而治之，而是统一建模，端到端）。</p> <p>针对第三个问题的回答：
小数据上行不通；语义不明；</p> <p>针对第四个问题的回答：
应用更多诸如强化学习、对抗学习等 DL 技术到 NLP 中；研究将人类智慧和语义纳入模型的更普遍方法</p></li></ul> <h2 id="附录"><a href="#附录" class="header-anchor">#</a> 附录</h2> <h3 id="nlp-灾难系列"><a href="#nlp-灾难系列" class="header-anchor">#</a> NLP 灾难系列</h3> <ul><li>熊大便当烤肉饭</li> <li>小龙女对杨过说：我曾经也想过过过过过过的生活。</li> <li>这个领带看着不错……才怪！</li></ul> <p><img src="/grocery_store/assets/img/2020-03-19-09-59-12.cda794c6.png" alt=""></p> <ul><li>这不是为了对手的保护，而是为了对手的保护</li> <li>广州拟立法禁食野生动物，食用人最高罚 10 万</li> <li></li></ul> <h3 id="顶会-研究机构"><a href="#顶会-研究机构" class="header-anchor">#</a> 顶会/研究机构</h3> <ul><li><p>ACL 计算机语言学协会</p> <p><a href="http://aclweb.org/anthology/index.html" target="_blank" rel="noopener noreferrer">ACL Anthology<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p></li> <li><p>NIPS 　每年 12 月由 NIPS 基金会主办的机器学习和神经计算领域的顶级会议</p></li> <li><p>EMNLP(Conference on Empirical Methods in Natural Language Processing)</p> <p>自然语言处理实证方法会议，2015 年 9 月 17-22 日在葡萄牙里斯本市召开</p></li></ul> <p>－ <a href="http://nlp.stanford.edu/" target="_blank" rel="noopener noreferrer">The Stanford Natural Language Processing Group<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p> <h3 id="历史发展"><a href="#历史发展" class="header-anchor">#</a> 历史发展</h3> <p>2019 年在 NLP 的发展史上是一个特殊的年份，在这一年，基于深度学习的 NLP 方法取得了光彩夺目的发展。</p></div> <footer class="page-edit"><!----> <!----></footer> <div class="page-nav"><p class="inner"><span class="prev">
      ←
      <a href="/grocery_store/section6/multimodel.html" class="prev">
        多模型多任务专题 {ignore=ture}
      </a></span> <span class="next"><a href="/grocery_store/section6/nlp_reading_comprehension.html">
        阅读理解
      </a>
      →
    </span></p></div> </main></div><div class="global-ui"><!----></div></div>
    <script src="/grocery_store/assets/js/app.47d3b9e2.js" defer></script><script src="/grocery_store/assets/js/3.e15ba04f.js" defer></script><script src="/grocery_store/assets/js/45.9345a9b8.js" defer></script>
  </body>
</html>
